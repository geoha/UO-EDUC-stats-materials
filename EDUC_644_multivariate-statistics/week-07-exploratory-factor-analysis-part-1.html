<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>8 Week 07: Exploratory Factor Analysis, Part 1 | Guidebook for Using R in Applied Multivariate Statistics</title>
  <meta name="description" content="Course guidebook for multivariate statistics" />
  <meta name="generator" content="bookdown 0.44 and GitBook 2.6.7" />

  <meta property="og:title" content="8 Week 07: Exploratory Factor Analysis, Part 1 | Guidebook for Using R in Applied Multivariate Statistics" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Course guidebook for multivariate statistics" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="8 Week 07: Exploratory Factor Analysis, Part 1 | Guidebook for Using R in Applied Multivariate Statistics" />
  
  <meta name="twitter:description" content="Course guidebook for multivariate statistics" />
  

<meta name="author" content="George M. Harrison" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="week-06-principal-components-analysis.html"/>
<link rel="next" href="week-08-exploratory-factor-analysis-part-2.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<link href="libs/tabwid-1.1.3/tabwid.css" rel="stylesheet" />
<script src="libs/tabwid-1.1.3/tabwid.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Multivariate Statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><i class="fa fa-check"></i><b>1</b> Terminology and Matrix Manipulations Used in Multivariate Statistics</a>
<ul>
<li class="chapter" data-level="1.1" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#scalars"><i class="fa fa-check"></i><b>1.1</b> Scalars</a></li>
<li class="chapter" data-level="1.2" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#vectors"><i class="fa fa-check"></i><b>1.2</b> Vectors</a></li>
<li class="chapter" data-level="1.3" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#matrices"><i class="fa fa-check"></i><b>1.3</b> Matrices</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#data-files-as-matrices"><i class="fa fa-check"></i><b>1.3.1</b> Data Files as Matrices</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#symmetrical-matrices"><i class="fa fa-check"></i><b>1.4</b> Symmetrical Matrices</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#variance-covariance-matrices"><i class="fa fa-check"></i><b>1.4.1</b> Variance-covariance Matrices</a></li>
<li class="chapter" data-level="1.4.2" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#greek-letters"><i class="fa fa-check"></i><b>1.4.2</b> Greek letters</a></li>
<li class="chapter" data-level="1.4.3" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#example-variance-covariance-matrix"><i class="fa fa-check"></i><b>1.4.3</b> Example variance-covariance matrix</a></li>
<li class="chapter" data-level="1.4.4" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#the-diagonal-and-trace"><i class="fa fa-check"></i><b>1.4.4</b> The diagonal and trace</a></li>
<li class="chapter" data-level="1.4.5" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#an-identity-matrix"><i class="fa fa-check"></i><b>1.4.5</b> An identity matrix</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#transposed-vectors-and-matrices"><i class="fa fa-check"></i><b>1.5</b> Transposed Vectors and Matrices</a></li>
<li class="chapter" data-level="1.6" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#matrix-addition-and-subtraction"><i class="fa fa-check"></i><b>1.6</b> Matrix Addition and Subtraction</a></li>
<li class="chapter" data-level="1.7" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#operations-on-matrices-with-a-scalar"><i class="fa fa-check"></i><b>1.7</b> Operations on Matrices with a Scalar</a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#sum-of-square-and-cross-product-sscp-matrices"><i class="fa fa-check"></i><b>1.7.1</b> Sum-of-square-and-cross-product (SSCP) matrices</a></li>
<li class="chapter" data-level="1.7.2" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#operations-on-elements-of-a-matrix-example-with-a-correlation-matrix"><i class="fa fa-check"></i><b>1.7.2</b> Operations on elements of a matrix: Example with a correlation matrix</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#matrix-multiplication"><i class="fa fa-check"></i><b>1.8</b> Matrix Multiplication</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#another-example-of-matrix-multiplication-to-get-the-sscp"><i class="fa fa-check"></i><b>1.8.1</b> Another example of matrix multiplication, to get the SSCP</a></li>
<li class="chapter" data-level="1.8.2" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#the-ordering-is-important"><i class="fa fa-check"></i><b>1.8.2</b> The ordering is important</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#inverse-of-a-matrix"><i class="fa fa-check"></i><b>1.9</b> Inverse of a Matrix</a></li>
<li class="chapter" data-level="1.10" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#matrix-determinants-as-generalized-variance"><i class="fa fa-check"></i><b>1.10</b> Matrix determinants, as generalized variance</a></li>
<li class="chapter" data-level="1.11" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#eigenvectors-and-eigenvalues"><i class="fa fa-check"></i><b>1.11</b> Eigenvectors and Eigenvalues</a></li>
<li class="chapter" data-level="1.12" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#summary"><i class="fa fa-check"></i><b>1.12</b> Summary</a></li>
<li class="chapter" data-level="1.13" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#optional-spss-and-r-code-to-manipulate-matrices"><i class="fa fa-check"></i><b>1.13</b> Optional: SPSS and R Code to Manipulate Matrices</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="hotellings-t2.html"><a href="hotellings-t2.html"><i class="fa fa-check"></i><b>2</b> Hotelling’s <span class="math inline">\(T^2\)</span></a>
<ul>
<li class="chapter" data-level="2.1" data-path="hotellings-t2.html"><a href="hotellings-t2.html#within-group-covariance-matrices"><i class="fa fa-check"></i><b>2.1</b> Within-group covariance matrices</a></li>
<li class="chapter" data-level="2.2" data-path="hotellings-t2.html"><a href="hotellings-t2.html#sscp-matrices"><i class="fa fa-check"></i><b>2.2</b> SSCP matrices</a></li>
<li class="chapter" data-level="2.3" data-path="hotellings-t2.html"><a href="hotellings-t2.html#conducting-the-test-using-functions"><i class="fa fa-check"></i><b>2.3</b> Conducting the Test using Functions</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="planned-comparisons-and-contrasts.html"><a href="planned-comparisons-and-contrasts.html"><i class="fa fa-check"></i><b>3</b> Planned Comparisons and Contrasts</a>
<ul>
<li class="chapter" data-level="3.1" data-path="planned-comparisons-and-contrasts.html"><a href="planned-comparisons-and-contrasts.html#data-from-week-1"><i class="fa fa-check"></i><b>3.1</b> Data from Week 1</a></li>
<li class="chapter" data-level="3.2" data-path="planned-comparisons-and-contrasts.html"><a href="planned-comparisons-and-contrasts.html#orthogonal-contrasts"><i class="fa fa-check"></i><b>3.2</b> Orthogonal contrasts</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="planned-comparisons-and-contrasts.html"><a href="planned-comparisons-and-contrasts.html#conditions-for-orthogonal-contrasts"><i class="fa fa-check"></i><b>3.2.1</b> Conditions for orthogonal contrasts</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="planned-comparisons-and-contrasts.html"><a href="planned-comparisons-and-contrasts.html#non-orthogonal-contrasts"><i class="fa fa-check"></i><b>3.3</b> Non-orthogonal contrasts</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="planned-comparisons-and-contrasts.html"><a href="planned-comparisons-and-contrasts.html#accounting-for-type-i-error-in-non-orthogonal-contrasts."><i class="fa fa-check"></i><b>3.3.1</b> Accounting for Type I error in non-orthogonal contrasts.</a></li>
<li class="chapter" data-level="3.3.2" data-path="planned-comparisons-and-contrasts.html"><a href="planned-comparisons-and-contrasts.html#post-hoc-tests"><i class="fa fa-check"></i><b>3.3.2</b> Post-hoc tests</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html"><i class="fa fa-check"></i><b>4</b> Week 03 Statistical Assumptions of MANOVA</a>
<ul>
<li class="chapter" data-level="4.1" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html#data"><i class="fa fa-check"></i><b>4.1</b> Data</a></li>
<li class="chapter" data-level="4.2" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html#descriptive-statistics"><i class="fa fa-check"></i><b>4.2</b> Descriptive statistics</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html#by-group-descriptive-statistics"><i class="fa fa-check"></i><b>4.2.1</b> By-group descriptive statistics</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html#normality-assumption"><i class="fa fa-check"></i><b>4.3</b> Normality Assumption</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html#univariate-normality"><i class="fa fa-check"></i><b>4.3.1</b> Univariate Normality</a></li>
<li class="chapter" data-level="4.3.2" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html#examining-univariate-normality-using-quantile-quantile-plots"><i class="fa fa-check"></i><b>4.3.2</b> Examining univariate normality using quantile-quantile plots</a></li>
<li class="chapter" data-level="4.3.3" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html#a-caution-about-statistical-tests-of-statistical-assumptions"><i class="fa fa-check"></i><b>4.3.3</b> A caution about statistical tests of statistical assumptions</a></li>
<li class="chapter" data-level="4.3.4" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html#mahalanobis-distance-and-multivariate-outliers"><i class="fa fa-check"></i><b>4.3.4</b> Mahalanobis Distance and Multivariate Outliers</a></li>
<li class="chapter" data-level="4.3.5" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html#statistical-tests-of-multivariate-normality"><i class="fa fa-check"></i><b>4.3.5</b> Statistical Tests of Multivariate Normality</a></li>
<li class="chapter" data-level="4.3.6" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html#homogeneity-of-covariance"><i class="fa fa-check"></i><b>4.3.6</b> Homogeneity of Covariance</a></li>
<li class="chapter" data-level="4.3.7" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html#conclusions-about-our-statistical-assumptions"><i class="fa fa-check"></i><b>4.3.7</b> Conclusions about our statistical assumptions</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html#fitting-the-manova-model"><i class="fa fa-check"></i><b>4.4</b> Fitting the MANOVA Model</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html#follow-up-analysis-with-univariate-f-tests"><i class="fa fa-check"></i><b>4.4.1</b> Follow-up analysis with univariate <em>F</em>-tests</a></li>
<li class="chapter" data-level="4.4.2" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html#effect-size"><i class="fa fa-check"></i><b>4.4.2</b> Effect size</a></li>
<li class="chapter" data-level="4.4.3" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html#comparing-the-model-without-outlier"><i class="fa fa-check"></i><b>4.4.3</b> Comparing the Model Without Outlier</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html#robust-one-way-manova"><i class="fa fa-check"></i><b>4.5</b> Robust One-way MANOVA</a></li>
<li class="chapter" data-level="4.6" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html#summary-1"><i class="fa fa-check"></i><b>4.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="week-04-factorial-manova.html"><a href="week-04-factorial-manova.html"><i class="fa fa-check"></i><b>5</b> Week 04 Factorial MANOVA</a>
<ul>
<li class="chapter" data-level="5.1" data-path="week-04-factorial-manova.html"><a href="week-04-factorial-manova.html#following-tabachnick-and-fidells-notes"><i class="fa fa-check"></i><b>5.1</b> Following Tabachnick and Fidell’s notes</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="week-04-factorial-manova.html"><a href="week-04-factorial-manova.html#another-way-to-get-the-sscp_w"><i class="fa fa-check"></i><b>5.1.1</b> Another way to get the <span class="math inline">\(SSCP_W\)</span>:</a></li>
<li class="chapter" data-level="5.1.2" data-path="week-04-factorial-manova.html"><a href="week-04-factorial-manova.html#mathbfsscp_between-for-each-factor"><i class="fa fa-check"></i><b>5.1.2</b> <span class="math inline">\(\mathbf{SSCP_{Between}}\)</span> for each factor</a></li>
<li class="chapter" data-level="5.1.3" data-path="week-04-factorial-manova.html"><a href="week-04-factorial-manova.html#mathbfsscp_between-for-the-interaction"><i class="fa fa-check"></i><b>5.1.3</b> <span class="math inline">\(\mathbf{SSCP_{Between}}\)</span> for the interaction</a></li>
<li class="chapter" data-level="5.1.4" data-path="week-04-factorial-manova.html"><a href="week-04-factorial-manova.html#these-by-hand-calculations-will-be-available-in-manova."><i class="fa fa-check"></i><b>5.1.4</b> These by-hand calculations will be available in <code>manova()</code>.</a></li>
<li class="chapter" data-level="5.1.5" data-path="week-04-factorial-manova.html"><a href="week-04-factorial-manova.html#calculating-the-generalized-variance"><i class="fa fa-check"></i><b>5.1.5</b> Calculating the generalized variance</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="week-04-factorial-manova.html"><a href="week-04-factorial-manova.html#calculating-wilks-lambda"><i class="fa fa-check"></i><b>5.2</b> Calculating Wilks’ Lambda</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="week-04-factorial-manova.html"><a href="week-04-factorial-manova.html#fit-the-factorial-manova"><i class="fa fa-check"></i><b>5.2.1</b> Fit the factorial MANOVA</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="week-04-factorial-manova.html"><a href="week-04-factorial-manova.html#mancova"><i class="fa fa-check"></i><b>5.3</b> MANCOVA</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="week-04-factorial-manova.html"><a href="week-04-factorial-manova.html#mancova-example"><i class="fa fa-check"></i><b>5.3.1</b> MANCOVA example</a></li>
<li class="chapter" data-level="5.3.2" data-path="week-04-factorial-manova.html"><a href="week-04-factorial-manova.html#lets-think-about-ordering-and-type-i-sums-of-squares"><i class="fa fa-check"></i><b>5.3.2</b> Let’s think about ordering and Type I sums of squares</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="week-05-discriminant-function-analysis.html"><a href="week-05-discriminant-function-analysis.html"><i class="fa fa-check"></i><b>6</b> Week 05, Discriminant Function Analysis</a>
<ul>
<li class="chapter" data-level="6.0.1" data-path="week-05-discriminant-function-analysis.html"><a href="week-05-discriminant-function-analysis.html#fitting-the-discriminant-function-analysis-model"><i class="fa fa-check"></i><b>6.0.1</b> Fitting the discriminant function analysis model</a></li>
<li class="chapter" data-level="6.0.2" data-path="week-05-discriminant-function-analysis.html"><a href="week-05-discriminant-function-analysis.html#dimension-reduction-analysis"><i class="fa fa-check"></i><b>6.0.2</b> Dimension reduction analysis</a></li>
<li class="chapter" data-level="6.0.3" data-path="week-05-discriminant-function-analysis.html"><a href="week-05-discriminant-function-analysis.html#structure-and-standardized-discriminant-function-coefficients"><i class="fa fa-check"></i><b>6.0.3</b> Structure and standardized discriminant function coefficients</a></li>
<li class="chapter" data-level="6.0.4" data-path="week-05-discriminant-function-analysis.html"><a href="week-05-discriminant-function-analysis.html#discriminant-function-scores"><i class="fa fa-check"></i><b>6.0.4</b> Discriminant function scores</a></li>
<li class="chapter" data-level="6.0.5" data-path="week-05-discriminant-function-analysis.html"><a href="week-05-discriminant-function-analysis.html#group-means-centroids-of-the-discriminant-function-scores"><i class="fa fa-check"></i><b>6.0.5</b> Group means (centroids) of the discriminant-function scores</a></li>
<li class="chapter" data-level="6.0.6" data-path="week-05-discriminant-function-analysis.html"><a href="week-05-discriminant-function-analysis.html#bivariate-scatterplot"><i class="fa fa-check"></i><b>6.0.6</b> Bivariate scatterplot</a></li>
<li class="chapter" data-level="6.1" data-path="week-05-discriminant-function-analysis.html"><a href="week-05-discriminant-function-analysis.html#write-up"><i class="fa fa-check"></i><b>6.1</b> Write-up</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html"><i class="fa fa-check"></i><b>7</b> Week 06: Principal Components Analysis</a>
<ul>
<li class="chapter" data-level="7.1" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#example-1-to-see-some-properties-of-pca"><i class="fa fa-check"></i><b>7.1</b> Example 1, to see some properties of PCA</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#pca-with-covariance-vs.-correlation"><i class="fa fa-check"></i><b>7.1.1</b> PCA with Covariance vs. Correlation</a></li>
<li class="chapter" data-level="7.1.2" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#unscaled-eigenvectors"><i class="fa fa-check"></i><b>7.1.2</b> Unscaled eigenvectors</a></li>
<li class="chapter" data-level="7.1.3" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#looking-at-the-properties-of-eigenvectors-and-eigenvalues"><i class="fa fa-check"></i><b>7.1.3</b> Looking at the properties of eigenvectors and eigenvalues</a></li>
<li class="chapter" data-level="7.1.4" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#using-the-psych-package"><i class="fa fa-check"></i><b>7.1.4</b> Using the psych package</a></li>
<li class="chapter" data-level="7.1.5" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#determining-how-many-components-to-retain"><i class="fa fa-check"></i><b>7.1.5</b> Determining how many components to retain</a></li>
<li class="chapter" data-level="7.1.6" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#reduced-pca-model"><i class="fa fa-check"></i><b>7.1.6</b> Reduced PCA model</a></li>
<li class="chapter" data-level="7.1.7" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#interpretation-1"><i class="fa fa-check"></i><b>7.1.7</b> Interpretation</a></li>
<li class="chapter" data-level="7.1.8" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#interpretation-of-the-rotated-solution"><i class="fa fa-check"></i><b>7.1.8</b> Interpretation of the rotated solution</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#example-two"><i class="fa fa-check"></i><b>7.2</b> Example Two</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#pca-on-unstandardized-scores"><i class="fa fa-check"></i><b>7.2.1</b> PCA on Unstandardized Scores</a></li>
<li class="chapter" data-level="7.2.2" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#pca-on-the-standardized-scores"><i class="fa fa-check"></i><b>7.2.2</b> PCA on the Standardized Scores</a></li>
<li class="chapter" data-level="7.2.3" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#let-the-psych-package-do-the-work"><i class="fa fa-check"></i><b>7.2.3</b> Let the Psych Package Do the Work</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#heptathlon-example"><i class="fa fa-check"></i><b>7.3</b> Heptathlon example</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#data-and-data-cleaning"><i class="fa fa-check"></i><b>7.3.1</b> Data and data cleaning</a></li>
<li class="chapter" data-level="7.3.2" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#examine-assumption-of-linearity"><i class="fa fa-check"></i><b>7.3.2</b> Examine assumption of linearity</a></li>
<li class="chapter" data-level="7.3.3" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#multivariate-normality-and-outliers"><i class="fa fa-check"></i><b>7.3.3</b> Multivariate normality and outliers</a></li>
<li class="chapter" data-level="7.3.4" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#re-examine-linearity-and-normality"><i class="fa fa-check"></i><b>7.3.4</b> Re-examine linearity and normality</a></li>
<li class="chapter" data-level="7.3.5" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#fit-the-initial-pca"><i class="fa fa-check"></i><b>7.3.5</b> Fit the initial PCA</a></li>
<li class="chapter" data-level="7.3.6" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#determine-the-number-of-components-to-retain"><i class="fa fa-check"></i><b>7.3.6</b> Determine the number of components to retain</a></li>
<li class="chapter" data-level="7.3.7" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#refit-the-pca-model-using-rotation-if-that-is-acceptable-in-your-field"><i class="fa fa-check"></i><b>7.3.7</b> Refit the PCA model, using rotation if that is acceptable in your field</a></li>
<li class="chapter" data-level="7.3.8" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#interpret-the-components"><i class="fa fa-check"></i><b>7.3.8</b> Interpret the components</a></li>
<li class="chapter" data-level="7.3.9" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#saving-the-scores"><i class="fa fa-check"></i><b>7.3.9</b> Saving the scores</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#other-example-from-later-in-the-chapter-not-assigned"><i class="fa fa-check"></i><b>7.4</b> Other example from later in the chapter (not assigned)</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#usairpollution-data"><i class="fa fa-check"></i><b>7.4.1</b> USairpollution Data</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#another-resource"><i class="fa fa-check"></i><b>7.5</b> Another resource</a></li>
<li class="chapter" data-level="7.6" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#references"><i class="fa fa-check"></i><b>7.6</b> References</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="week-07-exploratory-factor-analysis-part-1.html"><a href="week-07-exploratory-factor-analysis-part-1.html"><i class="fa fa-check"></i><b>8</b> Week 07: Exploratory Factor Analysis, Part 1</a>
<ul>
<li class="chapter" data-level="8.1" data-path="week-07-exploratory-factor-analysis-part-1.html"><a href="week-07-exploratory-factor-analysis-part-1.html#introductory-comments"><i class="fa fa-check"></i><b>8.1</b> Introductory comments</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="week-07-exploratory-factor-analysis-part-1.html"><a href="week-07-exploratory-factor-analysis-part-1.html#packages"><i class="fa fa-check"></i><b>8.1.1</b> Packages</a></li>
<li class="chapter" data-level="8.1.2" data-path="week-07-exploratory-factor-analysis-part-1.html"><a href="week-07-exploratory-factor-analysis-part-1.html#efa-ne-pca"><i class="fa fa-check"></i><b>8.1.2</b> EFA <span class="math inline">\(\ne\)</span> PCA</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="week-07-exploratory-factor-analysis-part-1.html"><a href="week-07-exploratory-factor-analysis-part-1.html#preparatory-steps"><i class="fa fa-check"></i><b>8.2</b> Preparatory steps</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="week-07-exploratory-factor-analysis-part-1.html"><a href="week-07-exploratory-factor-analysis-part-1.html#examining-the-data"><i class="fa fa-check"></i><b>8.2.1</b> Examining the data</a></li>
<li class="chapter" data-level="8.2.2" data-path="week-07-exploratory-factor-analysis-part-1.html"><a href="week-07-exploratory-factor-analysis-part-1.html#addressing-assumptions"><i class="fa fa-check"></i><b>8.2.2</b> Addressing assumptions</a></li>
<li class="chapter" data-level="8.2.3" data-path="week-07-exploratory-factor-analysis-part-1.html"><a href="week-07-exploratory-factor-analysis-part-1.html#determining-factorability"><i class="fa fa-check"></i><b>8.2.3</b> Determining factorability</a></li>
<li class="chapter" data-level="8.2.4" data-path="week-07-exploratory-factor-analysis-part-1.html"><a href="week-07-exploratory-factor-analysis-part-1.html#determining-how-many-factors-to-retain"><i class="fa fa-check"></i><b>8.2.4</b> Determining how many factors to retain</a></li>
<li class="chapter" data-level="8.2.5" data-path="week-07-exploratory-factor-analysis-part-1.html"><a href="week-07-exploratory-factor-analysis-part-1.html#factor-analysis-with-a-data-matrix-raw-data"><i class="fa fa-check"></i><b>8.2.5</b> Factor analysis with a data matrix (raw data)</a></li>
<li class="chapter" data-level="8.2.6" data-path="week-07-exploratory-factor-analysis-part-1.html"><a href="week-07-exploratory-factor-analysis-part-1.html#factor-analysis-with-correlation-data"><i class="fa fa-check"></i><b>8.2.6</b> Factor analysis with correlation data</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="week-07-exploratory-factor-analysis-part-1.html"><a href="week-07-exploratory-factor-analysis-part-1.html#references-1"><i class="fa fa-check"></i><b>8.3</b> References</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="week-08-exploratory-factor-analysis-part-2.html"><a href="week-08-exploratory-factor-analysis-part-2.html"><i class="fa fa-check"></i><b>9</b> Week 08: Exploratory Factor Analysis, Part 2</a>
<ul>
<li class="chapter" data-level="9.1" data-path="week-08-exploratory-factor-analysis-part-2.html"><a href="week-08-exploratory-factor-analysis-part-2.html#factor-analysis-with-correlation-data-1"><i class="fa fa-check"></i><b>9.1</b> Factor analysis with correlation data</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="week-08-exploratory-factor-analysis-part-2.html"><a href="week-08-exploratory-factor-analysis-part-2.html#setting-up-the-data-1"><i class="fa fa-check"></i><b>9.1.1</b> Setting up the data</a></li>
<li class="chapter" data-level="9.1.2" data-path="week-08-exploratory-factor-analysis-part-2.html"><a href="week-08-exploratory-factor-analysis-part-2.html#preparatory-steps-2"><i class="fa fa-check"></i><b>9.1.2</b> Preparatory steps</a></li>
<li class="chapter" data-level="9.1.3" data-path="week-08-exploratory-factor-analysis-part-2.html"><a href="week-08-exploratory-factor-analysis-part-2.html#performing-the-factor-analysis-1"><i class="fa fa-check"></i><b>9.1.3</b> Performing the factor analysis</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="week-08-exploratory-factor-analysis-part-2.html"><a href="week-08-exploratory-factor-analysis-part-2.html#efa-with-categorical-data"><i class="fa fa-check"></i><b>9.2</b> EFA with categorical data</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="week-08-exploratory-factor-analysis-part-2.html"><a href="week-08-exploratory-factor-analysis-part-2.html#describing-categorical-data"><i class="fa fa-check"></i><b>9.2.1</b> Describing categorical data</a></li>
<li class="chapter" data-level="9.2.2" data-path="week-08-exploratory-factor-analysis-part-2.html"><a href="week-08-exploratory-factor-analysis-part-2.html#factor-scores"><i class="fa fa-check"></i><b>9.2.2</b> Factor scores</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="week-08-exploratory-factor-analysis-part-2.html"><a href="week-08-exploratory-factor-analysis-part-2.html#practice"><i class="fa fa-check"></i><b>9.3</b> Practice</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="week-08-exploratory-factor-analysis-part-2.html"><a href="week-08-exploratory-factor-analysis-part-2.html#summary-2"><i class="fa fa-check"></i><b>9.3.1</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="week-08-exploratory-factor-analysis-part-2.html"><a href="week-08-exploratory-factor-analysis-part-2.html#references-2"><i class="fa fa-check"></i><b>9.4</b> References</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="week-09-confirmatory-factor-analysis.html"><a href="week-09-confirmatory-factor-analysis.html"><i class="fa fa-check"></i><b>10</b> Week 09: Confirmatory Factor Analysis</a>
<ul>
<li class="chapter" data-level="10.1" data-path="week-09-confirmatory-factor-analysis.html"><a href="week-09-confirmatory-factor-analysis.html#preparatory-steps-3"><i class="fa fa-check"></i><b>10.1</b> Preparatory steps</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="week-09-confirmatory-factor-analysis.html"><a href="week-09-confirmatory-factor-analysis.html#describing-the-data"><i class="fa fa-check"></i><b>10.1.1</b> Describing the data</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="week-09-confirmatory-factor-analysis.html"><a href="week-09-confirmatory-factor-analysis.html#fitting-a-cfa-model"><i class="fa fa-check"></i><b>10.2</b> Fitting a CFA model</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="week-09-confirmatory-factor-analysis.html"><a href="week-09-confirmatory-factor-analysis.html#making-sure-the-model-was-correctly-estimated"><i class="fa fa-check"></i><b>10.2.1</b> Making sure the model was correctly estimated</a></li>
<li class="chapter" data-level="10.2.2" data-path="week-09-confirmatory-factor-analysis.html"><a href="week-09-confirmatory-factor-analysis.html#evaluating-the-model-fit"><i class="fa fa-check"></i><b>10.2.2</b> Evaluating the model fit</a></li>
<li class="chapter" data-level="10.2.3" data-path="week-09-confirmatory-factor-analysis.html"><a href="week-09-confirmatory-factor-analysis.html#interpretting-the-parameter-estimates"><i class="fa fa-check"></i><b>10.2.3</b> Interpretting the parameter estimates</a></li>
<li class="chapter" data-level="10.2.4" data-path="week-09-confirmatory-factor-analysis.html"><a href="week-09-confirmatory-factor-analysis.html#further-inspecting-our-model"><i class="fa fa-check"></i><b>10.2.4</b> Further inspecting our model</a></li>
<li class="chapter" data-level="10.2.5" data-path="week-09-confirmatory-factor-analysis.html"><a href="week-09-confirmatory-factor-analysis.html#further-inspecting-our-models-parameters"><i class="fa fa-check"></i><b>10.2.5</b> Further inspecting our model’s parameters</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="week-09-confirmatory-factor-analysis.html"><a href="week-09-confirmatory-factor-analysis.html#using-alternative-model-specifications"><i class="fa fa-check"></i><b>10.3</b> Using alternative model specifications</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="week-09-confirmatory-factor-analysis.html"><a href="week-09-confirmatory-factor-analysis.html#rescaling-the-factors-to-be-standardized"><i class="fa fa-check"></i><b>10.3.1</b> Rescaling the factors to be standardized</a></li>
<li class="chapter" data-level="10.3.2" data-path="week-09-confirmatory-factor-analysis.html"><a href="week-09-confirmatory-factor-analysis.html#specifying-a-competing-nested-model"><i class="fa fa-check"></i><b>10.3.2</b> Specifying a competing, nested, model</a></li>
<li class="chapter" data-level="10.3.3" data-path="week-09-confirmatory-factor-analysis.html"><a href="week-09-confirmatory-factor-analysis.html#conducting-model-comparisons"><i class="fa fa-check"></i><b>10.3.3</b> Conducting model comparisons</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="week-09-confirmatory-factor-analysis.html"><a href="week-09-confirmatory-factor-analysis.html#estimating-factor-scores"><i class="fa fa-check"></i><b>10.4</b> Estimating factor scores</a></li>
<li class="chapter" data-level="10.5" data-path="week-09-confirmatory-factor-analysis.html"><a href="week-09-confirmatory-factor-analysis.html#cfa-with-categorical-data"><i class="fa fa-check"></i><b>10.5</b> CFA with categorical data</a>
<ul>
<li class="chapter" data-level="10.5.1" data-path="week-09-confirmatory-factor-analysis.html"><a href="week-09-confirmatory-factor-analysis.html#using-data-from-an-r-package"><i class="fa fa-check"></i><b>10.5.1</b> Using data from an R package</a></li>
<li class="chapter" data-level="10.5.2" data-path="week-09-confirmatory-factor-analysis.html"><a href="week-09-confirmatory-factor-analysis.html#preparing-the-data"><i class="fa fa-check"></i><b>10.5.2</b> Preparing the data</a></li>
<li class="chapter" data-level="10.5.3" data-path="week-09-confirmatory-factor-analysis.html"><a href="week-09-confirmatory-factor-analysis.html#describing-the-data-1"><i class="fa fa-check"></i><b>10.5.3</b> Describing the data</a></li>
<li class="chapter" data-level="10.5.4" data-path="week-09-confirmatory-factor-analysis.html"><a href="week-09-confirmatory-factor-analysis.html#fitting-the-cfa-model"><i class="fa fa-check"></i><b>10.5.4</b> Fitting the CFA model</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="week-09-confirmatory-factor-analysis.html"><a href="week-09-confirmatory-factor-analysis.html#way-tldr-summary"><i class="fa fa-check"></i><b>10.6</b> Way TL;DR Summary</a></li>
<li class="chapter" data-level="10.7" data-path="week-09-confirmatory-factor-analysis.html"><a href="week-09-confirmatory-factor-analysis.html#references-3"><i class="fa fa-check"></i><b>10.7</b> References</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Guidebook for Using R in Applied Multivariate Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="week-07-exploratory-factor-analysis-part-1" class="section level1 hasAnchor" number="8">
<h1><span class="header-section-number">8</span> Week 07: Exploratory Factor Analysis, Part 1<a href="week-07-exploratory-factor-analysis-part-1.html#week-07-exploratory-factor-analysis-part-1" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="introductory-comments" class="section level2 hasAnchor" number="8.1">
<h2><span class="header-section-number">8.1</span> Introductory comments<a href="week-07-exploratory-factor-analysis-part-1.html#introductory-comments" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this handout, we will do the following:</p>
<ol style="list-style-type: decimal">
<li>Carry out the preparatory steps to factor analysis</li>
<li>Conduct a factor analysis with raw data and examine the output</li>
<li>conduct a factor analysis on correlation data</li>
</ol>
<div id="packages" class="section level3 hasAnchor" number="8.1.1">
<h3><span class="header-section-number">8.1.1</span> Packages<a href="week-07-exploratory-factor-analysis-part-1.html#packages" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this handout, we will use several packages.</p>
<div class="sourceCode" id="cb443"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb443-1"><a href="week-07-exploratory-factor-analysis-part-1.html#cb443-1" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb443-2"><a href="week-07-exploratory-factor-analysis-part-1.html#cb443-2" tabindex="-1"></a><span class="fu">library</span>(psych)</span>
<span id="cb443-3"><a href="week-07-exploratory-factor-analysis-part-1.html#cb443-3" tabindex="-1"></a><span class="fu">library</span>(car)</span>
<span id="cb443-4"><a href="week-07-exploratory-factor-analysis-part-1.html#cb443-4" tabindex="-1"></a><span class="fu">library</span>(mvnormtest)</span>
<span id="cb443-5"><a href="week-07-exploratory-factor-analysis-part-1.html#cb443-5" tabindex="-1"></a><span class="fu">library</span>(nFactors)</span>
<span id="cb443-6"><a href="week-07-exploratory-factor-analysis-part-1.html#cb443-6" tabindex="-1"></a><span class="fu">library</span>(EFA.MRFA)</span></code></pre></div>
<p>As is often the case with other operations in the R world, there exist different packages and functions for performing exploratory factor analysis (EFA). According to a review by Luo and colleagues <span class="citation">(<a href="#ref-luo_exploratory_2019">2019</a>)</span>, the most frequently used and comprehensive package for EFA appears to be the psych package <span class="citation">(<a href="#ref-R-psych">Revelle 2025</a>)</span>. Its <code>fa()</code> function can be used with raw data as well as with correlation or covariance matrices. The function looks like this:</p>
<div class="sourceCode" id="cb444"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb444-1"><a href="week-07-exploratory-factor-analysis-part-1.html#cb444-1" tabindex="-1"></a>psych<span class="sc">::</span><span class="fu">fa</span>(<span class="at">r =</span> , </span>
<span id="cb444-2"><a href="week-07-exploratory-factor-analysis-part-1.html#cb444-2" tabindex="-1"></a>          <span class="at">nfactors =</span> , </span>
<span id="cb444-3"><a href="week-07-exploratory-factor-analysis-part-1.html#cb444-3" tabindex="-1"></a>          <span class="at">n.obs =</span> ,</span>
<span id="cb444-4"><a href="week-07-exploratory-factor-analysis-part-1.html#cb444-4" tabindex="-1"></a>          <span class="at">fm =</span> , </span>
<span id="cb444-5"><a href="week-07-exploratory-factor-analysis-part-1.html#cb444-5" tabindex="-1"></a>          <span class="at">rotate =</span> )</span></code></pre></div>
<ul>
<li>The first argument, which can replace the <code>r =</code>, is for the data, whether it be a data frame of the raw data or a correlation or covariance matrix.</li>
<li>The <code>nfactors =</code> argument has as its default 1 but we should determine this number based on our preparatory steps, which we’ll do.</li>
<li>The <code>n.obs =</code> is required if we use a correlation or covariance matrix instead of raw data. If we use raw data, we exclude this argument.</li>
<li>The <code>fm =</code> argument is for the factor method. There are several options described in the help file. Two common ones are <code>fm = "pa"</code> for the <strong>p</strong>rincipal <strong>a</strong>xis method of factor extraction, and <code>fm = "ml"</code> for <strong>m</strong>aximum <strong>l</strong>ikelihood method of extraction, but others are available such as minimum residual solution, which is the default in this package.</li>
<li>The <code>rotate =</code> argument is for specifying the rotation method. There are several options listed in the help file, including <code>rotate = "none"</code>, which we can use in our preparatory steps, as well as <code>rotate = "promax"</code>, <code>"oblimin"</code>, and others for oblique rotations, which are frequently used if we assume that the constructs underlying the factors are related, as is often the case with studies in the social sciences. There are orthogonal rotations as well, including <code>"varimax"</code> and <code>"quartimax"</code>, and so forth. These are appropriate if we assume that the correlations between the factors should be zero.</li>
</ul>
<p>As usual, we can look in the help files to find other available arguments and options. In the examples in this handout, we will use promax rotation because we will assume that the constructs underlying the factors are related (though we may find evidence suggesting otherwise). We will also use principal axis factoring.</p>
</div>
<div id="efa-ne-pca" class="section level3 hasAnchor" number="8.1.2">
<h3><span class="header-section-number">8.1.2</span> EFA <span class="math inline">\(\ne\)</span> PCA<a href="week-07-exploratory-factor-analysis-part-1.html#efa-ne-pca" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Principal axis factoring (and any other EFA method) is <strong>not</strong> to be confused with principal components analysis (PCA), which strictly speaking is not a type of common factor analysis because it generates components rather than factors. Unlike factors, components include the unique variances of the observed variables. This is similar to how we calculate composite scores (on say a classroom test) by using a weighted sum of the variables—PCA finds the best possible weighted sum to explain the variance, including the error variance, in the set of data. PCA is appropriate when we are <strong>not</strong> interested in an underlying mental construct that explains the observed variables. When we assume that the factors <strong>do</strong> represent latent traits, common factor analysis is more appropriate than PCA because when we deal with factors, we assume that the factor is what is <strong>causing</strong> the observed scores to be high or low.</p>
</div>
</div>
<div id="preparatory-steps" class="section level2 hasAnchor" number="8.2">
<h2><span class="header-section-number">8.2</span> Preparatory steps<a href="week-07-exploratory-factor-analysis-part-1.html#preparatory-steps" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Here are some preparatory steps we should consider before jumping into a factor analysis.</p>
<ol style="list-style-type: decimal">
<li>Examine the data and the correlations</li>
<li>Address statistical assumptions</li>
<li>Determine the factorability of the data using the KMO and Bartlett tests</li>
<li>Identify the number of factors to retain for the factor analysis</li>
</ol>
<div id="examining-the-data" class="section level3 hasAnchor" number="8.2.1">
<h3><span class="header-section-number">8.2.1</span> Examining the data<a href="week-07-exploratory-factor-analysis-part-1.html#examining-the-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s import the data. This is a raw data set (rather than a covariance or correlation matrix), so each row represents a person. These data are described in the chapter. We have eight variables, each on a four-point Likert-type scale.</p>
<p><strong>Terms used to refer to the observed variables.</strong> In this handout, we call the observed variables <em>items</em> because they’re items on a survey. We can also call them <em>subtests</em> because items are smaller parts of a bigger test or survey.</p>
<div class="sourceCode" id="cb445"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb445-1"><a href="week-07-exploratory-factor-analysis-part-1.html#cb445-1" tabindex="-1"></a>raw <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;Week07_GSS_Science.csv&quot;</span>)</span>
<span id="cb445-2"><a href="week-07-exploratory-factor-analysis-part-1.html#cb445-2" tabindex="-1"></a><span class="fu">head</span>(raw)</span></code></pre></div>
<pre><code>##     PID NoIntrst Odd NoFun Boring Alone NoRelign Better Good Help
## 1 S0001        3   3     3      4     3        4      1    1    1
## 2 S0002        3   2     3      4     3        2      1    2    1
## 3 S0003        2   2     3      2     3        2      2    2    2
## 4 S0004        2   3     3      3     1        3      2    2    2
## 5 S0005        2   3     3      3     1        3      2    1    1
## 6 S0006        2   3     2      4     3        3      1    2    1</code></pre>
<p>We can also isolate our data frame to include only the columns with the observed data. In our data frame, we have an ID variable, <code>PID</code>, in the first column. So, we can use a <code>-1</code> in the column index to remove the first column and save our data to a new object, <code>dat</code>.</p>
<div class="sourceCode" id="cb447"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb447-1"><a href="week-07-exploratory-factor-analysis-part-1.html#cb447-1" tabindex="-1"></a>dat <span class="ot">&lt;-</span> raw[ , <span class="sc">-</span><span class="dv">1</span>] </span>
<span id="cb447-2"><a href="week-07-exploratory-factor-analysis-part-1.html#cb447-2" tabindex="-1"></a><span class="fu">head</span>(dat)</span></code></pre></div>
<pre><code>##   NoIntrst Odd NoFun Boring Alone NoRelign Better Good Help
## 1        3   3     3      4     3        4      1    1    1
## 2        3   2     3      4     3        2      1    2    1
## 3        2   2     3      2     3        2      2    2    2
## 4        2   3     3      3     1        3      2    2    2
## 5        2   3     3      3     1        3      2    1    1
## 6        2   3     2      4     3        3      1    2    1</code></pre>
<div id="descriptive-statistics-1" class="section level4 hasAnchor" number="8.2.1.1">
<h4><span class="header-section-number">8.2.1.1</span> Descriptive statistics<a href="week-07-exploratory-factor-analysis-part-1.html#descriptive-statistics-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Let’s look at the descriptive statistics using <code>describe()</code> from the psych package. We can see our usual descriptive statistics. It seems as though all of our variables are on similar scales, from 1 to 4, with the medians being integers, which makes sense because these are from Likert-type-scale items. Each has a skew and kurtosis within the -2 to +2 range.</p>
<div class="sourceCode" id="cb449"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb449-1"><a href="week-07-exploratory-factor-analysis-part-1.html#cb449-1" tabindex="-1"></a>psych<span class="sc">::</span><span class="fu">describe</span>(dat)</span></code></pre></div>
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
<em>n</em>
</th>
<th style="text-align:right;">
<em>M</em>
</th>
<th style="text-align:right;">
<em>sd</em>
</th>
<th style="text-align:right;">
Med
</th>
<th style="text-align:right;">
Min
</th>
<th style="text-align:right;">
Max
</th>
<th style="text-align:right;">
Skew
</th>
<th style="text-align:right;">
Kurtosis
</th>
<th style="text-align:right;">
<em>se<sub>M</sub></em>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;font-weight: bold;">
NoIntrst
</td>
<td style="text-align:right;">
371
</td>
<td style="text-align:right;">
2.67
</td>
<td style="text-align:right;">
0.64
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
-0.39
</td>
<td style="text-align:right;">
0.16
</td>
<td style="text-align:right;">
0.03
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;">
Odd
</td>
<td style="text-align:right;">
371
</td>
<td style="text-align:right;">
2.67
</td>
<td style="text-align:right;">
0.68
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
-0.47
</td>
<td style="text-align:right;">
0.19
</td>
<td style="text-align:right;">
0.04
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;">
NoFun
</td>
<td style="text-align:right;">
371
</td>
<td style="text-align:right;">
2.83
</td>
<td style="text-align:right;">
0.64
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
-0.53
</td>
<td style="text-align:right;">
0.79
</td>
<td style="text-align:right;">
0.03
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;">
Boring
</td>
<td style="text-align:right;">
371
</td>
<td style="text-align:right;">
2.96
</td>
<td style="text-align:right;">
0.60
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
-0.37
</td>
<td style="text-align:right;">
0.98
</td>
<td style="text-align:right;">
0.03
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;">
Alone
</td>
<td style="text-align:right;">
371
</td>
<td style="text-align:right;">
2.85
</td>
<td style="text-align:right;">
0.67
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
-0.70
</td>
<td style="text-align:right;">
1.08
</td>
<td style="text-align:right;">
0.03
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;">
NoRelign
</td>
<td style="text-align:right;">
371
</td>
<td style="text-align:right;">
2.61
</td>
<td style="text-align:right;">
0.69
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
-0.39
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
0.04
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;">
Better
</td>
<td style="text-align:right;">
371
</td>
<td style="text-align:right;">
1.90
</td>
<td style="text-align:right;">
0.51
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
0.09
</td>
<td style="text-align:right;">
1.71
</td>
<td style="text-align:right;">
0.03
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;">
Good
</td>
<td style="text-align:right;">
371
</td>
<td style="text-align:right;">
1.85
</td>
<td style="text-align:right;">
0.52
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
0.17
</td>
<td style="text-align:right;">
1.97
</td>
<td style="text-align:right;">
0.03
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;">
Help
</td>
<td style="text-align:right;">
371
</td>
<td style="text-align:right;">
1.76
</td>
<td style="text-align:right;">
0.46
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
-0.61
</td>
<td style="text-align:right;">
0.74
</td>
<td style="text-align:right;">
0.02
</td>
</tr>
</tbody>
</table>
<div id="correlation-matrix" class="section level5 hasAnchor" number="8.2.1.1.1">
<h5><span class="header-section-number">8.2.1.1.1</span> Correlation matrix<a href="week-07-exploratory-factor-analysis-part-1.html#correlation-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>We should take a look at the correlations among our variables to determine if factor analysis is appropriate. We expect at least some of these to differ from zero. Also, if there are any correlations that are unexpectedly very high in magnitude, such as close to 1 or -1, the two variables may be too collinear, indicating that they are potentially redundant and that we may seek to remove one or combine them. If we are writing a report, we might report this matrix in an appendix. Matrices like this allow future researchers to perform factor analyses on our correlation data if they wish to confirm the results with different model specifications or software.</p>
<p>We can also tentatively eyeball the correlations and see if they make sense given the theory (if one exists) or our general understanding of how we would expect the subtest scores to relate to one another in our population.</p>
<div class="sourceCode" id="cb450"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb450-1"><a href="week-07-exploratory-factor-analysis-part-1.html#cb450-1" tabindex="-1"></a>R <span class="ot">&lt;-</span> <span class="fu">cor</span>(dat)</span>
<span id="cb450-2"><a href="week-07-exploratory-factor-analysis-part-1.html#cb450-2" tabindex="-1"></a><span class="fu">round</span>(R,<span class="dv">3</span>)</span></code></pre></div>
Here is the prettified correlation matrix from these data.<a href="#fn36" class="footnote-ref" id="fnref36"><sup>36</sup></a>
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
NoIntrst
</th>
<th style="text-align:left;">
Odd
</th>
<th style="text-align:left;">
NoFun
</th>
<th style="text-align:left;">
Boring
</th>
<th style="text-align:left;">
Alone
</th>
<th style="text-align:left;">
NoRelign
</th>
<th style="text-align:left;">
Better
</th>
<th style="text-align:left;">
Good
</th>
<th style="text-align:left;">
Help
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;font-weight: bold;">
NoIntrst
</td>
<td style="text-align:left;">
1
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;">
Odd
</td>
<td style="text-align:left;">
.504
</td>
<td style="text-align:left;">
1
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;">
NoFun
</td>
<td style="text-align:left;">
.454
</td>
<td style="text-align:left;">
.388
</td>
<td style="text-align:left;">
1
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;">
Boring
</td>
<td style="text-align:left;">
.286
</td>
<td style="text-align:left;">
.303
</td>
<td style="text-align:left;">
.261
</td>
<td style="text-align:left;">
1
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;">
Alone
</td>
<td style="text-align:left;">
.351
</td>
<td style="text-align:left;">
.216
</td>
<td style="text-align:left;">
.260
</td>
<td style="text-align:left;">
.252
</td>
<td style="text-align:left;">
1
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;">
NoRelign
</td>
<td style="text-align:left;">
.292
</td>
<td style="text-align:left;">
.311
</td>
<td style="text-align:left;">
.264
</td>
<td style="text-align:left;">
.090
</td>
<td style="text-align:left;">
.096
</td>
<td style="text-align:left;">
1
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;">
Better
</td>
<td style="text-align:left;">
.123
</td>
<td style="text-align:left;">
.138
</td>
<td style="text-align:left;">
.113
</td>
<td style="text-align:left;">
-.136
</td>
<td style="text-align:left;">
.076
</td>
<td style="text-align:left;">
.067
</td>
<td style="text-align:left;">
1
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;">
Good
</td>
<td style="text-align:left;">
.049
</td>
<td style="text-align:left;">
.029
</td>
<td style="text-align:left;">
.046
</td>
<td style="text-align:left;">
-.131
</td>
<td style="text-align:left;">
.031
</td>
<td style="text-align:left;">
.051
</td>
<td style="text-align:left;">
.470
</td>
<td style="text-align:left;">
1
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;">
Help
</td>
<td style="text-align:left;">
.006
</td>
<td style="text-align:left;">
.027
</td>
<td style="text-align:left;">
.007
</td>
<td style="text-align:left;">
-.231
</td>
<td style="text-align:left;">
-.010
</td>
<td style="text-align:left;">
.046
</td>
<td style="text-align:left;">
.425
</td>
<td style="text-align:left;">
.402
</td>
<td style="text-align:left;">
1
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div id="addressing-assumptions" class="section level3 hasAnchor" number="8.2.2">
<h3><span class="header-section-number">8.2.2</span> Addressing assumptions<a href="week-07-exploratory-factor-analysis-part-1.html#addressing-assumptions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The assumptions for factor analysis are, not surprisingly, linearity and multivariate normality, along with absence of outliers <span class="citation">(<a href="#ref-tabachnick_using_2013">Tabachnick and Fidell 2013</a>)</span>.<a href="#fn37" class="footnote-ref" id="fnref37"><sup>37</sup></a> Linearity, outliers, and multivariate normality can all have an effect on the Pearson correlations that are analyzed and used in estimating the factor-analysis model. If these assumptions are not met, the legitimacy of our factor-analysis results can come into question.</p>
<p>We should also examine whether there is multicollinearity among the variables. Unlike PCA, which is actually one solution for dealing with multicollinearity, EFA will not arrive at a stable solution if there is extreme multicollinearity in the model. This is because it uses matrix inversion, which uses the determinant of the correlation matrix. If that determinant of the correlation matrix is zero or very close to zero, it’s like balancing a mountain on a pin-needle fulcrum.</p>
<div id="linearity" class="section level4 hasAnchor" number="8.2.2.1">
<h4><span class="header-section-number">8.2.2.1</span> Linearity<a href="week-07-exploratory-factor-analysis-part-1.html#linearity" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Linearity is perhaps the most salient assumption. The assumption is that the correlations among the observed variables and factors are linear. Non-linear relationships can result in misspecified correlations and therefore a misspecified factor-analysis model. We might consider examining scatter plots of all pairs of the observed variables. The Base R <code>plot()</code> function on the set of variables provides all possible bivariate scatterplots.</p>
<div class="sourceCode" id="cb451"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb451-1"><a href="week-07-exploratory-factor-analysis-part-1.html#cb451-1" tabindex="-1"></a><span class="fu">plot</span>(dat)</span></code></pre></div>
<p><img src="multivariate_stats_files/figure-html/unnamed-chunk-232-1.png" width="672" /></p>
<p>Because each of these dependent variables (which we’re calling subtests or items because they’re items in a survey) is on an ordinal scale with four points, it is difficult to tell if there are nonlinear relationships. Among the nine variables, it seems that <code>Help</code> <em>might</em> have some nonlinear relations with the other variables but it is hard to tell because of the ordinal nature of the data.</p>
<p>Note that with ordinal data having so few used points on the scale (in this case a maximum of four per variable), a garden-variety factor analysis will not be appropriate—we’ll practice dealing with that in the second part of our factor-analysis journey.</p>
<p>This brings up an important point: Linearity rests on the assumption that the observed variables are continuous; that is, that our data are on interval- or ratio-level scales. Categorical and ordered categorical data are <strong>not</strong> well suited to factor analysis. Generally, if our data have fewer than five functioning categories, we have a hard time claiming that our data are continuous.<a href="#fn38" class="footnote-ref" id="fnref38"><sup>38</sup></a></p>
</div>
<div id="outliers-and-multivariate-normality" class="section level4 hasAnchor" number="8.2.2.2">
<h4><span class="header-section-number">8.2.2.2</span> Outliers and multivariate normality<a href="week-07-exploratory-factor-analysis-part-1.html#outliers-and-multivariate-normality" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>If we do a complete analysis, we should examine both univariate and multivariate outliers. For the univariate outliers, we can use standardized scores as was done in previous lessons or box-and-whisker plots. For now, let’s focus on the multivariate outliers, as these are often overlooked in research.</p>
<p>Let’s examine the Mahalanobis distances to see if we can identify outliers and examine multivariate normality:</p>
<div class="sourceCode" id="cb452"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb452-1"><a href="week-07-exploratory-factor-analysis-part-1.html#cb452-1" tabindex="-1"></a>varbs <span class="ot">&lt;-</span> <span class="fu">cbind</span>(dat) <span class="co"># Note that in our data, we&#39;re using all of the columns</span></span>
<span id="cb452-2"><a href="week-07-exploratory-factor-analysis-part-1.html#cb452-2" tabindex="-1"></a>distances <span class="ot">&lt;-</span> <span class="fu">mahalanobis</span>(varbs, </span>
<span id="cb452-3"><a href="week-07-exploratory-factor-analysis-part-1.html#cb452-3" tabindex="-1"></a>                         <span class="at">center =</span> <span class="fu">colMeans</span>(varbs), </span>
<span id="cb452-4"><a href="week-07-exploratory-factor-analysis-part-1.html#cb452-4" tabindex="-1"></a>                         <span class="at">cov =</span> <span class="fu">cov</span>(varbs))</span>
<span id="cb452-5"><a href="week-07-exploratory-factor-analysis-part-1.html#cb452-5" tabindex="-1"></a>raw<span class="sc">$</span>distances <span class="ot">&lt;-</span> distances</span>
<span id="cb452-6"><a href="week-07-exploratory-factor-analysis-part-1.html#cb452-6" tabindex="-1"></a></span>
<span id="cb452-7"><a href="week-07-exploratory-factor-analysis-part-1.html#cb452-7" tabindex="-1"></a><span class="co"># Let&#39;s keep a record of those who exceed the p &lt; .001 criterion, if any:</span></span>
<span id="cb452-8"><a href="week-07-exploratory-factor-analysis-part-1.html#cb452-8" tabindex="-1"></a>raw<span class="sc">$</span>p <span class="ot">&lt;-</span> <span class="fu">pchisq</span>(distances,</span>
<span id="cb452-9"><a href="week-07-exploratory-factor-analysis-part-1.html#cb452-9" tabindex="-1"></a>                <span class="at">df =</span> (<span class="fu">ncol</span>(varbs)<span class="sc">-</span><span class="dv">1</span>), <span class="co"># df is number of variables - 1</span></span>
<span id="cb452-10"><a href="week-07-exploratory-factor-analysis-part-1.html#cb452-10" tabindex="-1"></a>                <span class="at">lower.tail =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p>We can see which cases in our data set are flagged as outliers. I think Base R is better here than tidyverse.</p>
<div class="sourceCode" id="cb453"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb453-1"><a href="week-07-exploratory-factor-analysis-part-1.html#cb453-1" tabindex="-1"></a><span class="co"># Let&#39;s use Base R to save a column that indicates whether the case is an outlier.</span></span>
<span id="cb453-2"><a href="week-07-exploratory-factor-analysis-part-1.html#cb453-2" tabindex="-1"></a>raw<span class="sc">$</span>outlier <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(raw<span class="sc">$</span>p <span class="sc">&lt;</span> .<span class="dv">001</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb453-3"><a href="week-07-exploratory-factor-analysis-part-1.html#cb453-3" tabindex="-1"></a></span>
<span id="cb453-4"><a href="week-07-exploratory-factor-analysis-part-1.html#cb453-4" tabindex="-1"></a><span class="co"># Let&#39;s print those rows for which their outlier status is true:</span></span>
<span id="cb453-5"><a href="week-07-exploratory-factor-analysis-part-1.html#cb453-5" tabindex="-1"></a>out.liars <span class="ot">&lt;-</span> raw[raw<span class="sc">$</span>outlier <span class="sc">==</span> <span class="dv">1</span>, <span class="fu">c</span>(<span class="st">&quot;PID&quot;</span>, <span class="st">&quot;distances&quot;</span>, <span class="st">&quot;p&quot;</span>, <span class="st">&quot;outlier&quot;</span>)]</span>
<span id="cb453-6"><a href="week-07-exploratory-factor-analysis-part-1.html#cb453-6" tabindex="-1"></a>out.liars</span></code></pre></div>
<pre><code>##       PID distances            p outlier
## 50  S0050  33.71409 4.576934e-05       1
## 64  S0064  35.36739 2.291685e-05       1
## 66  S0066  31.44294 1.171502e-04       1
## 79  S0079  29.36322 2.737579e-04       1
## 117 S0117  26.48447 8.674130e-04       1
## 124 S0124  29.33548 2.768522e-04       1
## 126 S0126  27.76881 5.202634e-04       1
## 136 S0136  42.43203 1.123463e-06       1
## 160 S0160  28.47775 3.914343e-04       1
## 181 S0181  37.24053 1.039369e-05       1
## 201 S0201  29.42374 2.671253e-04       1
## 248 S0248  43.53517 6.963654e-07       1
## 310 S0310  30.77297 1.541915e-04       1
## 322 S0322  26.71397 7.920251e-04       1
## 328 S0328  57.08836 1.730643e-09       1
## 331 S0331  38.88572 5.161244e-06       1
## 348 S0348  28.77977 3.465846e-04       1</code></pre>
<p>There are 17 multivariate outliers.</p>
<p>For convenience, we can sort those rows by their distances to see who are the most extreme outliers.<a href="#fn39" class="footnote-ref" id="fnref39"><sup>39</sup></a></p>
<div class="sourceCode" id="cb455"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb455-1"><a href="week-07-exploratory-factor-analysis-part-1.html#cb455-1" tabindex="-1"></a>out.liars <span class="sc">%&gt;%</span> </span>
<span id="cb455-2"><a href="week-07-exploratory-factor-analysis-part-1.html#cb455-2" tabindex="-1"></a>  <span class="fu">arrange</span>(<span class="fu">desc</span>(distances))</span></code></pre></div>
<pre><code>##      PID distances            p outlier
## 1  S0328  57.08836 1.730643e-09       1
## 2  S0248  43.53517 6.963654e-07       1
## 3  S0136  42.43203 1.123463e-06       1
## 4  S0331  38.88572 5.161244e-06       1
## 5  S0181  37.24053 1.039369e-05       1
## 6  S0064  35.36739 2.291685e-05       1
## 7  S0050  33.71409 4.576934e-05       1
## 8  S0066  31.44294 1.171502e-04       1
## 9  S0310  30.77297 1.541915e-04       1
## 10 S0201  29.42374 2.671253e-04       1
## 11 S0079  29.36322 2.737579e-04       1
## 12 S0124  29.33548 2.768522e-04       1
## 13 S0348  28.77977 3.465846e-04       1
## 14 S0160  28.47775 3.914343e-04       1
## 15 S0126  27.76881 5.202634e-04       1
## 16 S0322  26.71397 7.920251e-04       1
## 17 S0117  26.48447 8.674130e-04       1</code></pre>
<p>It seems like Person <code>S0328</code> is the biggest outlier, <code>S0248</code> is the second most outlying case, and so forth.</p>
<p>Let’s examine the multivariate Q-Q plot based on the Mahalanobis distances:<a href="#fn40" class="footnote-ref" id="fnref40"><sup>40</sup></a></p>
<div class="sourceCode" id="cb457"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb457-1"><a href="week-07-exploratory-factor-analysis-part-1.html#cb457-1" tabindex="-1"></a>car<span class="sc">::</span><span class="fu">qqPlot</span>(distances, </span>
<span id="cb457-2"><a href="week-07-exploratory-factor-analysis-part-1.html#cb457-2" tabindex="-1"></a>            <span class="at">distribution =</span> <span class="st">&quot;chisq&quot;</span>, <span class="at">df =</span> <span class="fu">mean</span>(distances), </span>
<span id="cb457-3"><a href="week-07-exploratory-factor-analysis-part-1.html#cb457-3" tabindex="-1"></a>            <span class="at">lwd =</span> <span class="dv">1</span>, </span>
<span id="cb457-4"><a href="week-07-exploratory-factor-analysis-part-1.html#cb457-4" tabindex="-1"></a>            <span class="at">grid =</span> <span class="cn">FALSE</span>, </span>
<span id="cb457-5"><a href="week-07-exploratory-factor-analysis-part-1.html#cb457-5" tabindex="-1"></a>            <span class="at">main =</span> <span class="st">&quot;Multi-normal Q-Q Plot&quot;</span>, </span>
<span id="cb457-6"><a href="week-07-exploratory-factor-analysis-part-1.html#cb457-6" tabindex="-1"></a>            <span class="at">xlab =</span> <span class="fu">expression</span>(chi<span class="sc">^</span><span class="dv">2</span> <span class="sc">*</span> <span class="st">&quot; quantiles&quot;</span>), </span>
<span id="cb457-7"><a href="week-07-exploratory-factor-analysis-part-1.html#cb457-7" tabindex="-1"></a>            <span class="at">ylab =</span> <span class="fu">expression</span>(<span class="st">&quot;Mahalanobis distances &quot;</span><span class="sc">^</span><span class="dv">2</span>))</span></code></pre></div>
<p><img src="multivariate_stats_files/figure-html/unnamed-chunk-236-1.png" width="672" /></p>
<pre><code>## [1] 328 248</code></pre>
<p>If we are expected to also report a statistical test of normality, we can use the mulitvariate Shapiro-Wilk test that we used in Week 3. The <code>mshapiro.test()</code> function from the mvnormtest package <span class="citation">(<a href="#ref-R-mvnormtest">Jarek 2024</a>)</span>. This function requires the data be arranged as rows instead of columns, so we’ll use the transpose function, <code>t()</code> within the test.</p>
<div class="sourceCode" id="cb459"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb459-1"><a href="week-07-exploratory-factor-analysis-part-1.html#cb459-1" tabindex="-1"></a><span class="fu">library</span>(mvnormtest)</span>
<span id="cb459-2"><a href="week-07-exploratory-factor-analysis-part-1.html#cb459-2" tabindex="-1"></a><span class="fu">mshapiro.test</span>( <span class="fu">t</span>(dat) )</span></code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  Z
## W = 0.92551, p-value = 1.251e-12</code></pre>
<p>We see results that are consistent with our outliers and Mahalanobis distance Q-Q plot. The Shapiro-Wilk normality test (<em>W</em> = 0.93, <em>p</em> = &lt; .01) suggests the data are statistically significantly different from a normal distribution. This collection of evidence suggests that the multivariate normality assumption was not met.</p>
<div id="what-can-we-do" class="section level5 hasAnchor" number="8.2.2.2.1">
<h5><span class="header-section-number">8.2.2.2.1</span> What can we do<a href="week-07-exploratory-factor-analysis-part-1.html#what-can-we-do" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>We have some multivariate outliers. If our objective is to obtain a factor model to serve as evidence in appraising the validity of our score interpretations with this instrument as it is used with the broader population, we might opt to remove these observations before fitting the factor-analysis model. Some scholars may insist we retain outlying observations because they are indeed members of the population; nonetheless, they can have stronger effects on the model estimates than their non-outlying counterparts. On the other hand, if we intend to use the EFA to estimate these observations’ scores, we have little choice but to leave them in the data. Alternatively, we could take a sensitivity-analysis approach by comparing the results of two factor-analysis models, one with these cases removed and one with them included. If the results are arguably similar—such that we reach the same conclusions about the internal structure—we might then decide to retain our outliers as we could then report the results with greater confidence. Whatever our decision is, we should document what we have done. If we retain these outliers and do not perform that sensitivity analysis, we should report that the results of our factor analysis need to be taken with caution.</p>
<p>Because our data are not normally distributed, we should avoid using maximum likelihood as the factor extraction method. Furthermore, we should report that our data are not likely normally distributed but that we proceeded with the factor analysis anyway. We could also offer a qualification such as “to the extent that normality fails, the solution is degraded but may still be worthwhile” <span class="citation">(<a href="#ref-tabachnick_using_2013">Tabachnick and Fidell 2013, 618</a>)</span>.</p>
</div>
</div>
<div id="absence-of-perfect-multicollinearity" class="section level4 hasAnchor" number="8.2.2.3">
<h4><span class="header-section-number">8.2.2.3</span> Absence of perfect multicollinearity<a href="week-07-exploratory-factor-analysis-part-1.html#absence-of-perfect-multicollinearity" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>If we’re examining a correlation matrix and find two variables correlate very strongly, such as <em>r</em> = .97, this suggests there is some degree of mutlicollinearity. However, we should also consider that multicollinearity can go unnoticed in an eyeballing of the correlation matrix because it can be among combinations of variables. We can go back to our earlier studies in multiple regression and examine the squared multiple correlation (SMC) of each variable when we regress it on all the other variables. We can also examine the variance inflation factor (VIF), which is calculated from the SMC. With the SMC, if we find an SMC greater than .90 or .95, we have diagnosed multicollinearity. With VIF &gt; 10, which corresponds to an SMC of .90, we can also flag multicollinearity.</p>
<p>For EFA, the computational issue has to do with whether we can estimate a factor-analytic structure from the data. If we have perfect multicollinearity in our data set, we will have what is called a <em>singular matrix</em> and a determinant of zero. In that case, we can<strong>not</strong> trust the results of the EFA model.</p>
<p>With this, an easy way to assess whether there is perfect multicollinearity is to calculate the determinant of the correlation matrix. If that is zero, we have perfect multicollinearity. If it is very close to zero, we should take a look at our data and consider asking why this multicollinearity is present. Let’s use the <code>det()</code> function:</p>
<div class="sourceCode" id="cb461"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb461-1"><a href="week-07-exploratory-factor-analysis-part-1.html#cb461-1" tabindex="-1"></a><span class="fu">det</span>(R)</span></code></pre></div>
<pre><code>## [1] 0.1918348</code></pre>
<p>The determinant of the correlation matrix is positive and not very close to zero, so we’re in good shape. We can report something like “We calculated the determinant of the correlation matrix and found it to be positive, suggesting that there will not be problems in estimating the EFA model due to perfect multicollinearity.”</p>
<p>If there is perfect multicollinearity, we should remove the variable that is perfectly linearly related with the others, as it is redundant. When strong multicollinearity is present, we have to use our judgment to determine whether the redundant variable(s) are legitimate for our theory or expectations about the data. If we find strong multicollinearity and we want to avoid their overrepresentation in the EFA mdoel, we can replace the set of multicollinear varaibles with a composite of them. For example, if we have three variables that are multicollinear, we can fit a PCA on those three variables and save each observation’s component score, then use that component score in our EFA model.</p>
<p><strong>What’s it look like when multicollinearity is present?</strong></p>
<p>To demonstrate what the determinant and EFA results would look like if we had perfect multicollinearity, let’s create a temporary data frame, <code>bad</code>, and create a new variable, <code>GoodHelp</code> that is a linear transformation of two of the existing variables (<code>Good</code> and <code>Help</code>), and calculate the determinant of that correlation matrix.</p>
<div class="sourceCode" id="cb463"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb463-1"><a href="week-07-exploratory-factor-analysis-part-1.html#cb463-1" tabindex="-1"></a>bad       <span class="ot">&lt;-</span> dat</span>
<span id="cb463-2"><a href="week-07-exploratory-factor-analysis-part-1.html#cb463-2" tabindex="-1"></a>bad<span class="sc">$</span>GoodHelp <span class="ot">&lt;-</span> (bad<span class="sc">$</span>Help <span class="sc">*</span> .<span class="dv">5</span>) <span class="sc">+</span> (bad<span class="sc">$</span>Good <span class="sc">*</span> .<span class="dv">5</span>)</span>
<span id="cb463-3"><a href="week-07-exploratory-factor-analysis-part-1.html#cb463-3" tabindex="-1"></a>Rbad      <span class="ot">&lt;-</span> <span class="fu">cor</span>(bad)</span></code></pre></div>
<p>Notice that we might not observe this perfect multicollinearity in the correlation matrix. Our new variable correlates .81 and .86 with the two variables from which we created it.</p>
<div class="sourceCode" id="cb464"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb464-1"><a href="week-07-exploratory-factor-analysis-part-1.html#cb464-1" tabindex="-1"></a><span class="fu">round</span>(Rbad, <span class="dv">2</span>)[<span class="dv">8</span><span class="sc">:</span><span class="dv">10</span>, <span class="dv">8</span><span class="sc">:</span><span class="dv">10</span>]</span></code></pre></div>
<pre><code>##          Good Help GoodHelp
## Good     1.00 0.40     0.86
## Help     0.40 1.00     0.81
## GoodHelp 0.86 0.81     1.00</code></pre>
<p>However, the determinant is zero:</p>
<div class="sourceCode" id="cb466"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb466-1"><a href="week-07-exploratory-factor-analysis-part-1.html#cb466-1" tabindex="-1"></a><span class="fu">det</span>(Rbad)</span></code></pre></div>
<pre><code>## [1] 0</code></pre>
<p>If we fit an EFA model, we’ll get errors that say the matrix was not positive definite or that the correlation matrix was singular. Let’s try this and see what happens:</p>
<div class="sourceCode" id="cb468"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb468-1"><a href="week-07-exploratory-factor-analysis-part-1.html#cb468-1" tabindex="-1"></a>fa.bad <span class="ot">&lt;-</span> psych<span class="sc">::</span><span class="fu">fa</span>(bad, </span>
<span id="cb468-2"><a href="week-07-exploratory-factor-analysis-part-1.html#cb468-2" tabindex="-1"></a>          <span class="at">nfactors =</span> <span class="fu">ncol</span>(bad), </span>
<span id="cb468-3"><a href="week-07-exploratory-factor-analysis-part-1.html#cb468-3" tabindex="-1"></a>          <span class="at">rotate =</span> <span class="st">&quot;none&quot;</span>)</span></code></pre></div>
<pre><code>## In smc, smcs &lt; 0 were set to .0
## In smc, smcs &lt; 0 were set to .0
## In smc, smcs &lt; 0 were set to .0</code></pre>
<pre><code>## In factor.scores, the correlation matrix is singular, the pseudo inverse is  used</code></pre>
<p>Notice the warnings that the matrix was not positive definite or that the correlation matrix was singular. When we conduct an EFA (or CFA) and we have those kinds of messages, we should <strong>not</strong> trust the results.</p>
<p>Sometimes, multicollinearity can go unnoticed if we only rely on the EFA output. Here, let’s again create a second version of our multicollinear variable, but this time make <code>GoodHelp</code> have extreme collinearity instead of perfect collinearity.</p>
<div class="sourceCode" id="cb471"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb471-1"><a href="week-07-exploratory-factor-analysis-part-1.html#cb471-1" tabindex="-1"></a><span class="co"># This is just for demonstration</span></span>
<span id="cb471-2"><a href="week-07-exploratory-factor-analysis-part-1.html#cb471-2" tabindex="-1"></a>bad       <span class="ot">&lt;-</span> dat</span>
<span id="cb471-3"><a href="week-07-exploratory-factor-analysis-part-1.html#cb471-3" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1122</span>)</span>
<span id="cb471-4"><a href="week-07-exploratory-factor-analysis-part-1.html#cb471-4" tabindex="-1"></a><span class="co"># Creating a tiny random number for each row of the data frame:</span></span>
<span id="cb471-5"><a href="week-07-exploratory-factor-analysis-part-1.html#cb471-5" tabindex="-1"></a>tiny.rand.num <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="fu">nrow</span>(bad), <span class="dv">0</span>, .<span class="dv">005</span>) </span>
<span id="cb471-6"><a href="week-07-exploratory-factor-analysis-part-1.html#cb471-6" tabindex="-1"></a><span class="co"># Creating a variable with very strong, but not perfect, collinearity:</span></span>
<span id="cb471-7"><a href="week-07-exploratory-factor-analysis-part-1.html#cb471-7" tabindex="-1"></a>bad<span class="sc">$</span>GoodHelp <span class="ot">&lt;-</span>(bad<span class="sc">$</span>Help <span class="sc">*</span> .<span class="dv">5</span>) <span class="sc">+</span> (bad<span class="sc">$</span>Good <span class="sc">*</span> .<span class="dv">5</span>) <span class="sc">+</span> tiny.rand.num </span>
<span id="cb471-8"><a href="week-07-exploratory-factor-analysis-part-1.html#cb471-8" tabindex="-1"></a>Rbad      <span class="ot">&lt;-</span> <span class="fu">cor</span>(bad)</span>
<span id="cb471-9"><a href="week-07-exploratory-factor-analysis-part-1.html#cb471-9" tabindex="-1"></a><span class="fu">det</span>(Rbad)</span></code></pre></div>
<pre><code>## [1] 2.930893e-05</code></pre>
<p>The determinant is very small. If this is the denominator of a fraction (e.g., when we take the inverse of a matrix), the result will be a very big number.</p>
<p>If we conducted EFA on this output, we might not see the error but we will notice that the collinear variable and the two from which it came seem to overrepresent the first factor. This can result in a biased interpretation of what that factors mean; we would need to examine our theory of how we expect these items (or subtests) to be related to each other in our decision about how to deal with this strong multicollinearity. If we can remove the redundant variable, we should.</p>
<div class="sourceCode" id="cb473"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb473-1"><a href="week-07-exploratory-factor-analysis-part-1.html#cb473-1" tabindex="-1"></a>fa.bad2 <span class="ot">&lt;-</span> psych<span class="sc">::</span><span class="fu">fa</span>(bad, <span class="at">nfactors =</span> <span class="fu">ncol</span>(bad), <span class="at">rotate =</span> <span class="st">&quot;none&quot;</span>)</span>
<span id="cb473-2"><a href="week-07-exploratory-factor-analysis-part-1.html#cb473-2" tabindex="-1"></a>fa.bad2<span class="sc">$</span>loadings</span></code></pre></div>
<pre><code>## 
## Loadings:
##          MR1    MR2    MR3    MR4    MR5    MR6    MR7    MR8    MR9    MR10  
## NoIntrst  0.130  0.749                      -0.191 -0.239  0.135 -0.238       
## Odd       0.130  0.726 -0.163                0.385 -0.212 -0.154  0.104       
## NoFun     0.115  0.643                      -0.275  0.233 -0.330              
## Boring   -0.225  0.562  0.411  0.424         0.186  0.180                     
## Alone            0.430  0.119  0.109  0.173 -0.237 -0.111  0.169  0.279       
## NoRelign  0.119  0.413 -0.230 -0.204 -0.321         0.274  0.263              
## Better    0.663               -0.219  0.504  0.174  0.161                     
## Good      0.829         0.469 -0.211 -0.193                                   
## Help      0.800 -0.137 -0.381  0.436                                          
## GoodHelp  0.973 -0.111         0.110 -0.132                                   
## 
##                  MR1   MR2   MR3   MR4   MR5   MR6   MR7   MR8   MR9  MR10
## SS loadings    2.830 2.213 0.643 0.538 0.449 0.385 0.306 0.258 0.153 0.000
## Proportion Var 0.283 0.221 0.064 0.054 0.045 0.038 0.031 0.026 0.015 0.000
## Cumulative Var 0.283 0.504 0.569 0.622 0.667 0.706 0.736 0.762 0.778 0.778</code></pre>
<p><strong>If we need to report the VIF</strong></p>
<p>If our reviewers demand we report the variance inflation factors, we can use the <code>vif()</code> function from the car package <span class="citation">(<a href="#ref-R-car">Fox, Weisberg, and Price 2024</a>)</span>. However, this approach assumes we have a regression model or an ANOVA, whereas in EFA, we merely have a bunch of observed variables. A workaround is to use a meaningless numeric variable, such as ID number or case number, on the set of variables and then use the <code>vif()</code> function on that model output. Those variables that exceed 10 are flagged as multicollinear—in our temporary, bad, data set, the last three variables clearly exceed this:</p>
<div class="sourceCode" id="cb475"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb475-1"><a href="week-07-exploratory-factor-analysis-part-1.html#cb475-1" tabindex="-1"></a>bad<span class="sc">$</span>casenum <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(bad)</span>
<span id="cb475-2"><a href="week-07-exploratory-factor-analysis-part-1.html#cb475-2" tabindex="-1"></a>lm.for.vif <span class="ot">&lt;-</span> <span class="fu">lm</span>(casenum <span class="sc">~</span>  NoIntrst <span class="sc">+</span> Odd <span class="sc">+</span> NoFun <span class="sc">+</span> Boring <span class="sc">+</span> Alone <span class="sc">+</span> NoRelign <span class="sc">+</span> Better <span class="sc">+</span> Good <span class="sc">+</span> Help <span class="sc">+</span> GoodHelp, <span class="at">data =</span> bad)   </span>
<span id="cb475-3"><a href="week-07-exploratory-factor-analysis-part-1.html#cb475-3" tabindex="-1"></a>car<span class="sc">::</span><span class="fu">vif</span>( lm.for.vif)</span></code></pre></div>
<pre><code>##    NoIntrst         Odd       NoFun      Boring       Alone    NoRelign      Better        Good 
##    1.635752    1.509177    1.375369    1.278744    1.190307    1.160450    1.464816 2638.768269 
##        Help    GoodHelp 
## 2047.808400 6545.267350</code></pre>
<p>Back to our actual data, without the <code>GoodHelp</code> variable, none of our variables exceeds this VIF &gt; 10 criterion, so we can conclude that multicollinearity is not a problem:</p>
<div class="sourceCode" id="cb477"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb477-1"><a href="week-07-exploratory-factor-analysis-part-1.html#cb477-1" tabindex="-1"></a>raw<span class="sc">$</span>casenum <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(raw)</span>
<span id="cb477-2"><a href="week-07-exploratory-factor-analysis-part-1.html#cb477-2" tabindex="-1"></a>lm.for.vif <span class="ot">&lt;-</span> <span class="fu">lm</span>(casenum <span class="sc">~</span>  NoIntrst <span class="sc">+</span> Odd <span class="sc">+</span> NoFun <span class="sc">+</span> Boring <span class="sc">+</span> Alone <span class="sc">+</span> NoRelign <span class="sc">+</span> Better <span class="sc">+</span> Good <span class="sc">+</span> Help, <span class="at">data =</span> raw)   </span>
<span id="cb477-3"><a href="week-07-exploratory-factor-analysis-part-1.html#cb477-3" tabindex="-1"></a>car<span class="sc">::</span><span class="fu">vif</span>( lm.for.vif)</span></code></pre></div>
<pre><code>## NoIntrst      Odd    NoFun   Boring    Alone NoRelign   Better     Good     Help 
## 1.630878 1.508602 1.374342 1.276896 1.190307 1.159588 1.457194 1.374892 1.349730</code></pre>
</div>
</div>
<div id="determining-factorability" class="section level3 hasAnchor" number="8.2.3">
<h3><span class="header-section-number">8.2.3</span> Determining factorability<a href="week-07-exploratory-factor-analysis-part-1.html#determining-factorability" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The Kaiser-Meyer-Olkin (KMO) statistic predicts if data are likely to factor well given the correlations and partial correlations. The statistic, ranging from 0 to 1, roughly estimates the proportion of variance in the data that might be explained by factors. If we use Kaiser’s <span class="citation">(<a href="#ref-kaiser_index_1974">1974</a>)</span> guidelines, a suggested cutoff for determining factorability of the sample data is <span class="math inline">\(\text{KMO} \geq .60\)</span>.</p>
<div class="sourceCode" id="cb479"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb479-1"><a href="week-07-exploratory-factor-analysis-part-1.html#cb479-1" tabindex="-1"></a>psych<span class="sc">::</span><span class="fu">KMO</span>(dat)</span></code></pre></div>
<pre><code>## Kaiser-Meyer-Olkin factor adequacy
## Call: psych::KMO(r = dat)
## Overall MSA =  0.75
## MSA for each item = 
## NoIntrst      Odd    NoFun   Boring    Alone NoRelign   Better     Good     Help 
##     0.76     0.76     0.82     0.77     0.79     0.81     0.68     0.68     0.71</code></pre>
<p>The total KMO is 0.75, indicating that, based on this test, we can probably conduct a factor analysis.</p>
<p>With the Bartlett’s sphericity test, the null hypothesis is that the correlation matrix of the sample data comes from a population in which the variables have zero correlations with each other. In other words, the correlation matrix in the population is an identity matrix. Since we <strong>do</strong> need a set of correlated variables to perform a factor analysis, we want H0 to be rejected.</p>
<div class="sourceCode" id="cb481"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb481-1"><a href="week-07-exploratory-factor-analysis-part-1.html#cb481-1" tabindex="-1"></a>psych<span class="sc">::</span><span class="fu">cortest.bartlett</span>(dat)</span></code></pre></div>
<pre><code>## $chisq
## [1] 604.5854
## 
## $p.value
## [1] 2.276163e-104
## 
## $df
## [1] 36</code></pre>
<p>The test suggests we can reject the null hypothesis that the data are not collinear. This is no surprise to us if we saw in our correlation matrix that there are at least some moderate correlations in our data. Nonetheless, given this and the results of the KMO test, we can proceed with factor analysis.</p>
</div>
<div id="determining-how-many-factors-to-retain" class="section level3 hasAnchor" number="8.2.4">
<h3><span class="header-section-number">8.2.4</span> Determining how many factors to retain<a href="week-07-exploratory-factor-analysis-part-1.html#determining-how-many-factors-to-retain" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>It is important to use multiple methods to determine how many factors to retain. Some researchers (not I) use the eigenvalue-greater-than-one approach, also called the K1 criterion named after the Kaiser’s <span class="citation">(<a href="#ref-kaiser_index_1974">1974</a>)</span> work, to determining the number of factors to retain. It is important to note that this method was developed for principal components analysis rather than factor analysis. If you do use that approach, interpret the results with caution. It is a good idea to accompany that method with at least one of these other three methods.</p>
<div id="parallel-analysis-1" class="section level4 hasAnchor" number="8.2.4.1">
<h4><span class="header-section-number">8.2.4.1</span> Parallel analysis<a href="week-07-exploratory-factor-analysis-part-1.html#parallel-analysis-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We used the parallel analysis <span class="citation">(<a href="#ref-horn_rationale_1965">Horn 1965</a>)</span> in the unit on PCA. It provides a method for comparing our data to randomly generated data sets having the same number of persons and subtests. After ranking these randomly generated data from low to high, the procedure can estimate the eigenvalue at each <span class="math inline">\(n\)</span>th factor at the 95 percentile level. We compare the eigenvalues calculated from our observed data with those of the randomly generated data. If with a given model, such as one with <span class="math inline">\(n\)</span> factors, 95% of the randomly drawn samples have eigenvalues that exceed what we observe in our data, we can assume that the factor structure in our data lies outside that 95 percent range (in other words, our eigenvalues do not likely to occur by dumb luck). If this occurs at the <span class="math inline">\(n\)</span>th factor, we can assume that the earlier number of factors explained more of the common variance than would occur by chance.</p>
<p>We can use the <code>parallel()</code> function from the nFactors package <span class="citation">(<a href="#ref-R-nFactors">Raiche and Magis 2025</a>)</span> to perform a parallel analysis. The parallel analysis takes random draws of data and estimates the eigenvalues that would occur randomly in each random draw. The mean of the random draws is reported, along with the 95 percentile. We want to use the 95 percentile.</p>
<p>The first step is to compute the eigenvalues. The eigenvalues in the parallel analysis with factor analysis are from the <strong>reduced</strong> correlation matrix, meaning that the diagonal of the correlation matrix excludes the subtests’ unique variances. To distinguish these eigenvalues from their counterparts from an unreduced matrix—the <em>initial eigenvalues</em>—we can labels these as <em>reduced</em> or <em>extracted</em> eigenvalues.</p>
<div class="sourceCode" id="cb483"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb483-1"><a href="week-07-exploratory-factor-analysis-part-1.html#cb483-1" tabindex="-1"></a><span class="fu">library</span>(nFactors)</span>
<span id="cb483-2"><a href="week-07-exploratory-factor-analysis-part-1.html#cb483-2" tabindex="-1"></a><span class="co"># help(package=&quot;nFactors&quot;)</span></span>
<span id="cb483-3"><a href="week-07-exploratory-factor-analysis-part-1.html#cb483-3" tabindex="-1"></a>n_p  <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">complete.cases</span>(dat)) <span class="co"># The number of persons in our data</span></span>
<span id="cb483-4"><a href="week-07-exploratory-factor-analysis-part-1.html#cb483-4" tabindex="-1"></a>n_nu <span class="ot">&lt;-</span> <span class="fu">ncol</span>(dat) <span class="co"># The number of variables in our data</span></span>
<span id="cb483-5"><a href="week-07-exploratory-factor-analysis-part-1.html#cb483-5" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)     <span class="co"># To reproduce our randomly generated results.</span></span>
<span id="cb483-6"><a href="week-07-exploratory-factor-analysis-part-1.html#cb483-6" tabindex="-1"></a>ReducedEig <span class="ot">&lt;-</span> <span class="fu">eigenComputes</span>(dat, <span class="at">model =</span> <span class="st">&quot;factors&quot;</span>, <span class="at">use =</span> <span class="st">&quot;complete&quot;</span>)</span>
<span id="cb483-7"><a href="week-07-exploratory-factor-analysis-part-1.html#cb483-7" tabindex="-1"></a>n_factors  <span class="ot">&lt;-</span> <span class="fu">length</span>(ReducedEig)</span>
<span id="cb483-8"><a href="week-07-exploratory-factor-analysis-part-1.html#cb483-8" tabindex="-1"></a>paral <span class="ot">&lt;-</span> <span class="fu">parallel</span>(<span class="at">subject =</span> n_p,  </span>
<span id="cb483-9"><a href="week-07-exploratory-factor-analysis-part-1.html#cb483-9" tabindex="-1"></a>                      <span class="at">var =</span> n_nu, </span>
<span id="cb483-10"><a href="week-07-exploratory-factor-analysis-part-1.html#cb483-10" tabindex="-1"></a>                      <span class="at">rep =</span> <span class="dv">100</span>,</span>
<span id="cb483-11"><a href="week-07-exploratory-factor-analysis-part-1.html#cb483-11" tabindex="-1"></a>                 <span class="at">quantile =</span> .<span class="dv">95</span>, </span>
<span id="cb483-12"><a href="week-07-exploratory-factor-analysis-part-1.html#cb483-12" tabindex="-1"></a>                   <span class="at">model  =</span> <span class="st">&quot;factors&quot;</span>)</span>
<span id="cb483-13"><a href="week-07-exploratory-factor-analysis-part-1.html#cb483-13" tabindex="-1"></a></span>
<span id="cb483-14"><a href="week-07-exploratory-factor-analysis-part-1.html#cb483-14" tabindex="-1"></a>ParallelAna <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">Nfactor  =</span> <span class="dv">1</span><span class="sc">:</span>n_factors,</span>
<span id="cb483-15"><a href="week-07-exploratory-factor-analysis-part-1.html#cb483-15" tabindex="-1"></a>                           ReducedEig,</span>
<span id="cb483-16"><a href="week-07-exploratory-factor-analysis-part-1.html#cb483-16" tabindex="-1"></a>                           <span class="at">RandEigM =</span> paral<span class="sc">$</span>eigen<span class="sc">$</span>mevpea,</span>
<span id="cb483-17"><a href="week-07-exploratory-factor-analysis-part-1.html#cb483-17" tabindex="-1"></a>                           <span class="at">RandEig95=</span> paral<span class="sc">$</span>eigen<span class="sc">$</span>qevpea)</span>
<span id="cb483-18"><a href="week-07-exploratory-factor-analysis-part-1.html#cb483-18" tabindex="-1"></a>ParallelAna <span class="ot">&lt;-</span> <span class="fu">round</span>(ParallelAna, <span class="dv">3</span>)</span>
<span id="cb483-19"><a href="week-07-exploratory-factor-analysis-part-1.html#cb483-19" tabindex="-1"></a>ParallelAna</span>
<span id="cb483-20"><a href="week-07-exploratory-factor-analysis-part-1.html#cb483-20" tabindex="-1"></a><span class="co"># write.csv(ParallelAna,&quot;ParallelAnalysis.csv&quot;,row.names = FALSE)</span></span></code></pre></div>
<pre><code>##   Nfactor ReducedEig RandEigM RandEig95
## 1       1      1.816    0.268     0.346
## 2       2      1.227    0.181     0.237
## 3       3      0.127    0.119     0.171
## 4       4     -0.047    0.064     0.108
## 5       5     -0.090    0.013     0.050
## 6       6     -0.114   -0.031    -0.002
## 7       7     -0.148   -0.083    -0.047
## 8       8     -0.179   -0.136    -0.100
## 9       9     -0.237   -0.202    -0.160</code></pre>
<p>In our output, we can use the rightmost column and identify at what point 95% of the randomly drawn data’s eigenvalues exceed our reduced-eigenvalue estimates. We see that it is at factor number <span class="math inline">\(3\)</span> that our observed reduced eigenvalue (<span class="math inline">\(0.127\)</span>) is exceeded by the randomly generated eigenvalue (<span class="math inline">\(0.171\)</span>). Based on this, we step down to one factor below this and decide we should retain <span class="math inline">\(2\)</span> factors.<a href="#fn41" class="footnote-ref" id="fnref41"><sup>41</sup></a></p>
<div id="scree-plot-1" class="section level5 hasAnchor" number="8.2.4.1.1">
<h5><span class="header-section-number">8.2.4.1.1</span> Scree plot<a href="week-07-exploratory-factor-analysis-part-1.html#scree-plot-1" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>We can use the objects we created in the parallel analysis to plot the factor numbers and eigenvalues (or reduced eigenvalues, in this case). This is a scree plot. The term comes from boulders falling down a cliff and collecting at the bottom.</p>
<div class="sourceCode" id="cb485"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb485-1"><a href="week-07-exploratory-factor-analysis-part-1.html#cb485-1" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb485-2"><a href="week-07-exploratory-factor-analysis-part-1.html#cb485-2" tabindex="-1"></a>scree <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">Factor_n =</span> <span class="fu">as.factor</span>(<span class="dv">1</span><span class="sc">:</span>n_factors), </span>
<span id="cb485-3"><a href="week-07-exploratory-factor-analysis-part-1.html#cb485-3" tabindex="-1"></a>                  <span class="at">Eigenvalue =</span> ReducedEig)</span>
<span id="cb485-4"><a href="week-07-exploratory-factor-analysis-part-1.html#cb485-4" tabindex="-1"></a><span class="fu">ggplot</span>(scree, <span class="fu">aes</span>(<span class="at">x =</span> Factor_n, <span class="at">y =</span> Eigenvalue, <span class="at">group =</span> <span class="dv">1</span>)) <span class="sc">+</span> </span>
<span id="cb485-5"><a href="week-07-exploratory-factor-analysis-part-1.html#cb485-5" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb485-6"><a href="week-07-exploratory-factor-analysis-part-1.html#cb485-6" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;Number of factors&quot;</span>) <span class="sc">+</span></span>
<span id="cb485-7"><a href="week-07-exploratory-factor-analysis-part-1.html#cb485-7" tabindex="-1"></a>  <span class="fu">labs</span>( <span class="at">title =</span> <span class="st">&quot;Scree Plot&quot;</span>, </span>
<span id="cb485-8"><a href="week-07-exploratory-factor-analysis-part-1.html#cb485-8" tabindex="-1"></a>        <span class="at">subtitle =</span> <span class="st">&quot;(Based on the reduced correlation matrix)&quot;</span>) </span></code></pre></div>
<p><img src="multivariate_stats_files/figure-html/unnamed-chunk-252-1.png" width="672" /></p>
<p>We see that most of the “debris” from the third factor and onward form a kind of even slope. This suggests that the number of factors above this third one might be optimal. Scree plots are somewhat subjective in their interpretation. One could argue that this one has as an inflection point between the third and fourth factor, suggesting we retain three factors instead of just two.</p>
<p>Optionally, we can can generate a scree plot that uses the <strong>unreduced</strong> correlation matrix, which is what other programs such as SPSS and SAS produce. Let’s use the <code>fa()</code> function from the psych package to perform a factor analysis solely to extract the eigenvalues at this stage. One of the arguments is <code>nfactors =</code>, which we do not know at this stage, so we specify it as the number of subtests, which is the number of columns in our data frame <code>ncol(dat)</code>:</p>
<div class="sourceCode" id="cb486"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb486-1"><a href="week-07-exploratory-factor-analysis-part-1.html#cb486-1" tabindex="-1"></a>fafitfree <span class="ot">&lt;-</span> psych<span class="sc">::</span><span class="fu">fa</span>(dat,</span>
<span id="cb486-2"><a href="week-07-exploratory-factor-analysis-part-1.html#cb486-2" tabindex="-1"></a>                       <span class="at">nfactors =</span> <span class="fu">ncol</span>(dat), </span>
<span id="cb486-3"><a href="week-07-exploratory-factor-analysis-part-1.html#cb486-3" tabindex="-1"></a>                       <span class="at">rotate =</span> <span class="st">&quot;none&quot;</span>)</span>
<span id="cb486-4"><a href="week-07-exploratory-factor-analysis-part-1.html#cb486-4" tabindex="-1"></a>n_factors <span class="ot">&lt;-</span> <span class="fu">length</span>(fafitfree<span class="sc">$</span>e.values)</span>
<span id="cb486-5"><a href="week-07-exploratory-factor-analysis-part-1.html#cb486-5" tabindex="-1"></a>scree     <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb486-6"><a href="week-07-exploratory-factor-analysis-part-1.html#cb486-6" tabindex="-1"></a>               <span class="at">Factor_n =</span>  <span class="fu">as.factor</span>(<span class="dv">1</span><span class="sc">:</span>n_factors), </span>
<span id="cb486-7"><a href="week-07-exploratory-factor-analysis-part-1.html#cb486-7" tabindex="-1"></a>             <span class="at">Eigenvalue =</span> fafitfree<span class="sc">$</span>e.values)</span>
<span id="cb486-8"><a href="week-07-exploratory-factor-analysis-part-1.html#cb486-8" tabindex="-1"></a><span class="fu">ggplot</span>(scree, <span class="fu">aes</span>(<span class="at">x =</span> Factor_n, <span class="at">y =</span> Eigenvalue, <span class="at">group =</span> <span class="dv">1</span>)) <span class="sc">+</span> </span>
<span id="cb486-9"><a href="week-07-exploratory-factor-analysis-part-1.html#cb486-9" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb486-10"><a href="week-07-exploratory-factor-analysis-part-1.html#cb486-10" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;Number of factors&quot;</span>) <span class="sc">+</span></span>
<span id="cb486-11"><a href="week-07-exploratory-factor-analysis-part-1.html#cb486-11" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">&quot;Initial eigenvalue&quot;</span>) <span class="sc">+</span></span>
<span id="cb486-12"><a href="week-07-exploratory-factor-analysis-part-1.html#cb486-12" tabindex="-1"></a>  <span class="fu">labs</span>( <span class="at">title =</span> <span class="st">&quot;Scree Plot&quot;</span>, </span>
<span id="cb486-13"><a href="week-07-exploratory-factor-analysis-part-1.html#cb486-13" tabindex="-1"></a>     <span class="at">subtitle =</span> <span class="st">&quot;(Based on the unreduced correlation matrix)&quot;</span>)</span></code></pre></div>
<p><img src="multivariate_stats_files/figure-html/unnamed-chunk-253-1.png" width="672" /></p>
<p>The shape of this plot is similar to that generated from the reduced correlation matrix (though the eigenvalues on the Y axis are larger because the unique variance has not yet been removed from the correlation matrix). If we also opted to use the eigenvalue-greater-than-one approach, we could use this plot and see that Factor 3 is below this criterion.<a href="#fn42" class="footnote-ref" id="fnref42"><sup>42</sup></a></p>
<p>Finally, we can use the psych package’s <code>fa.parallel()</code> function, which produces the parallel analysis and the scree plot all in one. I would <strong>ignore the horizontal line at the Eigenvalue = 1 K1 criterion</strong> as that is for eigenvalues from the <strong>un</strong>reduced correlation matrix or for principal components analysis, not for the <strong>reduced</strong> correlation matrix, which is what we have here.</p>
<div class="sourceCode" id="cb487"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb487-1"><a href="week-07-exploratory-factor-analysis-part-1.html#cb487-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)   <span class="co"># To reproduce our randomly generated parallel data sets.</span></span>
<span id="cb487-2"><a href="week-07-exploratory-factor-analysis-part-1.html#cb487-2" tabindex="-1"></a>para.psych <span class="ot">&lt;-</span> psych<span class="sc">::</span><span class="fu">fa.parallel</span>(dat, </span>
<span id="cb487-3"><a href="week-07-exploratory-factor-analysis-part-1.html#cb487-3" tabindex="-1"></a>                                 <span class="at">fa =</span> <span class="st">&quot;fa&quot;</span>,</span>
<span id="cb487-4"><a href="week-07-exploratory-factor-analysis-part-1.html#cb487-4" tabindex="-1"></a>                                 <span class="at">nfactors =</span> <span class="fu">length</span>(<span class="fu">names</span>(dat)))</span></code></pre></div>
<p><img src="multivariate_stats_files/figure-html/unnamed-chunk-254-1.png" width="672" /></p>
<pre><code>## Parallel analysis suggests that the number of factors =  2  and the number of components =  NA</code></pre>
</div>
<div id="hull-method" class="section level5 hasAnchor" number="8.2.4.1.2">
<h5><span class="header-section-number">8.2.4.1.2</span> Hull method<a href="week-07-exploratory-factor-analysis-part-1.html#hull-method" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>We can use the Hull method <span class="citation">(<a href="#ref-lorenzo-seva_hull_2011">Lorenzo-Seva, Timmerman, and Kiers 2011</a>)</span> to identify the number of factors at which the fit of the model and the parsimony of the model are a good balance. In a perfect model, we would have a large number of factors but such a model would be inefficient if there is shared variance among the subtests. We can use the <code>hullEFA()</code> function from the EFA.MRFA package <span class="citation">(<a href="#ref-R-EFA.MRFA">Navarro-Gonzalez and Lorenzo-Seva 2021</a>)</span>.</p>
<div class="sourceCode" id="cb489"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb489-1"><a href="week-07-exploratory-factor-analysis-part-1.html#cb489-1" tabindex="-1"></a><span class="fu">library</span>(EFA.MRFA)</span>
<span id="cb489-2"><a href="week-07-exploratory-factor-analysis-part-1.html#cb489-2" tabindex="-1"></a><span class="fu">hullEFA</span>(dat, <span class="at">index_hull =</span> <span class="st">&quot;CFI&quot;</span>)</span></code></pre></div>
<pre><code>## HULL METHOD - CFI INDEX
## 
##         q      f          g       st
##         0      0.0000    36       0.0000 
##         1      0.6930    27       1.9389 
##         2      1.0107    19       433.6852 
##         3      1.0114    12       0.0000 
## 
## Number of advised dimensions: 2 
## 
## -----------------------------------------------</code></pre>
<p><img src="multivariate_stats_files/figure-html/unnamed-chunk-255-1.png" width="672" /></p>
<p>We included one argument in that function, <code>index_hull = "CFI"</code>, which sets the index to be the comparative fit index (CFI).<a href="#fn43" class="footnote-ref" id="fnref43"><sup>43</sup></a> There are other fit indexes available, as described in the help files <span class="citation">(and in <a href="#ref-lorenzo-seva_hull_2011">Lorenzo-Seva, Timmerman, and Kiers 2011</a>)</span>. The CFI is a global fit index that we will use in confirmatory factor analysis. The logical range of the CFI is between 0 and 1, with the best possible fit being 1. We see the estimate at the model with two factors is already at 1 (even estimated to be higher than the logical maximum), so it is a clear indication that more than two factors will not improve the fit.</p>
<p>In the output, each row is its own model. The first row is the null model, with 0 factors, the second, a model with 1 factor, and so forth. The <code>f</code> column provides the estimate of the fit index (CFI in our case) for each model. The <code>g</code> column provides the degrees of freedom from this model.<a href="#fn44" class="footnote-ref" id="fnref44"><sup>44</sup></a> With more factors, we have fewer degrees of freedom because we are estimating more factor loadings. We want the best return for our use of these degrees of freedom—the best bang for our buck. The rightmost column is the Hull test value. The highest value indicates the optimal number of factors to retain. The graph from the output is provided for us to interpret where there is a leveling off of model fit as we include more factors. The results here are consistent with those of the parallel analysis and the scree plot. We might report something like this:</p>
<blockquote>
<p>Based on the parallel analysis from the nFactors package <span class="citation">(<a href="#ref-R-nFactors">Raiche and Magis 2025</a>)</span>, the scree plot with the reduced correlation matrix, and the Hull test from the EFA.MRFA package <span class="citation">(<a href="#ref-R-EFA.MRFA">Navarro-Gonzalez and Lorenzo-Seva 2021</a>)</span>, we determined that the optimal number of factors to retain for our exploratory factor analysis is two.</p>
</blockquote>
<p>A limitation of the Hull analysis from this package at this time, it seems, is that it requires a raw data set with complete data rather than a correlation matrix.</p>
</div>
</div>
</div>
<div id="factor-analysis-with-a-data-matrix-raw-data" class="section level3 hasAnchor" number="8.2.5">
<h3><span class="header-section-number">8.2.5</span> Factor analysis with a data matrix (raw data)<a href="week-07-exploratory-factor-analysis-part-1.html#factor-analysis-with-a-data-matrix-raw-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Using the <code>fa()</code> function from the psych package, we can now specify a two factor solution. The first argument is the data set; the second is the number of factors. Let’s use the principal axis factor extraction method (<code>fa = "pa"</code>), which is very common. Note that if we had multivariate normality, we could use maximum likelihood estimation, (<code>fa = "ml"</code>), but that assumption was not met. Let’s also start with Promax rotation (rather than varimax, which is what we used in PCA). Promax rotation is very common and is an oblique rotation—in other words, it permits correlations among the factors. We select this because factors tend to be correlated in education, psychology, and the other social sciences.</p>
<p>If you encounter research that uses orthogonal rotation in the social sciences without also investigating oblique rotation, ask why they assume the factors are assumed to be unrelated; be skeptical.</p>
<div id="determine-how-much-of-the-variance-is-explained-by-the-n-factors" class="section level4 hasAnchor" number="8.2.5.1">
<h4><span class="header-section-number">8.2.5.1</span> Determine how much of the variance is explained by the <em>n</em> factors<a href="week-07-exploratory-factor-analysis-part-1.html#determine-how-much-of-the-variance-is-explained-by-the-n-factors" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Let’s fit the EFA model without rotation to obtain the proportion of variance explained by our two factors.</p>
<div class="sourceCode" id="cb491"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb491-1"><a href="week-07-exploratory-factor-analysis-part-1.html#cb491-1" tabindex="-1"></a><span class="co"># help(&quot;fa&quot;, package=&quot;psych&quot;)</span></span>
<span id="cb491-2"><a href="week-07-exploratory-factor-analysis-part-1.html#cb491-2" tabindex="-1"></a>fafit.norot <span class="ot">&lt;-</span> psych<span class="sc">::</span><span class="fu">fa</span>(dat, </span>
<span id="cb491-3"><a href="week-07-exploratory-factor-analysis-part-1.html#cb491-3" tabindex="-1"></a>                   <span class="at">nfactors =</span> <span class="dv">2</span>, </span>
<span id="cb491-4"><a href="week-07-exploratory-factor-analysis-part-1.html#cb491-4" tabindex="-1"></a>                   <span class="at">fm =</span> <span class="st">&quot;pa&quot;</span>,</span>
<span id="cb491-5"><a href="week-07-exploratory-factor-analysis-part-1.html#cb491-5" tabindex="-1"></a>                   <span class="at">rotate =</span> <span class="st">&quot;none&quot;</span>)</span>
<span id="cb491-6"><a href="week-07-exploratory-factor-analysis-part-1.html#cb491-6" tabindex="-1"></a><span class="fu">print</span>( fafit.norot, <span class="at">digits =</span> <span class="dv">4</span>)</span></code></pre></div>
<pre><code>## Factor Analysis using method =  pa
## Call: psych::fa(r = dat, nfactors = 2, rotate = &quot;none&quot;, fm = &quot;pa&quot;)
## Standardized loadings (pattern matrix) based upon correlation matrix
##             PA1     PA2     h2     u2   com
## NoIntrst 0.7470 -0.0788 0.5643 0.4357 1.022
## Odd      0.6615 -0.0596 0.4411 0.5589 1.016
## NoFun    0.6074 -0.0587 0.3724 0.6276 1.019
## Boring   0.3910 -0.3563 0.2798 0.7202 1.983
## Alone    0.4123 -0.0674 0.1745 0.8255 1.053
## NoRelign 0.3870  0.0081 0.1498 0.8502 1.001
## Better   0.2479  0.6645 0.5030 0.4970 1.273
## Good     0.1392  0.6235 0.4081 0.5919 1.099
## Help     0.0732  0.6327 0.4057 0.5943 1.027
## 
##                          PA1    PA2
## SS loadings           1.9234 1.3754
## Proportion Var        0.2137 0.1528
## Cumulative Var        0.2137 0.3665
## Proportion Explained  0.5831 0.4169
## Cumulative Proportion 0.5831 1.0000
## 
## Mean item complexity =  1.2
## Test of the hypothesis that 2 factors are sufficient.
## 
## df null model =  36  with the objective function =  1.6511 with Chi Square =  604.5854
## df of  the model are 19  and the objective function was  0.0619 
## 
## The root mean square of the residuals (RMSR) is  0.0286 
## The df corrected root mean square of the residuals is  0.0394 
## 
## The harmonic n.obs is  371 with the empirical chi square  21.8428  with prob &lt;  0.2921 
## The total n.obs was  371  with Likelihood Chi Square =  22.5947  with prob &lt;  0.2557 
## 
## Tucker Lewis Index of factoring reliability =  0.98797
## RMSEA index =  0.02242  and the 90 % confidence intervals are  0 0.05298
## BIC =  -89.8131
## Fit based upon off diagonal values = 0.9863
## Measures of factor score adequacy             
##                                                      PA1    PA2
## Correlation of (regression) scores with factors   0.8795 0.8414
## Multiple R square of scores with factors          0.7736 0.7079
## Minimum correlation of possible factor scores     0.5471 0.4157</code></pre>
<p>We can see, where it says <code>Cumulative Var</code>, that the proportion of variance explained is .3665, or 36.65%.</p>
</div>
<div id="fit-the-efa-with-rotation-for-interpretation" class="section level4 hasAnchor" number="8.2.5.2">
<h4><span class="header-section-number">8.2.5.2</span> Fit the EFA with rotation, for interpretation<a href="week-07-exploratory-factor-analysis-part-1.html#fit-the-efa-with-rotation-for-interpretation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="sourceCode" id="cb493"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb493-1"><a href="week-07-exploratory-factor-analysis-part-1.html#cb493-1" tabindex="-1"></a><span class="co"># help(&quot;fa&quot;, package=&quot;psych&quot;)</span></span>
<span id="cb493-2"><a href="week-07-exploratory-factor-analysis-part-1.html#cb493-2" tabindex="-1"></a>fafit <span class="ot">&lt;-</span> psych<span class="sc">::</span><span class="fu">fa</span>(dat, </span>
<span id="cb493-3"><a href="week-07-exploratory-factor-analysis-part-1.html#cb493-3" tabindex="-1"></a>                   <span class="at">nfactors =</span> <span class="dv">2</span>, </span>
<span id="cb493-4"><a href="week-07-exploratory-factor-analysis-part-1.html#cb493-4" tabindex="-1"></a>                   <span class="at">fm =</span> <span class="st">&quot;pa&quot;</span>,</span>
<span id="cb493-5"><a href="week-07-exploratory-factor-analysis-part-1.html#cb493-5" tabindex="-1"></a>                   <span class="at">rotate =</span> <span class="st">&quot;promax&quot;</span>)</span>
<span id="cb493-6"><a href="week-07-exploratory-factor-analysis-part-1.html#cb493-6" tabindex="-1"></a>n_factors <span class="ot">&lt;-</span> <span class="fu">length</span>(fafit<span class="sc">$</span>e.values)</span>
<span id="cb493-7"><a href="week-07-exploratory-factor-analysis-part-1.html#cb493-7" tabindex="-1"></a>fafit</span></code></pre></div>
<pre><code>## Factor Analysis using method =  pa
## Call: psych::fa(r = dat, nfactors = 2, rotate = &quot;promax&quot;, fm = &quot;pa&quot;)
## Standardized loadings (pattern matrix) based upon correlation matrix
##            PA1   PA2   h2   u2 com
## NoIntrst  0.75  0.05 0.56 0.44 1.0
## Odd       0.66  0.05 0.44 0.56 1.0
## NoFun     0.61  0.04 0.37 0.63 1.0
## Boring    0.45 -0.29 0.28 0.72 1.7
## Alone     0.42  0.00 0.17 0.83 1.0
## NoRelign  0.38  0.07 0.15 0.85 1.1
## Better    0.13  0.70 0.50 0.50 1.1
## Good      0.03  0.64 0.41 0.59 1.0
## Help     -0.03  0.64 0.41 0.59 1.0
## 
##                        PA1  PA2
## SS loadings           1.91 1.39
## Proportion Var        0.21 0.15
## Cumulative Var        0.21 0.37
## Proportion Explained  0.58 0.42
## Cumulative Proportion 0.58 1.00
## 
##  With factor correlations of 
##     PA1 PA2
## PA1   1   0
## PA2   0   1
## 
## Mean item complexity =  1.1
## Test of the hypothesis that 2 factors are sufficient.
## 
## df null model =  36  with the objective function =  1.65 with Chi Square =  604.59
## df of  the model are 19  and the objective function was  0.06 
## 
## The root mean square of the residuals (RMSR) is  0.03 
## The df corrected root mean square of the residuals is  0.04 
## 
## The harmonic n.obs is  371 with the empirical chi square  21.84  with prob &lt;  0.29 
## The total n.obs was  371  with Likelihood Chi Square =  22.59  with prob &lt;  0.26 
## 
## Tucker Lewis Index of factoring reliability =  0.988
## RMSEA index =  0.022  and the 90 % confidence intervals are  0 0.053
## BIC =  -89.81
## Fit based upon off diagonal values = 0.99
## Measures of factor score adequacy             
##                                                    PA1  PA2
## Correlation of (regression) scores with factors   0.88 0.84
## Multiple R square of scores with factors          0.77 0.71
## Minimum correlation of possible factor scores     0.54 0.42</code></pre>
<p>The matrix of factor loadings is presented. These are also called <strong>pattern coefficients</strong> for oblique rotations such as this one. The factors are the columns, which here have a “PA” affixed to the factor number to remind us that <strong>p</strong>rincipal <strong>a</strong>xis factoring was used. The qualitative names of these factors that eventually go into a report are usually determined by the researcher and their interpretations of the types of subtests (or items) that are reflected by the factor. The <code>h2</code> and <code>u2</code> are communalities and unique variances.<a href="#fn45" class="footnote-ref" id="fnref45"><sup>45</sup></a></p>
<p>We can prepare this output so it is more readable. For instance, we can ask that the loadings that do not exceed a cut-off value, such as <span class="math inline">\(.32\)</span>, be excluded from the printout.<a href="#fn46" class="footnote-ref" id="fnref46"><sup>46</sup></a> This cut-off criterion is a decision rule we should establish before conducting our analysis. We can can also sort the report so that the subtests are grouped by their strongest factor and in the order of loadings within each factor.</p>
<div class="sourceCode" id="cb495"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb495-1"><a href="week-07-exploratory-factor-analysis-part-1.html#cb495-1" tabindex="-1"></a><span class="fu">print</span>(fafit, </span>
<span id="cb495-2"><a href="week-07-exploratory-factor-analysis-part-1.html#cb495-2" tabindex="-1"></a>      <span class="at">cut =</span> .<span class="dv">32</span>, </span>
<span id="cb495-3"><a href="week-07-exploratory-factor-analysis-part-1.html#cb495-3" tabindex="-1"></a>      <span class="at">sort =</span> <span class="cn">TRUE</span>, </span>
<span id="cb495-4"><a href="week-07-exploratory-factor-analysis-part-1.html#cb495-4" tabindex="-1"></a>      <span class="at">digits =</span> <span class="dv">3</span>)</span></code></pre></div>
<pre><code>## Factor Analysis using method =  pa
## Call: psych::fa(r = dat, nfactors = 2, rotate = &quot;promax&quot;, fm = &quot;pa&quot;)
## Standardized loadings (pattern matrix) based upon correlation matrix
##          item    PA1    PA2    h2    u2  com
## NoIntrst    1  0.750        0.564 0.436 1.01
## Odd         2  0.662        0.441 0.559 1.01
## NoFun       3  0.609        0.372 0.628 1.01
## Boring      4  0.445        0.280 0.720 1.71
## Alone       5  0.418        0.175 0.825 1.00
## NoRelign    6  0.380        0.150 0.850 1.07
## Better      7         0.696 0.503 0.497 1.07
## Good        8         0.638 0.408 0.592 1.01
## Help        9         0.636 0.406 0.594 1.01
## 
##                         PA1   PA2
## SS loadings           1.908 1.391
## Proportion Var        0.212 0.155
## Cumulative Var        0.212 0.367
## Proportion Explained  0.578 0.422
## Cumulative Proportion 0.578 1.000
## 
##  With factor correlations of 
##       PA1   PA2
## PA1 1.000 0.003
## PA2 0.003 1.000
## 
## Mean item complexity =  1.1
## Test of the hypothesis that 2 factors are sufficient.
## 
## df null model =  36  with the objective function =  1.651 with Chi Square =  604.585
## df of  the model are 19  and the objective function was  0.062 
## 
## The root mean square of the residuals (RMSR) is  0.029 
## The df corrected root mean square of the residuals is  0.039 
## 
## The harmonic n.obs is  371 with the empirical chi square  21.843  with prob &lt;  0.292 
## The total n.obs was  371  with Likelihood Chi Square =  22.595  with prob &lt;  0.256 
## 
## Tucker Lewis Index of factoring reliability =  0.988
## RMSEA index =  0.0224  and the 90 % confidence intervals are  0 0.053
## BIC =  -89.813
## Fit based upon off diagonal values = 0.986
## Measures of factor score adequacy             
##                                                     PA1   PA2
## Correlation of (regression) scores with factors   0.878 0.843
## Multiple R square of scores with factors          0.770 0.711
## Minimum correlation of possible factor scores     0.541 0.423</code></pre>
<div id="more-on-the-percentage-of-variance-accounted-for" class="section level5 hasAnchor" number="8.2.5.2.1">
<h5><span class="header-section-number">8.2.5.2.1</span> More on the percentage of variance accounted for<a href="week-07-exploratory-factor-analysis-part-1.html#more-on-the-percentage-of-variance-accounted-for" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Some advisers and journals like to see a table of proportions of variances, such as Table 12.9 in the reading. We can isolate information from the factor analysis output to get the proportion of variance.</p>
<div class="sourceCode" id="cb497"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb497-1"><a href="week-07-exploratory-factor-analysis-part-1.html#cb497-1" tabindex="-1"></a>PcntVarTable <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">Factor =</span> <span class="dv">1</span><span class="sc">:</span>n_factors,</span>
<span id="cb497-2"><a href="week-07-exploratory-factor-analysis-part-1.html#cb497-2" tabindex="-1"></a>                         <span class="at">Eigenval =</span> fafit<span class="sc">$</span>e.values,</span>
<span id="cb497-3"><a href="week-07-exploratory-factor-analysis-part-1.html#cb497-3" tabindex="-1"></a>                          <span class="at">PcntVar =</span> fafit<span class="sc">$</span>e.values <span class="sc">/</span> n_factors <span class="sc">*</span> <span class="dv">100</span>)</span>
<span id="cb497-4"><a href="week-07-exploratory-factor-analysis-part-1.html#cb497-4" tabindex="-1"></a>PcntVarTable<span class="sc">$</span>Cumul_Pcnt_var <span class="ot">&lt;-</span> <span class="fu">cumsum</span>(PcntVarTable<span class="sc">$</span>PcntVar)</span>
<span id="cb497-5"><a href="week-07-exploratory-factor-analysis-part-1.html#cb497-5" tabindex="-1"></a>PcntVarTable <span class="sc">%&gt;%</span> </span>
<span id="cb497-6"><a href="week-07-exploratory-factor-analysis-part-1.html#cb497-6" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="fu">across</span>(<span class="fu">where</span>(is.numeric), round, <span class="dv">2</span>))</span>
<span id="cb497-7"><a href="week-07-exploratory-factor-analysis-part-1.html#cb497-7" tabindex="-1"></a><span class="co"># An alternative to the line obove is Base R:</span></span>
<span id="cb497-8"><a href="week-07-exploratory-factor-analysis-part-1.html#cb497-8" tabindex="-1"></a><span class="co"># PcntVarTable[2:4] &lt;- round(PcntVarTable[2:4],2) </span></span>
<span id="cb497-9"><a href="week-07-exploratory-factor-analysis-part-1.html#cb497-9" tabindex="-1"></a><span class="co"># PcntVarTable</span></span></code></pre></div>
<pre><code>##   Factor Eigenval PcntVar Cumul_Pcnt_var
## 1      1     2.53   28.09          28.09
## 2      2     1.95   21.69          49.78
## 3      3     0.95   10.60          60.38
## 4      4     0.74    8.17          68.55
## 5      5     0.67    7.42          75.97
## 6      6     0.62    6.92          82.88
## 7      7     0.58    6.42          89.30
## 8      8     0.51    5.68          94.99
## 9      9     0.45    5.01         100.00</code></pre>
<p>Here’s the <strong><em>Extraction sums of squared loadings</em></strong> table that Bandalos exemplifies in Table 12.7 and which is automatically generated in SPSS. The eigenvalues here are from the EFA model with the specified number of factors <strong><em>before</em></strong> rotation is applied.</p>
<div class="sourceCode" id="cb499"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb499-1"><a href="week-07-exploratory-factor-analysis-part-1.html#cb499-1" tabindex="-1"></a>SS.loadings    <span class="ot">&lt;-</span> fafit<span class="sc">$</span>values[<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>] <span class="co"># We only extracted the first two, so use 1:2</span></span>
<span id="cb499-2"><a href="week-07-exploratory-factor-analysis-part-1.html#cb499-2" tabindex="-1"></a>PcntVar        <span class="ot">&lt;-</span>  <span class="dv">100</span> <span class="sc">*</span> (SS.loadings <span class="sc">/</span> n_factors )</span>
<span id="cb499-3"><a href="week-07-exploratory-factor-analysis-part-1.html#cb499-3" tabindex="-1"></a>Cumul_Pcnt_var <span class="ot">&lt;-</span> <span class="fu">cumsum</span>(PcntVar)</span>
<span id="cb499-4"><a href="week-07-exploratory-factor-analysis-part-1.html#cb499-4" tabindex="-1"></a><span class="fu">data.frame</span>(SS.loadings, PcntVar, Cumul_Pcnt_var) <span class="sc">%&gt;%</span> </span>
<span id="cb499-5"><a href="week-07-exploratory-factor-analysis-part-1.html#cb499-5" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="fu">across</span>(<span class="fu">where</span>(is.numeric), round, <span class="dv">3</span>))</span></code></pre></div>
<pre><code>##   SS.loadings PcntVar Cumul_Pcnt_var
## 1       1.923  21.371         21.371
## 2       1.375  15.282         36.653</code></pre>
<p>We can see this also if we fit the EFA on the data without rotation:</p>
<div class="sourceCode" id="cb501"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb501-1"><a href="week-07-exploratory-factor-analysis-part-1.html#cb501-1" tabindex="-1"></a>fafit.unrot <span class="ot">&lt;-</span> psych<span class="sc">::</span><span class="fu">fa</span>(dat, <span class="at">nfactors =</span> <span class="dv">2</span>, <span class="at">fm =</span> <span class="st">&quot;pa&quot;</span>, <span class="at">rotate =</span> <span class="st">&quot;none&quot;</span>) <span class="co"># no rotation</span></span>
<span id="cb501-2"><a href="week-07-exploratory-factor-analysis-part-1.html#cb501-2" tabindex="-1"></a>SS.loadings    <span class="ot">&lt;-</span> <span class="fu">diag</span>( <span class="fu">t</span>(fafit.unrot<span class="sc">$</span>loadings) <span class="sc">%*%</span> fafit.unrot<span class="sc">$</span>loadings )</span>
<span id="cb501-3"><a href="week-07-exploratory-factor-analysis-part-1.html#cb501-3" tabindex="-1"></a>PcntVar        <span class="ot">&lt;-</span>  <span class="dv">100</span> <span class="sc">*</span> (SS.loadings <span class="sc">/</span> n_factors )</span>
<span id="cb501-4"><a href="week-07-exploratory-factor-analysis-part-1.html#cb501-4" tabindex="-1"></a>Cumul_Pcnt_var <span class="ot">&lt;-</span> <span class="fu">cumsum</span>(PcntVar)</span>
<span id="cb501-5"><a href="week-07-exploratory-factor-analysis-part-1.html#cb501-5" tabindex="-1"></a><span class="fu">data.frame</span>(SS.loadings, PcntVar, Cumul_Pcnt_var) <span class="sc">%&gt;%</span> </span>
<span id="cb501-6"><a href="week-07-exploratory-factor-analysis-part-1.html#cb501-6" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="fu">across</span>(<span class="fu">where</span>(is.numeric), round, <span class="dv">3</span>))</span></code></pre></div>
<pre><code>##     SS.loadings PcntVar Cumul_Pcnt_var
## PA1       1.923  21.371         21.371
## PA2       1.375  15.282         36.653</code></pre>
</div>
</div>
<div id="reduced-correlation-matrix" class="section level4 hasAnchor" number="8.2.5.3">
<h4><span class="header-section-number">8.2.5.3</span> Reduced correlation matrix<a href="week-07-exploratory-factor-analysis-part-1.html#reduced-correlation-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<!-- Here is Equation 12.1 in the text: -->
<!-- $$X_{i\nu} = w_{\nu 1}F_{1i} + w_{\nu 2}F_{2i} + \cdots + w_{\nu f}F_{fi} + U_{i\nu}$$ -->
<p>Notice that we can reproduced the original correlation matrix from the matrix of factor loadings (pattern coefficients), the correlation matrix among the factors, and the residuals. Bandalos uses <span class="math inline">\(\mathbf{\Lambda}\)</span> (uppercase lambda) for the factor loadings matrix, <span class="math inline">\(\mathbf{\Phi}\)</span> (uppercase Phi) for the matrix of correlations among the factors, and <span class="math inline">\(\mathbf{\Theta}\)</span> (uppercase theta) for the residuals. Unfortunately, this symbol system differs from the notation used in Everitt and Hothorn when we discussed PCA. We will see these symbols again in the CFA chapter.</p>
<p>Equation 2.2 in Bandalos (p. 309) is this:</p>
<p><span class="math display">\[ \mathbf{\hat{\Sigma}}_x = \mathbf{\Lambda} \mathbf{\Phi} \mathbf{\Lambda}^\prime + \mathbf{\Theta}\]</span>
(I added a hat to <span class="math inline">\(\mathbf{\Sigma}\)</span> because it is an estimate of the population matrix, not the actual population matrix.)</p>
<p>Here is the <span class="math inline">\(\Lambda\)</span> matrix, which is our columns of factor loadings:</p>
<div class="sourceCode" id="cb503"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb503-1"><a href="week-07-exploratory-factor-analysis-part-1.html#cb503-1" tabindex="-1"></a><span class="fu">cbind</span>(fafit<span class="sc">$</span>loadings)</span></code></pre></div>
<pre><code>##                  PA1         PA2
## NoIntrst  0.74965067  0.04542267
## Odd       0.66211762  0.05019876
## NoFun     0.60867412  0.04221005
## Boring    0.44526412 -0.28699704
## Alone     0.41774831  0.00149331
## NoRelign  0.38013411  0.07174633
## Better    0.13275747  0.69628707
## Good      0.03249128  0.63789387
## Help     -0.03408238  0.63612802</code></pre>
<p>Here is <span class="math inline">\(\Phi\)</span>:</p>
<div class="sourceCode" id="cb505"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb505-1"><a href="week-07-exploratory-factor-analysis-part-1.html#cb505-1" tabindex="-1"></a>fafit<span class="sc">$</span>Phi</span></code></pre></div>
<pre><code>##             PA1         PA2
## PA1 1.000000000 0.003239124
## PA2 0.003239124 1.000000000</code></pre>
<p>Notice that the factors very weakly correlate. This is unusual in social-science data.</p>
<p>Let’s calculate the reduced correlation matrix:</p>
<div class="sourceCode" id="cb507"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb507-1"><a href="week-07-exploratory-factor-analysis-part-1.html#cb507-1" tabindex="-1"></a>corr_reduced <span class="ot">&lt;-</span> fafit<span class="sc">$</span>loadings <span class="sc">%*%</span> fafit<span class="sc">$</span>Phi <span class="sc">%*%</span> <span class="fu">t</span>(fafit<span class="sc">$</span>loadings)</span>
<span id="cb507-2"><a href="week-07-exploratory-factor-analysis-part-1.html#cb507-2" tabindex="-1"></a><span class="fu">round</span>(corr_reduced, <span class="dv">3</span>)</span></code></pre></div>
<pre><code>##          NoIntrst   Odd NoFun Boring  Alone NoRelign Better   Good   Help
## NoIntrst    0.564 0.499 0.458  0.320  0.313    0.288  0.133  0.055  0.005
## Odd         0.499 0.441 0.405  0.280  0.277    0.256  0.124  0.055  0.011
## NoFun       0.458 0.405 0.372  0.258  0.254    0.235  0.112  0.048  0.007
## Boring      0.320 0.280 0.258  0.280  0.185    0.148 -0.140 -0.168 -0.197
## Alone       0.313 0.277 0.254  0.185  0.175    0.159  0.057  0.015 -0.012
## NoRelign    0.288 0.256 0.235  0.148  0.159    0.150  0.101  0.059  0.033
## Better      0.133 0.124 0.112 -0.140  0.057    0.101  0.503  0.449  0.439
## Good        0.055 0.055 0.048 -0.168  0.015    0.059  0.449  0.408  0.405
## Help        0.005 0.011 0.007 -0.197 -0.012    0.033  0.439  0.405  0.406</code></pre>
<p>Notice that if we add the residuals, we get the original, observed correlation matrix:</p>
<div class="sourceCode" id="cb509"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb509-1"><a href="week-07-exploratory-factor-analysis-part-1.html#cb509-1" tabindex="-1"></a>Theta <span class="ot">&lt;-</span> fafit<span class="sc">$</span>residual</span>
<span id="cb509-2"><a href="week-07-exploratory-factor-analysis-part-1.html#cb509-2" tabindex="-1"></a><span class="fu">round</span>( corr_reduced <span class="sc">+</span> Theta , <span class="dv">3</span>)</span></code></pre></div>
<pre><code>##          NoIntrst   Odd NoFun Boring  Alone NoRelign Better   Good   Help
## NoIntrst    1.000 0.504 0.454  0.286  0.351    0.292  0.123  0.049  0.006
## Odd         0.504 1.000 0.388  0.303  0.216    0.311  0.138  0.029  0.027
## NoFun       0.454 0.388 1.000  0.261  0.260    0.264  0.113  0.046  0.007
## Boring      0.286 0.303 0.261  1.000  0.252    0.090 -0.136 -0.131 -0.231
## Alone       0.351 0.216 0.260  0.252  1.000    0.096  0.076  0.031 -0.010
## NoRelign    0.292 0.311 0.264  0.090  0.096    1.000  0.067  0.051  0.046
## Better      0.123 0.138 0.113 -0.136  0.076    0.067  1.000  0.470  0.425
## Good        0.049 0.029 0.046 -0.131  0.031    0.051  0.470  1.000  0.402
## Help        0.006 0.027 0.007 -0.231 -0.010    0.046  0.425  0.402  1.000</code></pre>
<p>Also, notice that the diagonal of the reduced correlation matrix contains the communalities of the EFA model:</p>
<div class="sourceCode" id="cb511"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb511-1"><a href="week-07-exploratory-factor-analysis-part-1.html#cb511-1" tabindex="-1"></a><span class="fu">diag</span>(corr_reduced)</span></code></pre></div>
<pre><code>##  NoIntrst       Odd     NoFun    Boring     Alone  NoRelign    Better      Good      Help 
## 0.5642599 0.4411350 0.3724323 0.2797996 0.1745199 0.1498262 0.5030391 0.4080985 0.4056800</code></pre>
<p>This is the same as that from the EFA output:</p>
<div class="sourceCode" id="cb513"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb513-1"><a href="week-07-exploratory-factor-analysis-part-1.html#cb513-1" tabindex="-1"></a>fafit<span class="sc">$</span>communality</span></code></pre></div>
<pre><code>##  NoIntrst       Odd     NoFun    Boring     Alone  NoRelign    Better      Good      Help 
## 0.5642599 0.4411350 0.3724323 0.2797996 0.1745199 0.1498262 0.5030391 0.4080985 0.4056800</code></pre>
<p><strong>This reduced correlation is what distinguishes EFA from PCA. The factors are based on this reduced correlation matrix, after removing the error.</strong></p>
</div>
<div id="examining-pattern-coefficients-the-factor-loadings" class="section level4 hasAnchor" number="8.2.5.4">
<h4><span class="header-section-number">8.2.5.4</span> Examining pattern coefficients (the factor loadings)<a href="week-07-exploratory-factor-analysis-part-1.html#examining-pattern-coefficients-the-factor-loadings" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We can now see which subtests load on which factors. In a diagram, these would be the loadings on the arrows from the factor to each subtest. Factor loadings are also called pattern coefficients (the latter being a more accurate term for oblique rotations). The squared pattern coefficients represent the percentage of variance in the observed subtest that is explained by the factor after partialing out the variance explained by the other factors.</p>
<p>The <code>fafit$loadings</code> reports the salient pattern coefficients. In the output, none of the coefficients seems to be complex in this example. We see that none has a loading on another factor with an absolute value greater than our specified cutoff of <span class="math inline">\(.32\)</span>.</p>
<div class="sourceCode" id="cb515"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb515-1"><a href="week-07-exploratory-factor-analysis-part-1.html#cb515-1" tabindex="-1"></a>fafit<span class="sc">$</span>loadings </span></code></pre></div>
<pre><code>## 
## Loadings:
##          PA1    PA2   
## NoIntrst  0.750       
## Odd       0.662       
## NoFun     0.609       
## Boring    0.445 -0.287
## Alone     0.418       
## NoRelign  0.380       
## Better    0.133  0.696
## Good             0.638
## Help             0.636
## 
##                  PA1   PA2
## SS loadings    1.908 1.390
## Proportion Var 0.212 0.154
## Cumulative Var 0.212 0.366</code></pre>
<p>We can isolate the part of the output that has all of the values and round them to a certain number of decimal places if we choose.</p>
<div class="sourceCode" id="cb517"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb517-1"><a href="week-07-exploratory-factor-analysis-part-1.html#cb517-1" tabindex="-1"></a><span class="fu">round</span>(fafit<span class="sc">$</span>loadings[<span class="dv">1</span><span class="sc">:</span>n_factors,],<span class="dv">3</span>)</span></code></pre></div>
<pre><code>##             PA1    PA2
## NoIntrst  0.750  0.045
## Odd       0.662  0.050
## NoFun     0.609  0.042
## Boring    0.445 -0.287
## Alone     0.418  0.001
## NoRelign  0.380  0.072
## Better    0.133  0.696
## Good      0.032  0.638
## Help     -0.034  0.636</code></pre>
<p>Let’s assign these loadings to an object called <code>PattM</code> to remind us this is the pattern matrix. We’ll use this in our report.</p>
<div class="sourceCode" id="cb519"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb519-1"><a href="week-07-exploratory-factor-analysis-part-1.html#cb519-1" tabindex="-1"></a>PattM <span class="ot">&lt;-</span> fafit<span class="sc">$</span>loadings[<span class="dv">1</span><span class="sc">:</span>n_factors,]</span>
<span id="cb519-2"><a href="week-07-exploratory-factor-analysis-part-1.html#cb519-2" tabindex="-1"></a><span class="co">#write.csv(PatternM,&quot;PatternMatrix.csv&quot;,row.names=FALSE)</span></span></code></pre></div>
<p>If we use these same data with another program, such as SPSS, and produce the pattern matrix, the result will be close but may not be precisely the same as that produced here because of then different algorithms used in the programs <span class="citation">(<a href="#ref-R-psych">Revelle 2025</a>)</span>.<a href="#fn47" class="footnote-ref" id="fnref47"><sup>47</sup></a></p>
<p>We can look at the factor diagram using the <code>fa.diagram()</code> function in the psych package. This will display the loadings (i.e., the pattern coefficients in this oblique model). We use the <code>cut = .32</code> argument to display only those parameters with an absolute value exceeding the criterion we decided a priori. We use the <code>simple = FALSE</code> argument in case any variable has a loading on more than one factor that exceeds our cut-off criterion (setting it to <code>TRUE</code> will display only the variable’s largest loading, which can hide important information if there are complex loadings).</p>
<div class="sourceCode" id="cb520"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb520-1"><a href="week-07-exploratory-factor-analysis-part-1.html#cb520-1" tabindex="-1"></a><span class="fu">fa.diagram</span>(fafit, <span class="at">digits =</span> <span class="dv">2</span>, <span class="at">main =</span> <span class="st">&quot;Factor Diagram&quot;</span>, </span>
<span id="cb520-2"><a href="week-07-exploratory-factor-analysis-part-1.html#cb520-2" tabindex="-1"></a>           <span class="at">cut =</span> .<span class="dv">32</span>, </span>
<span id="cb520-3"><a href="week-07-exploratory-factor-analysis-part-1.html#cb520-3" tabindex="-1"></a>           <span class="at">simple =</span> F, </span>
<span id="cb520-4"><a href="week-07-exploratory-factor-analysis-part-1.html#cb520-4" tabindex="-1"></a>           <span class="at">errors =</span> T)</span></code></pre></div>
<p><img src="multivariate_stats_files/figure-html/unnamed-chunk-271-1.png" width="672" /></p>
<p>Also, notice that there does not appear to be a correlation between the two factors, at least not one that exceeds our cut value. The interfactor correlations make up the off-diagonals in the Phi matrix.</p>
<div class="sourceCode" id="cb521"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb521-1"><a href="week-07-exploratory-factor-analysis-part-1.html#cb521-1" tabindex="-1"></a><span class="fu">round</span>(fafit<span class="sc">$</span>Phi,<span class="dv">3</span>)</span></code></pre></div>
<pre><code>##       PA1   PA2
## PA1 1.000 0.003
## PA2 0.003 1.000</code></pre>
<p>The correlation between the two factors is indeed very weak. Let’s examine the <strong>structure</strong> matrix of the coefficients.</p>
<div class="sourceCode" id="cb523"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb523-1"><a href="week-07-exploratory-factor-analysis-part-1.html#cb523-1" tabindex="-1"></a>fafit<span class="sc">$</span>Structure[<span class="dv">1</span><span class="sc">:</span>n_factors,]</span></code></pre></div>
<pre><code>##                  PA1          PA2
## NoIntrst  0.74979780  0.047850884
## Odd       0.66228022  0.052343438
## NoFun     0.60881084  0.044181626
## Boring    0.44433450 -0.285554773
## Alone     0.41775315  0.002846448
## NoRelign  0.38036651  0.072977632
## Better    0.13501283  0.696717085
## Good      0.03455750  0.637999111
## Help     -0.03202188  0.636017628</code></pre>
<p>We can examine the structure matrix.<a href="#fn48" class="footnote-ref" id="fnref48"><sup>48</sup></a> In this example, the correlation between the factors is so low that the pattern and structure matrices are very similar. The structure coefficients include the correlation with the factor without partialling out the other factors, so they are often greater in magnitude than the pattern coefficients.</p>
<p>When we use oblique rotation, it is good practice to present both the pattern and structure coefficients. These are so similar that we should probably re-run the model with orthogonal rotation. This pattern would be very unexpected in a study in which we hypothesize that the two constructs underlying these factors are theoretically related.</p>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:unnamed-chunk-275">Table 8.1: </span>Pattern and structure coefficients of subtests on each factor
</caption>
<thead>
<tr>
<th style="empty-cells: hide;border-bottom:hidden;" colspan="1">
</th>
<th style="border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="2">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
Pattern coefficients
</div>
</th>
<th style="border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="2">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
Structure coefficients
</div>
</th>
</tr>
<tr>
<th style="text-align:left;">
Subtest
</th>
<th style="text-align:left;">
F1
</th>
<th style="text-align:left;">
F2
</th>
<th style="text-align:left;">
F1
</th>
<th style="text-align:left;">
F2
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
NoIntrst
</td>
<td style="text-align:left;">
<span style=" font-weight: bold;    color: black !important;">.750</span>
</td>
<td style="text-align:left;">
<span style="     color: black !important;">.045</span>
</td>
<td style="text-align:left;">
<span style=" font-weight: bold;    color: black !important;">.750</span>
</td>
<td style="text-align:left;">
<span style="     color: black !important;">.048</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
Odd
</td>
<td style="text-align:left;">
<span style=" font-weight: bold;    color: black !important;">.662</span>
</td>
<td style="text-align:left;">
<span style="     color: black !important;">.050</span>
</td>
<td style="text-align:left;">
<span style=" font-weight: bold;    color: black !important;">.662</span>
</td>
<td style="text-align:left;">
<span style="     color: black !important;">.052</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
NoFun
</td>
<td style="text-align:left;">
<span style=" font-weight: bold;    color: black !important;">.609</span>
</td>
<td style="text-align:left;">
<span style="     color: black !important;">.042</span>
</td>
<td style="text-align:left;">
<span style=" font-weight: bold;    color: black !important;">.609</span>
</td>
<td style="text-align:left;">
<span style="     color: black !important;">.044</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
Boring
</td>
<td style="text-align:left;">
<span style=" font-weight: bold;    color: black !important;">.445</span>
</td>
<td style="text-align:left;">
<span style="     color: black !important;">-.287</span>
</td>
<td style="text-align:left;">
<span style=" font-weight: bold;    color: black !important;">.444</span>
</td>
<td style="text-align:left;">
<span style="     color: black !important;">-.286</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
Alone
</td>
<td style="text-align:left;">
<span style=" font-weight: bold;    color: black !important;">.418</span>
</td>
<td style="text-align:left;">
<span style="     color: black !important;">.001</span>
</td>
<td style="text-align:left;">
<span style=" font-weight: bold;    color: black !important;">.418</span>
</td>
<td style="text-align:left;">
<span style="     color: black !important;">.003</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
NoRelign
</td>
<td style="text-align:left;">
<span style=" font-weight: bold;    color: black !important;">.380</span>
</td>
<td style="text-align:left;">
<span style="     color: black !important;">.072</span>
</td>
<td style="text-align:left;">
<span style=" font-weight: bold;    color: black !important;">.380</span>
</td>
<td style="text-align:left;">
<span style="     color: black !important;">.073</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
Better
</td>
<td style="text-align:left;">
<span style="     color: black !important;">.133</span>
</td>
<td style="text-align:left;">
<span style=" font-weight: bold;    color: black !important;">.696</span>
</td>
<td style="text-align:left;">
<span style="     color: black !important;">.135</span>
</td>
<td style="text-align:left;">
<span style=" font-weight: bold;    color: black !important;">.697</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
Good
</td>
<td style="text-align:left;">
<span style="     color: black !important;">.032</span>
</td>
<td style="text-align:left;">
<span style=" font-weight: bold;    color: black !important;">.638</span>
</td>
<td style="text-align:left;">
<span style="     color: black !important;">.035</span>
</td>
<td style="text-align:left;">
<span style=" font-weight: bold;    color: black !important;">.638</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
Help
</td>
<td style="text-align:left;">
<span style="     color: black !important;">-.034</span>
</td>
<td style="text-align:left;">
<span style=" font-weight: bold;    color: black !important;">.636</span>
</td>
<td style="text-align:left;">
<span style="     color: black !important;">-.032</span>
</td>
<td style="text-align:left;">
<span style=" font-weight: bold;    color: black !important;">.636</span>
</td>
</tr>
</tbody>
<tfoot>
<tr>
<td style="padding: 0; border:0;" colspan="100%">
<sup></sup> <em><em>Note</em></em>. Coefficients with absolute value &gt; .32 are boldfaced.
</td>
</tr>
</tfoot>
</table>
</div>
</div>
<div id="factor-analysis-with-correlation-data" class="section level3 hasAnchor" number="8.2.6">
<h3><span class="header-section-number">8.2.6</span> Factor analysis with correlation data<a href="week-07-exploratory-factor-analysis-part-1.html#factor-analysis-with-correlation-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Sometimes we only have access to correlation or covariance data. We can perform factor analysis on correlation matrices using nearly the same set of operations.<a href="#fn49" class="footnote-ref" id="fnref49"><sup>49</sup></a></p>
<div id="setting-up-the-data" class="section level4 hasAnchor" number="8.2.6.1">
<h4><span class="header-section-number">8.2.6.1</span> Setting up the data<a href="week-07-exploratory-factor-analysis-part-1.html#setting-up-the-data" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>If we had a full (symmetrical) matrix in a CSV file, we can import these into R. However, it will not be in a matrix format but as a data frame, so we need to use the <code>as.matrix()</code> function and provide names for the columns and rows.<a href="#fn50" class="footnote-ref" id="fnref50"><sup>50</sup></a></p>
<div class="sourceCode" id="cb525"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb525-1"><a href="week-07-exploratory-factor-analysis-part-1.html#cb525-1" tabindex="-1"></a>R_df <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;Week08_GSS_ScienceCorrs.csv&quot;</span>)</span>
<span id="cb525-2"><a href="week-07-exploratory-factor-analysis-part-1.html#cb525-2" tabindex="-1"></a>R    <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(R_df)</span>
<span id="cb525-3"><a href="week-07-exploratory-factor-analysis-part-1.html#cb525-3" tabindex="-1"></a><span class="fu">rownames</span>(R) <span class="ot">&lt;-</span> <span class="fu">colnames</span>(R) <span class="ot">&lt;-</span> <span class="fu">names</span>(R_df)</span>
<span id="cb525-4"><a href="week-07-exploratory-factor-analysis-part-1.html#cb525-4" tabindex="-1"></a>R</span></code></pre></div>
<pre><code>##          NoIntrst   Odd NoFun Boring  Alone NoRelign Better   Good   Help
## NoIntrst    1.000 0.504 0.454  0.286  0.351    0.292  0.123  0.049  0.006
## Odd         0.504 1.000 0.388  0.303  0.216    0.311  0.138  0.029  0.027
## NoFun       0.454 0.388 1.000  0.261  0.260    0.264  0.113  0.046  0.007
## Boring      0.286 0.303 0.261  1.000  0.252    0.090 -0.136 -0.131 -0.231
## Alone       0.351 0.216 0.260  0.252  1.000    0.096  0.076  0.031 -0.010
## NoRelign    0.292 0.311 0.264  0.090  0.096    1.000  0.067  0.051  0.046
## Better      0.123 0.138 0.113 -0.136  0.076    0.067  1.000  0.470  0.425
## Good        0.049 0.029 0.046 -0.131  0.031    0.051  0.470  1.000  0.402
## Help        0.006 0.027 0.007 -0.231 -0.010    0.046  0.425  0.402  1.000</code></pre>
<p>Now we have a matrix, called <strong>R</strong>, which we can use in our factor analysis functions.</p>
<p>One more step is that we need to specify the <em>n</em>-size, if we have access to that information. Let’s say we knew it was 372.</p>
<div class="sourceCode" id="cb527"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb527-1"><a href="week-07-exploratory-factor-analysis-part-1.html#cb527-1" tabindex="-1"></a>n_p <span class="ot">&lt;-</span> <span class="dv">372</span></span></code></pre></div>
</div>
<div id="preparatory-steps-1" class="section level4 hasAnchor" number="8.2.6.2">
<h4><span class="header-section-number">8.2.6.2</span> Preparatory steps<a href="week-07-exploratory-factor-analysis-part-1.html#preparatory-steps-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>If all we have is the correlation matrix and <em>n</em>-sizes, we won’t be able to report the descriptive statistics or the assumptions having to do with linearity. We can use most of the same procedures to determine the factorability and the number of factors to retain.</p>
<p>Paradoxically, the <code>eigenComputes()</code> function returns inflated reduced eigenvalues if we use the <code>cor = TRUE</code> argument. If we set it to <code>FALSE</code>, we observe similar results as with our raw data. For now, it seems that the Hull method from the EFA.MRFA package is not available with correlation data. The results are not reported here.</p>
<div class="sourceCode" id="cb528"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb528-1"><a href="week-07-exploratory-factor-analysis-part-1.html#cb528-1" tabindex="-1"></a><span class="fu">library</span>(nFactors)</span>
<span id="cb528-2"><a href="week-07-exploratory-factor-analysis-part-1.html#cb528-2" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb528-3"><a href="week-07-exploratory-factor-analysis-part-1.html#cb528-3" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;psych&quot;</span>)</span>
<span id="cb528-4"><a href="week-07-exploratory-factor-analysis-part-1.html#cb528-4" tabindex="-1"></a><span class="fu">KMO</span>(R)</span>
<span id="cb528-5"><a href="week-07-exploratory-factor-analysis-part-1.html#cb528-5" tabindex="-1"></a><span class="fu">cortest.bartlett</span>(R, <span class="at">n =</span> n_p)</span>
<span id="cb528-6"><a href="week-07-exploratory-factor-analysis-part-1.html#cb528-6" tabindex="-1"></a>ReducedEig <span class="ot">&lt;-</span> <span class="fu">eigenComputes</span>(R, <span class="at">cor =</span> <span class="cn">FALSE</span>, <span class="at">model =</span> <span class="st">&quot;factors&quot;</span>)</span>
<span id="cb528-7"><a href="week-07-exploratory-factor-analysis-part-1.html#cb528-7" tabindex="-1"></a>n_nu       <span class="ot">&lt;-</span> <span class="fu">ncol</span>(R) <span class="co"># The number of variables in our data</span></span>
<span id="cb528-8"><a href="week-07-exploratory-factor-analysis-part-1.html#cb528-8" tabindex="-1"></a>n_factors  <span class="ot">&lt;-</span> <span class="fu">length</span>(ReducedEig)</span>
<span id="cb528-9"><a href="week-07-exploratory-factor-analysis-part-1.html#cb528-9" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb528-10"><a href="week-07-exploratory-factor-analysis-part-1.html#cb528-10" tabindex="-1"></a>paral <span class="ot">&lt;-</span> <span class="fu">parallel</span>(<span class="at">subject =</span> n_p,</span>
<span id="cb528-11"><a href="week-07-exploratory-factor-analysis-part-1.html#cb528-11" tabindex="-1"></a>                      <span class="at">var =</span> n_nu,</span>
<span id="cb528-12"><a href="week-07-exploratory-factor-analysis-part-1.html#cb528-12" tabindex="-1"></a>                      <span class="at">rep =</span> <span class="dv">100</span>,</span>
<span id="cb528-13"><a href="week-07-exploratory-factor-analysis-part-1.html#cb528-13" tabindex="-1"></a>                 <span class="at">quantile =</span> .<span class="dv">95</span>, </span>
<span id="cb528-14"><a href="week-07-exploratory-factor-analysis-part-1.html#cb528-14" tabindex="-1"></a>                    <span class="at">model =</span> <span class="st">&quot;factors&quot;</span>)</span>
<span id="cb528-15"><a href="week-07-exploratory-factor-analysis-part-1.html#cb528-15" tabindex="-1"></a>ParallelAna <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">Nfactor=</span> <span class="dv">1</span><span class="sc">:</span>n_factors,</span>
<span id="cb528-16"><a href="week-07-exploratory-factor-analysis-part-1.html#cb528-16" tabindex="-1"></a>                        ReducedEig,</span>
<span id="cb528-17"><a href="week-07-exploratory-factor-analysis-part-1.html#cb528-17" tabindex="-1"></a>                        <span class="at">RandEigM =</span> paral<span class="sc">$</span>eigen<span class="sc">$</span>mevpea,</span>
<span id="cb528-18"><a href="week-07-exploratory-factor-analysis-part-1.html#cb528-18" tabindex="-1"></a>                       <span class="at">RandEig95 =</span> paral<span class="sc">$</span>eigen<span class="sc">$</span>qevpea)</span>
<span id="cb528-19"><a href="week-07-exploratory-factor-analysis-part-1.html#cb528-19" tabindex="-1"></a>ParallelAna <span class="ot">&lt;-</span> <span class="fu">round</span>(ParallelAna, <span class="dv">3</span>)</span>
<span id="cb528-20"><a href="week-07-exploratory-factor-analysis-part-1.html#cb528-20" tabindex="-1"></a>ParallelAna</span>
<span id="cb528-21"><a href="week-07-exploratory-factor-analysis-part-1.html#cb528-21" tabindex="-1"></a><span class="co"># Scree plot</span></span>
<span id="cb528-22"><a href="week-07-exploratory-factor-analysis-part-1.html#cb528-22" tabindex="-1"></a>scree <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">Factor_n =</span> <span class="fu">as.factor</span>(<span class="dv">1</span><span class="sc">:</span>n_factors), </span>
<span id="cb528-23"><a href="week-07-exploratory-factor-analysis-part-1.html#cb528-23" tabindex="-1"></a>                  <span class="at">Eigenvalue =</span> ReducedEig)</span>
<span id="cb528-24"><a href="week-07-exploratory-factor-analysis-part-1.html#cb528-24" tabindex="-1"></a><span class="fu">ggplot</span>(scree, <span class="fu">aes</span>(<span class="at">x =</span> Factor_n, <span class="at">y =</span> Eigenvalue, <span class="at">group =</span> <span class="dv">1</span>)) <span class="sc">+</span> </span>
<span id="cb528-25"><a href="week-07-exploratory-factor-analysis-part-1.html#cb528-25" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb528-26"><a href="week-07-exploratory-factor-analysis-part-1.html#cb528-26" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;Number of factors&quot;</span>) <span class="sc">+</span></span>
<span id="cb528-27"><a href="week-07-exploratory-factor-analysis-part-1.html#cb528-27" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">&quot;Eigenvalue&quot;</span>) <span class="sc">+</span></span>
<span id="cb528-28"><a href="week-07-exploratory-factor-analysis-part-1.html#cb528-28" tabindex="-1"></a>  <span class="fu">labs</span>( <span class="at">title =</span> <span class="st">&quot;Scree Plot&quot;</span>, </span>
<span id="cb528-29"><a href="week-07-exploratory-factor-analysis-part-1.html#cb528-29" tabindex="-1"></a>     <span class="at">subtitle =</span> <span class="st">&quot;(Based on the reduced correlation matrix)&quot;</span>)</span></code></pre></div>
</div>
<div id="performing-the-factor-analysis" class="section level4 hasAnchor" number="8.2.6.3">
<h4><span class="header-section-number">8.2.6.3</span> Performing the factor analysis<a href="week-07-exploratory-factor-analysis-part-1.html#performing-the-factor-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We can use the same <code>fa()</code> function we used with the raw data set, but instead use the correlation matrix and specify the number of observations using the <code>n.obs =</code> argument. The results will not be identical to those in our raw data set because our correlation matrix was based on rounded values, which is typical when we use data from publications or reports.</p>
<div class="sourceCode" id="cb529"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb529-1"><a href="week-07-exploratory-factor-analysis-part-1.html#cb529-1" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;psych&quot;</span>)</span>
<span id="cb529-2"><a href="week-07-exploratory-factor-analysis-part-1.html#cb529-2" tabindex="-1"></a>fafit2 <span class="ot">&lt;-</span> <span class="fu">fa</span>(R, </span>
<span id="cb529-3"><a href="week-07-exploratory-factor-analysis-part-1.html#cb529-3" tabindex="-1"></a>             <span class="at">nfactors =</span> <span class="dv">2</span>, </span>
<span id="cb529-4"><a href="week-07-exploratory-factor-analysis-part-1.html#cb529-4" tabindex="-1"></a>             <span class="at">n.obs =</span> n_p, </span>
<span id="cb529-5"><a href="week-07-exploratory-factor-analysis-part-1.html#cb529-5" tabindex="-1"></a>             <span class="at">fm =</span> <span class="st">&quot;pa&quot;</span>, </span>
<span id="cb529-6"><a href="week-07-exploratory-factor-analysis-part-1.html#cb529-6" tabindex="-1"></a>             <span class="at">rotate =</span> <span class="st">&quot;promax&quot;</span>)</span>
<span id="cb529-7"><a href="week-07-exploratory-factor-analysis-part-1.html#cb529-7" tabindex="-1"></a>fafit2</span></code></pre></div>
</div>
</div>
</div>
<div id="references-1" class="section level2 hasAnchor" number="8.3">
<h2><span class="header-section-number">8.3</span> References<a href="week-07-exploratory-factor-analysis-part-1.html#references-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-R-car" class="csl-entry">
Fox, John, Sanford Weisberg, and Brad Price. 2024. <em>Car: Companion to Applied Regression</em>. <a href="https://r-forge.r-project.org/projects/car/">https://r-forge.r-project.org/projects/car/</a>.
</div>
<div id="ref-horn_rationale_1965" class="csl-entry">
Horn, John L. 1965. <span>“A Rationale and Test for the Number of Factors in Factor Analysis.”</span> <em>Psychometrika</em> 30 (2): 179–85. <a href="https://doi.org/10.1007/BF02289447">https://doi.org/10.1007/BF02289447</a>.
</div>
<div id="ref-R-mvnormtest" class="csl-entry">
Jarek, Sławomir. 2024. <em>Mvnormtest: Normality Test for Multivariate Variables</em>. <a href="https://doi.org/10.32614/CRAN.package.mvnormtest">https://doi.org/10.32614/CRAN.package.mvnormtest</a>.
</div>
<div id="ref-kaiser_index_1974" class="csl-entry">
Kaiser, Henry F. 1974. <span>“An Index of Factorial Simplicity.”</span> <em>Psychometrika</em> 39 (1): 31–36. <a href="https://doi.org/10.1007/BF02291575">https://doi.org/10.1007/BF02291575</a>.
</div>
<div id="ref-R-corrr" class="csl-entry">
Kuhn, Max, Simon Jackson, and Jorge Cimentada. 2025. <em>Corrr: Correlations in r</em>. <a href="https://github.com/tidymodels/corrr">https://github.com/tidymodels/corrr</a>.
</div>
<div id="ref-lorenzo-seva_hull_2011" class="csl-entry">
Lorenzo-Seva, Urbano, Marieke E. Timmerman, and Henk A. L. Kiers. 2011. <span>“The <span>Hull</span> Method for Selecting the Number of Common Factors.”</span> <em>Multivariate Behavioral Research</em> 46 (2): 340–64. <a href="https://doi.org/10.1080/00273171.2011.564527">https://doi.org/10.1080/00273171.2011.564527</a>.
</div>
<div id="ref-luo_exploratory_2019" class="csl-entry">
Luo, Lan, Cara Arizmendi, and Kathleen M. Gates. 2019. <span>“Exploratory Factor Analysis (<span>EFA</span>) Programs in <span>R</span>.”</span> <em>Structural Equation Modeling: A Multidisciplinary Journal</em> 26 (5): 819–26. <a href="https://doi.org/10.1080/10705511.2019.1615835">https://doi.org/10.1080/10705511.2019.1615835</a>.
</div>
<div id="ref-R-EFA.MRFA" class="csl-entry">
Navarro-Gonzalez, David, and Urbano Lorenzo-Seva. 2021. <em>EFA.MRFA: Dimensionality Assessment Using Minimum Rank Factor Analysis</em>. <a href="https://doi.org/10.32614/CRAN.package.EFA.MRFA">https://doi.org/10.32614/CRAN.package.EFA.MRFA</a>.
</div>
<div id="ref-R-nFactors" class="csl-entry">
Raiche, Gilles, and David Magis. 2025. <em>nFactors: Parallel Analysis and Other Non Graphical Solutions to the Cattell Scree Test</em>. <a href="https://doi.org/10.32614/CRAN.package.nFactors">https://doi.org/10.32614/CRAN.package.nFactors</a>.
</div>
<div id="ref-R-psych" class="csl-entry">
Revelle, William. 2025. <em>Psych: Procedures for Psychological, Psychometric, and Personality Research</em>. <a href="https://personality-project.org/r/psych/">https://personality-project.org/r/psych/</a>.
</div>
<div id="ref-tabachnick_using_2013" class="csl-entry">
Tabachnick, Barbara G., and Linda S. Fidell. 2013. <em>Using Multivariate Statistics</em>. 6th ed. Boston: Pearson Education.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="36">
<li id="fn36"><p>We can use the corrr package <span class="citation">(<a href="#ref-R-corrr">Kuhn, Jackson, and Cimentada 2025</a>)</span> to get prettified correlation outputs.<a href="week-07-exploratory-factor-analysis-part-1.html#fnref36" class="footnote-back">↩︎</a></p></li>
<li id="fn37"><p>As with all of our analyses, missing data can be a problem, too.<a href="week-07-exploratory-factor-analysis-part-1.html#fnref37" class="footnote-back">↩︎</a></p></li>
<li id="fn38"><p>There are alternative methods we can use with data that do not meet this assumption. One is to use a polychoric correlation, which we do in the second part of our factor-analysis journey. We can also use polytomous item-response modeling such as the partial credit model or the graded response model.<a href="week-07-exploratory-factor-analysis-part-1.html#fnref38" class="footnote-back">↩︎</a></p></li>
<li id="fn39"><p>Here’s the code for printing an ordered set using the Base R approach, which is a little convoluted but is useful if tidyverse misbehaves: <code>out.liars[ order(out.liars$distances, decreasing = TRUE) , ]</code>.<a href="week-07-exploratory-factor-analysis-part-1.html#fnref39" class="footnote-back">↩︎</a></p></li>
<li id="fn40"><p>We can also use the psych package’s <code>outlier()</code> function to plot multivariate outliers. <code>psych::outlier(dat)</code>.<a href="week-07-exploratory-factor-analysis-part-1.html#fnref40" class="footnote-back">↩︎</a></p></li>
<li id="fn41"><p>The psych package and EFA.MRFA package also offer functions for parallel analysis. Another option is to use the paran package, where our code might look like this: <code>paran(dat, centile = 95, cfa = TRUE, all = TRUE, graph = TRUE, seed = 123)</code>. In those results, we can report the number of adjusted eigenvalues above zero—that adjustment is explained in the documents cited in the package’s help files. If we have a correlation matrix instead of raw data, we should replace the first argument with two arguments to specify the name of our correlation matrix and the number of observations in our data, such as <code>mat = R, n = n_p</code>.<a href="week-07-exploratory-factor-analysis-part-1.html#fnref41" class="footnote-back">↩︎</a></p></li>
<li id="fn42"><p>The reduced eigenvalues are not appropriate for the K1 criterion.<a href="week-07-exploratory-factor-analysis-part-1.html#fnref42" class="footnote-back">↩︎</a></p></li>
<li id="fn43"><p>We could also set the extraction method to be <code>estr = "ML"</code> rather than the default, which is <code>estr = "ULS"</code>. Also, this function seems to require a raw data set (not a correlation or covariance matrices) and have zero missing data, so some data cleaning may be required before using this function.<a href="week-07-exploratory-factor-analysis-part-1.html#fnref43" class="footnote-back">↩︎</a></p></li>
<li id="fn44"><p>We can calculate the degrees of freedom as the number of variance and covariance elements in our raw data minus the number of parameters in the model. The number of elements is <span class="math inline">\(\frac{\nu(\nu+1)}{2}\)</span>, where <span class="math inline">\(\nu\)</span> is the number of subtests. With our <span class="math inline">\(9\)</span> subtests, we have <span class="math inline">\(45\)</span> elements. This is the number of elements in the variance-covariance matrix that include the diagonal and one side of the off-diagonal—in other words, all of the variances and unique covariances. The number of parameters is <span class="math inline">\(\nu + \nu{m}-\frac{m(m-1)}{2}\)</span>, where <span class="math inline">\(m\)</span> is the number of factors. This corresponds with the number of error estimates (<span class="math inline">\(\nu\)</span>), the number of loadings on the factors (<span class="math inline">\(\nu{m}\)</span>), and the number of covariances among the factors (<span class="math inline">\(\frac{m(m-1)}{2}\)</span>). With a two-factor model with <span class="math inline">\(9\)</span> subtests, we have <span class="math inline">\(9\)</span> error estimates, <span class="math inline">\(18\)</span> loadings, and <span class="math inline">\(1\)</span> covariance that is allowed between the two factors, so the number of parameters is <span class="math inline">\(9 + 2*9 -1 = 26\)</span> and the degrees of freedom is <span class="math inline">\(45 - 26 = 19\)</span>.<a href="week-07-exploratory-factor-analysis-part-1.html#fnref44" class="footnote-back">↩︎</a></p></li>
<li id="fn45"><p>Yes, “comm<strong>u</strong>nalities”, with a “u”, is the correct spelling.<a href="week-07-exploratory-factor-analysis-part-1.html#fnref45" class="footnote-back">↩︎</a></p></li>
<li id="fn46"><p>The value <span class="math inline">\(.32\)</span> is handy because we can interpret its squared value, <span class="math inline">\(0.1024\)</span>, as the minimum proportion of variance (i.e., 10%) in the observed variable (subtests) we wish to consider to be salient enough for consideration.<a href="week-07-exploratory-factor-analysis-part-1.html#fnref46" class="footnote-back">↩︎</a></p></li>
<li id="fn47"><p>For example, according to Revelle <span class="citation">(<a href="#ref-R-psych">2025</a>)</span>, SPSS uses Kaiser normalization whereas the psych packages <code>fa()</code> function does not.<a href="week-07-exploratory-factor-analysis-part-1.html#fnref47" class="footnote-back">↩︎</a></p></li>
<li id="fn48"><p>We could reproduce the structure coefficient matrix with matrix multiplication of the pattern-coefficient matrix and <span class="math inline">\(\Phi\)</span> <code>PattM %*% fafit$Phi</code>.<a href="week-07-exploratory-factor-analysis-part-1.html#fnref48" class="footnote-back">↩︎</a></p></li>
<li id="fn49"><p>If all we have is the correlation matrix, we are, of course, not able to estimate persons’ factor scores because we have no person-level data.<a href="week-07-exploratory-factor-analysis-part-1.html#fnref49" class="footnote-back">↩︎</a></p></li>
<li id="fn50"><p>In this step, we named the columns and rows of the matrix using <code>colnames()</code> and <code>rownames()</code> but we can also use the <code>dimnames =</code> argument in the <code>as.matrix()</code> function.<a href="week-07-exploratory-factor-analysis-part-1.html#fnref50" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="week-06-principal-components-analysis.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="week-08-exploratory-factor-analysis-part-2.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": null,
    "text": null
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
