<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>7 Week 06: Principal Components Analysis | Guidebook for Using R in Applied Multivariate Statistics</title>
  <meta name="description" content="Course guidebook for multivariate statistics" />
  <meta name="generator" content="bookdown 0.44 and GitBook 2.6.7" />

  <meta property="og:title" content="7 Week 06: Principal Components Analysis | Guidebook for Using R in Applied Multivariate Statistics" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Course guidebook for multivariate statistics" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="7 Week 06: Principal Components Analysis | Guidebook for Using R in Applied Multivariate Statistics" />
  
  <meta name="twitter:description" content="Course guidebook for multivariate statistics" />
  

<meta name="author" content="George M. Harrison" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="week-05-discriminant-function-analysis.html"/>
<link rel="next" href="week-07-exploratory-factor-analysis-part-1.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<link href="libs/tabwid-1.1.3/tabwid.css" rel="stylesheet" />
<script src="libs/tabwid-1.1.3/tabwid.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Multivariate Statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><i class="fa fa-check"></i><b>1</b> Terminology and Matrix Manipulations Used in Multivariate Statistics</a>
<ul>
<li class="chapter" data-level="1.1" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#scalars"><i class="fa fa-check"></i><b>1.1</b> Scalars</a></li>
<li class="chapter" data-level="1.2" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#vectors"><i class="fa fa-check"></i><b>1.2</b> Vectors</a></li>
<li class="chapter" data-level="1.3" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#matrices"><i class="fa fa-check"></i><b>1.3</b> Matrices</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#data-files-as-matrices"><i class="fa fa-check"></i><b>1.3.1</b> Data Files as Matrices</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#symmetrical-matrices"><i class="fa fa-check"></i><b>1.4</b> Symmetrical Matrices</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#variance-covariance-matrices"><i class="fa fa-check"></i><b>1.4.1</b> Variance-covariance Matrices</a></li>
<li class="chapter" data-level="1.4.2" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#greek-letters"><i class="fa fa-check"></i><b>1.4.2</b> Greek letters</a></li>
<li class="chapter" data-level="1.4.3" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#example-variance-covariance-matrix"><i class="fa fa-check"></i><b>1.4.3</b> Example variance-covariance matrix</a></li>
<li class="chapter" data-level="1.4.4" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#the-diagonal-and-trace"><i class="fa fa-check"></i><b>1.4.4</b> The diagonal and trace</a></li>
<li class="chapter" data-level="1.4.5" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#an-identity-matrix"><i class="fa fa-check"></i><b>1.4.5</b> An identity matrix</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#transposed-vectors-and-matrices"><i class="fa fa-check"></i><b>1.5</b> Transposed Vectors and Matrices</a></li>
<li class="chapter" data-level="1.6" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#matrix-addition-and-subtraction"><i class="fa fa-check"></i><b>1.6</b> Matrix Addition and Subtraction</a></li>
<li class="chapter" data-level="1.7" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#operations-on-matrices-with-a-scalar"><i class="fa fa-check"></i><b>1.7</b> Operations on Matrices with a Scalar</a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#sum-of-square-and-cross-product-sscp-matrices"><i class="fa fa-check"></i><b>1.7.1</b> Sum-of-square-and-cross-product (SSCP) matrices</a></li>
<li class="chapter" data-level="1.7.2" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#operations-on-elements-of-a-matrix-example-with-a-correlation-matrix"><i class="fa fa-check"></i><b>1.7.2</b> Operations on elements of a matrix: Example with a correlation matrix</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#matrix-multiplication"><i class="fa fa-check"></i><b>1.8</b> Matrix Multiplication</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#another-example-of-matrix-multiplication-to-get-the-sscp"><i class="fa fa-check"></i><b>1.8.1</b> Another example of matrix multiplication, to get the SSCP</a></li>
<li class="chapter" data-level="1.8.2" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#the-ordering-is-important"><i class="fa fa-check"></i><b>1.8.2</b> The ordering is important</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#inverse-of-a-matrix"><i class="fa fa-check"></i><b>1.9</b> Inverse of a Matrix</a></li>
<li class="chapter" data-level="1.10" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#matrix-determinants-as-generalized-variance"><i class="fa fa-check"></i><b>1.10</b> Matrix determinants, as generalized variance</a></li>
<li class="chapter" data-level="1.11" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#eigenvectors-and-eigenvalues"><i class="fa fa-check"></i><b>1.11</b> Eigenvectors and Eigenvalues</a></li>
<li class="chapter" data-level="1.12" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#summary"><i class="fa fa-check"></i><b>1.12</b> Summary</a></li>
<li class="chapter" data-level="1.13" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#optional-spss-and-r-code-to-manipulate-matrices"><i class="fa fa-check"></i><b>1.13</b> Optional: SPSS and R Code to Manipulate Matrices</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="hotellings-t2.html"><a href="hotellings-t2.html"><i class="fa fa-check"></i><b>2</b> Hotelling’s <span class="math inline">\(T^2\)</span></a>
<ul>
<li class="chapter" data-level="2.1" data-path="hotellings-t2.html"><a href="hotellings-t2.html#within-group-covariance-matrices"><i class="fa fa-check"></i><b>2.1</b> Within-group covariance matrices</a></li>
<li class="chapter" data-level="2.2" data-path="hotellings-t2.html"><a href="hotellings-t2.html#sscp-matrices"><i class="fa fa-check"></i><b>2.2</b> SSCP matrices</a></li>
<li class="chapter" data-level="2.3" data-path="hotellings-t2.html"><a href="hotellings-t2.html#conducting-the-test-using-functions"><i class="fa fa-check"></i><b>2.3</b> Conducting the Test using Functions</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="planned-comparisons-and-contrasts.html"><a href="planned-comparisons-and-contrasts.html"><i class="fa fa-check"></i><b>3</b> Planned Comparisons and Contrasts</a>
<ul>
<li class="chapter" data-level="3.1" data-path="planned-comparisons-and-contrasts.html"><a href="planned-comparisons-and-contrasts.html#data-from-week-1"><i class="fa fa-check"></i><b>3.1</b> Data from Week 1</a></li>
<li class="chapter" data-level="3.2" data-path="planned-comparisons-and-contrasts.html"><a href="planned-comparisons-and-contrasts.html#orthogonal-contrasts"><i class="fa fa-check"></i><b>3.2</b> Orthogonal contrasts</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="planned-comparisons-and-contrasts.html"><a href="planned-comparisons-and-contrasts.html#conditions-for-orthogonal-contrasts"><i class="fa fa-check"></i><b>3.2.1</b> Conditions for orthogonal contrasts</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="planned-comparisons-and-contrasts.html"><a href="planned-comparisons-and-contrasts.html#non-orthogonal-contrasts"><i class="fa fa-check"></i><b>3.3</b> Non-orthogonal contrasts</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="planned-comparisons-and-contrasts.html"><a href="planned-comparisons-and-contrasts.html#accounting-for-type-i-error-in-non-orthogonal-contrasts."><i class="fa fa-check"></i><b>3.3.1</b> Accounting for Type I error in non-orthogonal contrasts.</a></li>
<li class="chapter" data-level="3.3.2" data-path="planned-comparisons-and-contrasts.html"><a href="planned-comparisons-and-contrasts.html#post-hoc-tests"><i class="fa fa-check"></i><b>3.3.2</b> Post-hoc tests</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html"><i class="fa fa-check"></i><b>4</b> Week 03 Statistical Assumptions of MANOVA</a>
<ul>
<li class="chapter" data-level="4.1" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html#data"><i class="fa fa-check"></i><b>4.1</b> Data</a></li>
<li class="chapter" data-level="4.2" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html#descriptive-statistics"><i class="fa fa-check"></i><b>4.2</b> Descriptive statistics</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html#by-group-descriptive-statistics"><i class="fa fa-check"></i><b>4.2.1</b> By-group descriptive statistics</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html#normality-assumption"><i class="fa fa-check"></i><b>4.3</b> Normality Assumption</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html#univariate-normality"><i class="fa fa-check"></i><b>4.3.1</b> Univariate Normality</a></li>
<li class="chapter" data-level="4.3.2" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html#examining-univariate-normality-using-quantile-quantile-plots"><i class="fa fa-check"></i><b>4.3.2</b> Examining univariate normality using quantile-quantile plots</a></li>
<li class="chapter" data-level="4.3.3" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html#a-caution-about-statistical-tests-of-statistical-assumptions"><i class="fa fa-check"></i><b>4.3.3</b> A caution about statistical tests of statistical assumptions</a></li>
<li class="chapter" data-level="4.3.4" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html#mahalanobis-distance-and-multivariate-outliers"><i class="fa fa-check"></i><b>4.3.4</b> Mahalanobis Distance and Multivariate Outliers</a></li>
<li class="chapter" data-level="4.3.5" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html#statistical-tests-of-multivariate-normality"><i class="fa fa-check"></i><b>4.3.5</b> Statistical Tests of Multivariate Normality</a></li>
<li class="chapter" data-level="4.3.6" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html#homogeneity-of-covariance"><i class="fa fa-check"></i><b>4.3.6</b> Homogeneity of Covariance</a></li>
<li class="chapter" data-level="4.3.7" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html#conclusions-about-our-statistical-assumptions"><i class="fa fa-check"></i><b>4.3.7</b> Conclusions about our statistical assumptions</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html#fitting-the-manova-model"><i class="fa fa-check"></i><b>4.4</b> Fitting the MANOVA Model</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html#follow-up-analysis-with-univariate-f-tests"><i class="fa fa-check"></i><b>4.4.1</b> Follow-up analysis with univariate <em>F</em>-tests</a></li>
<li class="chapter" data-level="4.4.2" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html#effect-size"><i class="fa fa-check"></i><b>4.4.2</b> Effect size</a></li>
<li class="chapter" data-level="4.4.3" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html#comparing-the-model-without-outlier"><i class="fa fa-check"></i><b>4.4.3</b> Comparing the Model Without Outlier</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html#robust-one-way-manova"><i class="fa fa-check"></i><b>4.5</b> Robust One-way MANOVA</a></li>
<li class="chapter" data-level="4.6" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html#summary-1"><i class="fa fa-check"></i><b>4.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="week-04-factorial-manova.html"><a href="week-04-factorial-manova.html"><i class="fa fa-check"></i><b>5</b> Week 04 Factorial MANOVA</a>
<ul>
<li class="chapter" data-level="5.1" data-path="week-04-factorial-manova.html"><a href="week-04-factorial-manova.html#following-tabachnick-and-fidells-notes"><i class="fa fa-check"></i><b>5.1</b> Following Tabachnick and Fidell’s notes</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="week-04-factorial-manova.html"><a href="week-04-factorial-manova.html#another-way-to-get-the-sscp_w"><i class="fa fa-check"></i><b>5.1.1</b> Another way to get the <span class="math inline">\(SSCP_W\)</span>:</a></li>
<li class="chapter" data-level="5.1.2" data-path="week-04-factorial-manova.html"><a href="week-04-factorial-manova.html#mathbfsscp_between-for-each-factor"><i class="fa fa-check"></i><b>5.1.2</b> <span class="math inline">\(\mathbf{SSCP_{Between}}\)</span> for each factor</a></li>
<li class="chapter" data-level="5.1.3" data-path="week-04-factorial-manova.html"><a href="week-04-factorial-manova.html#mathbfsscp_between-for-the-interaction"><i class="fa fa-check"></i><b>5.1.3</b> <span class="math inline">\(\mathbf{SSCP_{Between}}\)</span> for the interaction</a></li>
<li class="chapter" data-level="5.1.4" data-path="week-04-factorial-manova.html"><a href="week-04-factorial-manova.html#these-by-hand-calculations-will-be-available-in-manova."><i class="fa fa-check"></i><b>5.1.4</b> These by-hand calculations will be available in <code>manova()</code>.</a></li>
<li class="chapter" data-level="5.1.5" data-path="week-04-factorial-manova.html"><a href="week-04-factorial-manova.html#calculating-the-generalized-variance"><i class="fa fa-check"></i><b>5.1.5</b> Calculating the generalized variance</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="week-04-factorial-manova.html"><a href="week-04-factorial-manova.html#calculating-wilks-lambda"><i class="fa fa-check"></i><b>5.2</b> Calculating Wilks’ Lambda</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="week-04-factorial-manova.html"><a href="week-04-factorial-manova.html#fit-the-factorial-manova"><i class="fa fa-check"></i><b>5.2.1</b> Fit the factorial MANOVA</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="week-04-factorial-manova.html"><a href="week-04-factorial-manova.html#mancova"><i class="fa fa-check"></i><b>5.3</b> MANCOVA</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="week-04-factorial-manova.html"><a href="week-04-factorial-manova.html#mancova-example"><i class="fa fa-check"></i><b>5.3.1</b> MANCOVA example</a></li>
<li class="chapter" data-level="5.3.2" data-path="week-04-factorial-manova.html"><a href="week-04-factorial-manova.html#lets-think-about-ordering-and-type-i-sums-of-squares"><i class="fa fa-check"></i><b>5.3.2</b> Let’s think about ordering and Type I sums of squares</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="week-05-discriminant-function-analysis.html"><a href="week-05-discriminant-function-analysis.html"><i class="fa fa-check"></i><b>6</b> Week 05, Discriminant Function Analysis</a>
<ul>
<li class="chapter" data-level="6.0.1" data-path="week-05-discriminant-function-analysis.html"><a href="week-05-discriminant-function-analysis.html#fitting-the-discriminant-function-analysis-model"><i class="fa fa-check"></i><b>6.0.1</b> Fitting the discriminant function analysis model</a></li>
<li class="chapter" data-level="6.0.2" data-path="week-05-discriminant-function-analysis.html"><a href="week-05-discriminant-function-analysis.html#dimension-reduction-analysis"><i class="fa fa-check"></i><b>6.0.2</b> Dimension reduction analysis</a></li>
<li class="chapter" data-level="6.0.3" data-path="week-05-discriminant-function-analysis.html"><a href="week-05-discriminant-function-analysis.html#structure-and-standardized-discriminant-function-coefficients"><i class="fa fa-check"></i><b>6.0.3</b> Structure and standardized discriminant function coefficients</a></li>
<li class="chapter" data-level="6.0.4" data-path="week-05-discriminant-function-analysis.html"><a href="week-05-discriminant-function-analysis.html#discriminant-function-scores"><i class="fa fa-check"></i><b>6.0.4</b> Discriminant function scores</a></li>
<li class="chapter" data-level="6.0.5" data-path="week-05-discriminant-function-analysis.html"><a href="week-05-discriminant-function-analysis.html#group-means-centroids-of-the-discriminant-function-scores"><i class="fa fa-check"></i><b>6.0.5</b> Group means (centroids) of the discriminant-function scores</a></li>
<li class="chapter" data-level="6.0.6" data-path="week-05-discriminant-function-analysis.html"><a href="week-05-discriminant-function-analysis.html#bivariate-scatterplot"><i class="fa fa-check"></i><b>6.0.6</b> Bivariate scatterplot</a></li>
<li class="chapter" data-level="6.1" data-path="week-05-discriminant-function-analysis.html"><a href="week-05-discriminant-function-analysis.html#write-up"><i class="fa fa-check"></i><b>6.1</b> Write-up</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html"><i class="fa fa-check"></i><b>7</b> Week 06: Principal Components Analysis</a>
<ul>
<li class="chapter" data-level="7.1" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#example-1-to-see-some-properties-of-pca"><i class="fa fa-check"></i><b>7.1</b> Example 1, to see some properties of PCA</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#pca-with-covariance-vs.-correlation"><i class="fa fa-check"></i><b>7.1.1</b> PCA with Covariance vs. Correlation</a></li>
<li class="chapter" data-level="7.1.2" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#unscaled-eigenvectors"><i class="fa fa-check"></i><b>7.1.2</b> Unscaled eigenvectors</a></li>
<li class="chapter" data-level="7.1.3" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#looking-at-the-properties-of-eigenvectors-and-eigenvalues"><i class="fa fa-check"></i><b>7.1.3</b> Looking at the properties of eigenvectors and eigenvalues</a></li>
<li class="chapter" data-level="7.1.4" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#using-the-psych-package"><i class="fa fa-check"></i><b>7.1.4</b> Using the psych package</a></li>
<li class="chapter" data-level="7.1.5" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#determining-how-many-components-to-retain"><i class="fa fa-check"></i><b>7.1.5</b> Determining how many components to retain</a></li>
<li class="chapter" data-level="7.1.6" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#reduced-pca-model"><i class="fa fa-check"></i><b>7.1.6</b> Reduced PCA model</a></li>
<li class="chapter" data-level="7.1.7" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#interpretation-1"><i class="fa fa-check"></i><b>7.1.7</b> Interpretation</a></li>
<li class="chapter" data-level="7.1.8" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#interpretation-of-the-rotated-solution"><i class="fa fa-check"></i><b>7.1.8</b> Interpretation of the rotated solution</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#example-two"><i class="fa fa-check"></i><b>7.2</b> Example Two</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#pca-on-unstandardized-scores"><i class="fa fa-check"></i><b>7.2.1</b> PCA on Unstandardized Scores</a></li>
<li class="chapter" data-level="7.2.2" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#pca-on-the-standardized-scores"><i class="fa fa-check"></i><b>7.2.2</b> PCA on the Standardized Scores</a></li>
<li class="chapter" data-level="7.2.3" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#let-the-psych-package-do-the-work"><i class="fa fa-check"></i><b>7.2.3</b> Let the Psych Package Do the Work</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#heptathlon-example"><i class="fa fa-check"></i><b>7.3</b> Heptathlon example</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#data-and-data-cleaning"><i class="fa fa-check"></i><b>7.3.1</b> Data and data cleaning</a></li>
<li class="chapter" data-level="7.3.2" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#examine-assumption-of-linearity"><i class="fa fa-check"></i><b>7.3.2</b> Examine assumption of linearity</a></li>
<li class="chapter" data-level="7.3.3" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#multivariate-normality-and-outliers"><i class="fa fa-check"></i><b>7.3.3</b> Multivariate normality and outliers</a></li>
<li class="chapter" data-level="7.3.4" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#re-examine-linearity-and-normality"><i class="fa fa-check"></i><b>7.3.4</b> Re-examine linearity and normality</a></li>
<li class="chapter" data-level="7.3.5" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#fit-the-initial-pca"><i class="fa fa-check"></i><b>7.3.5</b> Fit the initial PCA</a></li>
<li class="chapter" data-level="7.3.6" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#determine-the-number-of-components-to-retain"><i class="fa fa-check"></i><b>7.3.6</b> Determine the number of components to retain</a></li>
<li class="chapter" data-level="7.3.7" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#refit-the-pca-model-using-rotation-if-that-is-acceptable-in-your-field"><i class="fa fa-check"></i><b>7.3.7</b> Refit the PCA model, using rotation if that is acceptable in your field</a></li>
<li class="chapter" data-level="7.3.8" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#interpret-the-components"><i class="fa fa-check"></i><b>7.3.8</b> Interpret the components</a></li>
<li class="chapter" data-level="7.3.9" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#saving-the-scores"><i class="fa fa-check"></i><b>7.3.9</b> Saving the scores</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#other-example-from-later-in-the-chapter-not-assigned"><i class="fa fa-check"></i><b>7.4</b> Other example from later in the chapter (not assigned)</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#usairpollution-data"><i class="fa fa-check"></i><b>7.4.1</b> USairpollution Data</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#another-resource"><i class="fa fa-check"></i><b>7.5</b> Another resource</a></li>
<li class="chapter" data-level="7.6" data-path="week-06-principal-components-analysis.html"><a href="week-06-principal-components-analysis.html#references"><i class="fa fa-check"></i><b>7.6</b> References</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="week-07-exploratory-factor-analysis-part-1.html"><a href="week-07-exploratory-factor-analysis-part-1.html"><i class="fa fa-check"></i><b>8</b> Week 07: Exploratory Factor Analysis, Part 1</a>
<ul>
<li class="chapter" data-level="8.1" data-path="week-07-exploratory-factor-analysis-part-1.html"><a href="week-07-exploratory-factor-analysis-part-1.html#introductory-comments"><i class="fa fa-check"></i><b>8.1</b> Introductory comments</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="week-07-exploratory-factor-analysis-part-1.html"><a href="week-07-exploratory-factor-analysis-part-1.html#packages"><i class="fa fa-check"></i><b>8.1.1</b> Packages</a></li>
<li class="chapter" data-level="8.1.2" data-path="week-07-exploratory-factor-analysis-part-1.html"><a href="week-07-exploratory-factor-analysis-part-1.html#efa-ne-pca"><i class="fa fa-check"></i><b>8.1.2</b> EFA <span class="math inline">\(\ne\)</span> PCA</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="week-07-exploratory-factor-analysis-part-1.html"><a href="week-07-exploratory-factor-analysis-part-1.html#preparatory-steps"><i class="fa fa-check"></i><b>8.2</b> Preparatory steps</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="week-07-exploratory-factor-analysis-part-1.html"><a href="week-07-exploratory-factor-analysis-part-1.html#examining-the-data"><i class="fa fa-check"></i><b>8.2.1</b> Examining the data</a></li>
<li class="chapter" data-level="8.2.2" data-path="week-07-exploratory-factor-analysis-part-1.html"><a href="week-07-exploratory-factor-analysis-part-1.html#addressing-assumptions"><i class="fa fa-check"></i><b>8.2.2</b> Addressing assumptions</a></li>
<li class="chapter" data-level="8.2.3" data-path="week-07-exploratory-factor-analysis-part-1.html"><a href="week-07-exploratory-factor-analysis-part-1.html#determining-factorability"><i class="fa fa-check"></i><b>8.2.3</b> Determining factorability</a></li>
<li class="chapter" data-level="8.2.4" data-path="week-07-exploratory-factor-analysis-part-1.html"><a href="week-07-exploratory-factor-analysis-part-1.html#determining-how-many-factors-to-retain"><i class="fa fa-check"></i><b>8.2.4</b> Determining how many factors to retain</a></li>
<li class="chapter" data-level="8.2.5" data-path="week-07-exploratory-factor-analysis-part-1.html"><a href="week-07-exploratory-factor-analysis-part-1.html#factor-analysis-with-a-data-matrix-raw-data"><i class="fa fa-check"></i><b>8.2.5</b> Factor analysis with a data matrix (raw data)</a></li>
<li class="chapter" data-level="8.2.6" data-path="week-07-exploratory-factor-analysis-part-1.html"><a href="week-07-exploratory-factor-analysis-part-1.html#factor-analysis-with-correlation-data"><i class="fa fa-check"></i><b>8.2.6</b> Factor analysis with correlation data</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="week-07-exploratory-factor-analysis-part-1.html"><a href="week-07-exploratory-factor-analysis-part-1.html#references-1"><i class="fa fa-check"></i><b>8.3</b> References</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="week-08-exploratory-factor-analysis-part-2.html"><a href="week-08-exploratory-factor-analysis-part-2.html"><i class="fa fa-check"></i><b>9</b> Week 08: Exploratory Factor Analysis, Part 2</a>
<ul>
<li class="chapter" data-level="9.1" data-path="week-08-exploratory-factor-analysis-part-2.html"><a href="week-08-exploratory-factor-analysis-part-2.html#factor-analysis-with-correlation-data-1"><i class="fa fa-check"></i><b>9.1</b> Factor analysis with correlation data</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="week-08-exploratory-factor-analysis-part-2.html"><a href="week-08-exploratory-factor-analysis-part-2.html#setting-up-the-data-1"><i class="fa fa-check"></i><b>9.1.1</b> Setting up the data</a></li>
<li class="chapter" data-level="9.1.2" data-path="week-08-exploratory-factor-analysis-part-2.html"><a href="week-08-exploratory-factor-analysis-part-2.html#preparatory-steps-2"><i class="fa fa-check"></i><b>9.1.2</b> Preparatory steps</a></li>
<li class="chapter" data-level="9.1.3" data-path="week-08-exploratory-factor-analysis-part-2.html"><a href="week-08-exploratory-factor-analysis-part-2.html#performing-the-factor-analysis-1"><i class="fa fa-check"></i><b>9.1.3</b> Performing the factor analysis</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="week-08-exploratory-factor-analysis-part-2.html"><a href="week-08-exploratory-factor-analysis-part-2.html#efa-with-categorical-data"><i class="fa fa-check"></i><b>9.2</b> EFA with categorical data</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="week-08-exploratory-factor-analysis-part-2.html"><a href="week-08-exploratory-factor-analysis-part-2.html#describing-categorical-data"><i class="fa fa-check"></i><b>9.2.1</b> Describing categorical data</a></li>
<li class="chapter" data-level="9.2.2" data-path="week-08-exploratory-factor-analysis-part-2.html"><a href="week-08-exploratory-factor-analysis-part-2.html#factor-scores"><i class="fa fa-check"></i><b>9.2.2</b> Factor scores</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="week-08-exploratory-factor-analysis-part-2.html"><a href="week-08-exploratory-factor-analysis-part-2.html#practice"><i class="fa fa-check"></i><b>9.3</b> Practice</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="week-08-exploratory-factor-analysis-part-2.html"><a href="week-08-exploratory-factor-analysis-part-2.html#summary-2"><i class="fa fa-check"></i><b>9.3.1</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="week-08-exploratory-factor-analysis-part-2.html"><a href="week-08-exploratory-factor-analysis-part-2.html#references-2"><i class="fa fa-check"></i><b>9.4</b> References</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="week-09-confirmatory-factor-analysis.html"><a href="week-09-confirmatory-factor-analysis.html"><i class="fa fa-check"></i><b>10</b> Week 09: Confirmatory Factor Analysis</a>
<ul>
<li class="chapter" data-level="10.1" data-path="week-09-confirmatory-factor-analysis.html"><a href="week-09-confirmatory-factor-analysis.html#preparatory-steps-3"><i class="fa fa-check"></i><b>10.1</b> Preparatory steps</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="week-09-confirmatory-factor-analysis.html"><a href="week-09-confirmatory-factor-analysis.html#describing-the-data"><i class="fa fa-check"></i><b>10.1.1</b> Describing the data</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="week-09-confirmatory-factor-analysis.html"><a href="week-09-confirmatory-factor-analysis.html#fitting-a-cfa-model"><i class="fa fa-check"></i><b>10.2</b> Fitting a CFA model</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="week-09-confirmatory-factor-analysis.html"><a href="week-09-confirmatory-factor-analysis.html#making-sure-the-model-was-correctly-estimated"><i class="fa fa-check"></i><b>10.2.1</b> Making sure the model was correctly estimated</a></li>
<li class="chapter" data-level="10.2.2" data-path="week-09-confirmatory-factor-analysis.html"><a href="week-09-confirmatory-factor-analysis.html#evaluating-the-model-fit"><i class="fa fa-check"></i><b>10.2.2</b> Evaluating the model fit</a></li>
<li class="chapter" data-level="10.2.3" data-path="week-09-confirmatory-factor-analysis.html"><a href="week-09-confirmatory-factor-analysis.html#interpretting-the-parameter-estimates"><i class="fa fa-check"></i><b>10.2.3</b> Interpretting the parameter estimates</a></li>
<li class="chapter" data-level="10.2.4" data-path="week-09-confirmatory-factor-analysis.html"><a href="week-09-confirmatory-factor-analysis.html#further-inspecting-our-model"><i class="fa fa-check"></i><b>10.2.4</b> Further inspecting our model</a></li>
<li class="chapter" data-level="10.2.5" data-path="week-09-confirmatory-factor-analysis.html"><a href="week-09-confirmatory-factor-analysis.html#further-inspecting-our-models-parameters"><i class="fa fa-check"></i><b>10.2.5</b> Further inspecting our model’s parameters</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="week-09-confirmatory-factor-analysis.html"><a href="week-09-confirmatory-factor-analysis.html#using-alternative-model-specifications"><i class="fa fa-check"></i><b>10.3</b> Using alternative model specifications</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="week-09-confirmatory-factor-analysis.html"><a href="week-09-confirmatory-factor-analysis.html#rescaling-the-factors-to-be-standardized"><i class="fa fa-check"></i><b>10.3.1</b> Rescaling the factors to be standardized</a></li>
<li class="chapter" data-level="10.3.2" data-path="week-09-confirmatory-factor-analysis.html"><a href="week-09-confirmatory-factor-analysis.html#specifying-a-competing-nested-model"><i class="fa fa-check"></i><b>10.3.2</b> Specifying a competing, nested, model</a></li>
<li class="chapter" data-level="10.3.3" data-path="week-09-confirmatory-factor-analysis.html"><a href="week-09-confirmatory-factor-analysis.html#conducting-model-comparisons"><i class="fa fa-check"></i><b>10.3.3</b> Conducting model comparisons</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="week-09-confirmatory-factor-analysis.html"><a href="week-09-confirmatory-factor-analysis.html#estimating-factor-scores"><i class="fa fa-check"></i><b>10.4</b> Estimating factor scores</a></li>
<li class="chapter" data-level="10.5" data-path="week-09-confirmatory-factor-analysis.html"><a href="week-09-confirmatory-factor-analysis.html#cfa-with-categorical-data"><i class="fa fa-check"></i><b>10.5</b> CFA with categorical data</a>
<ul>
<li class="chapter" data-level="10.5.1" data-path="week-09-confirmatory-factor-analysis.html"><a href="week-09-confirmatory-factor-analysis.html#using-data-from-an-r-package"><i class="fa fa-check"></i><b>10.5.1</b> Using data from an R package</a></li>
<li class="chapter" data-level="10.5.2" data-path="week-09-confirmatory-factor-analysis.html"><a href="week-09-confirmatory-factor-analysis.html#preparing-the-data"><i class="fa fa-check"></i><b>10.5.2</b> Preparing the data</a></li>
<li class="chapter" data-level="10.5.3" data-path="week-09-confirmatory-factor-analysis.html"><a href="week-09-confirmatory-factor-analysis.html#describing-the-data-1"><i class="fa fa-check"></i><b>10.5.3</b> Describing the data</a></li>
<li class="chapter" data-level="10.5.4" data-path="week-09-confirmatory-factor-analysis.html"><a href="week-09-confirmatory-factor-analysis.html#fitting-the-cfa-model"><i class="fa fa-check"></i><b>10.5.4</b> Fitting the CFA model</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="week-09-confirmatory-factor-analysis.html"><a href="week-09-confirmatory-factor-analysis.html#way-tldr-summary"><i class="fa fa-check"></i><b>10.6</b> Way TL;DR Summary</a></li>
<li class="chapter" data-level="10.7" data-path="week-09-confirmatory-factor-analysis.html"><a href="week-09-confirmatory-factor-analysis.html#references-3"><i class="fa fa-check"></i><b>10.7</b> References</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Guidebook for Using R in Applied Multivariate Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="week-06-principal-components-analysis" class="section level1 hasAnchor" number="7">
<h1><span class="header-section-number">7</span> Week 06: Principal Components Analysis<a href="week-06-principal-components-analysis.html#week-06-principal-components-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Principal components analysis is a method developed by Pearson and later, independently, by Hottelling, in the early 20th century. It is a method for reducing the number of dimensions—or components—in a (usually large) set of variables. Originally, this was to identify the first, or principal, component, but there are often more dimensions that may be of interest in our research. The maximum number of components is <em>p</em> (or as Everitt and Hothorn <span class="citation">(<a href="#ref-everitt_introduction_2011">2011</a>)</span> label it, <em>q</em>), which is the number of variables. In this approach, we can think of the composites as dependent variables and the observed variables as the independent variables. This is similar to discriminant function analysis (DFA) in the sense that we’re creating optimal composites of variables based on the correlations in the data, but different in that—unlike DFA—in PCA we’re not optimizing these composites so that they explain group differences. We’re simply reducing the data to fewer dimensions. We can use the resulting composites scores in follow-up analyses, such as including them as the dependent or independent variable in a regression.</p>
<p>Let’s use the examples in Chapter 3 of Everitt and Hothorn (2011). Along the way, we’ll explore several features of principal components analysis, including the scaled and unscaled eigenvectors, the eigenvalues and their relationship to the variance of the components, reproducing the original covariance matrix (or correlation matrix if that is what our PCA is fit to), and different methods for calculating cases’ component scores. We’ll also meddle with different functions for PCA, including <code>princomp()</code>, <code>prcomp()</code>, <code>eigen()</code> of Base R, and the psych package’s <code>pca()</code> function <span class="citation">(<a href="#ref-R-psych">Revelle 2025</a>)</span>.<a href="#fn28" class="footnote-ref" id="fnref28"><sup>28</sup></a> If you’re using SPSS, your PCA results will resemble those of the psych package’s output, which under the hood uses these other Base R functions.</p>
<div id="example-1-to-see-some-properties-of-pca" class="section level2 hasAnchor" number="7.1">
<h2><span class="header-section-number">7.1</span> Example 1, to see some properties of PCA<a href="week-06-principal-components-analysis.html#example-1-to-see-some-properties-of-pca" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Here is the correlation matrix from the chapter. (I simply copied and pasted this and reformatted it, so there might be rounding error discrepancies with what is reported in the chapter.) The <em>N</em>-size is 72 but we do not have individual cases’ data.</p>
<div class="sourceCode" id="cb307"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb307-1"><a href="week-06-principal-components-analysis.html#cb307-1" tabindex="-1"></a>varbnames <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;rblood&quot;</span>, <span class="st">&quot;plate&quot;</span>, <span class="st">&quot;wblood&quot;</span>, <span class="st">&quot;neut&quot;</span>, <span class="st">&quot;lymph&quot;</span>, <span class="st">&quot;bilir&quot;</span>, <span class="st">&quot;sodium&quot;</span>, <span class="st">&quot;potass&quot;</span>)</span>
<span id="cb307-2"><a href="week-06-principal-components-analysis.html#cb307-2" tabindex="-1"></a></span>
<span id="cb307-3"><a href="week-06-principal-components-analysis.html#cb307-3" tabindex="-1"></a>blood_corr <span class="ot">&lt;-</span> <span class="fu">matrix</span>(</span>
<span id="cb307-4"><a href="week-06-principal-components-analysis.html#cb307-4" tabindex="-1"></a>  <span class="fu">c</span>(    <span class="dv">1</span>,   <span class="fl">0.29</span>,   <span class="fl">0.202</span>, <span class="sc">-</span><span class="fl">0.055</span>, <span class="sc">-</span><span class="fl">0.105</span>, <span class="sc">-</span><span class="fl">0.252</span>, <span class="sc">-</span><span class="fl">0.229</span>,  <span class="fl">0.058</span>,</span>
<span id="cb307-5"><a href="week-06-principal-components-analysis.html#cb307-5" tabindex="-1"></a>    <span class="fl">0.290</span>,      <span class="dv">1</span>,   <span class="fl">0.415</span>,  <span class="fl">0.285</span>, <span class="sc">-</span><span class="fl">0.376</span>, <span class="sc">-</span><span class="fl">0.349</span>, <span class="sc">-</span><span class="fl">0.164</span>, <span class="sc">-</span><span class="fl">0.129</span>,</span>
<span id="cb307-6"><a href="week-06-principal-components-analysis.html#cb307-6" tabindex="-1"></a>    <span class="fl">0.202</span>,  <span class="fl">0.415</span>,       <span class="dv">1</span>,  <span class="fl">0.419</span>, <span class="sc">-</span><span class="fl">0.521</span>, <span class="sc">-</span><span class="fl">0.441</span>, <span class="sc">-</span><span class="fl">0.145</span>, <span class="sc">-</span><span class="fl">0.076</span>,</span>
<span id="cb307-7"><a href="week-06-principal-components-analysis.html#cb307-7" tabindex="-1"></a>   <span class="sc">-</span><span class="fl">0.055</span>,  <span class="fl">0.285</span>,   <span class="fl">0.419</span>,      <span class="dv">1</span>, <span class="sc">-</span><span class="fl">0.877</span>, <span class="sc">-</span><span class="fl">0.076</span>,  <span class="fl">0.023</span>, <span class="sc">-</span><span class="fl">0.131</span>,</span>
<span id="cb307-8"><a href="week-06-principal-components-analysis.html#cb307-8" tabindex="-1"></a>   <span class="sc">-</span><span class="fl">0.105</span>, <span class="sc">-</span><span class="fl">0.376</span>,  <span class="sc">-</span><span class="fl">0.521</span>, <span class="sc">-</span><span class="fl">0.877</span>,      <span class="dv">1</span>,  <span class="fl">0.206</span>,  <span class="fl">0.034</span>,  <span class="fl">0.151</span>,</span>
<span id="cb307-9"><a href="week-06-principal-components-analysis.html#cb307-9" tabindex="-1"></a>   <span class="sc">-</span><span class="fl">0.252</span>, <span class="sc">-</span><span class="fl">0.349</span>,  <span class="sc">-</span><span class="fl">0.441</span>, <span class="sc">-</span><span class="fl">0.076</span>,  <span class="fl">0.206</span>,      <span class="dv">1</span>,  <span class="fl">0.192</span>,  <span class="fl">0.077</span>,</span>
<span id="cb307-10"><a href="week-06-principal-components-analysis.html#cb307-10" tabindex="-1"></a>   <span class="sc">-</span><span class="fl">0.229</span>, <span class="sc">-</span><span class="fl">0.164</span>,  <span class="sc">-</span><span class="fl">0.145</span>,  <span class="fl">0.023</span>,  <span class="fl">0.034</span>,  <span class="fl">0.192</span>,      <span class="dv">1</span>,  <span class="fl">0.423</span>,</span>
<span id="cb307-11"><a href="week-06-principal-components-analysis.html#cb307-11" tabindex="-1"></a>    <span class="fl">0.058</span>, <span class="sc">-</span><span class="fl">0.129</span>,  <span class="sc">-</span><span class="fl">0.076</span>, <span class="sc">-</span><span class="fl">0.131</span>,  <span class="fl">0.151</span>,  <span class="fl">0.077</span>,  <span class="fl">0.423</span>,      <span class="dv">1</span>),</span>
<span id="cb307-12"><a href="week-06-principal-components-analysis.html#cb307-12" tabindex="-1"></a>  <span class="at">ncol =</span> <span class="dv">8</span>, <span class="at">byrow =</span> <span class="cn">TRUE</span>,</span>
<span id="cb307-13"><a href="week-06-principal-components-analysis.html#cb307-13" tabindex="-1"></a>  <span class="at">dimnames =</span>  <span class="fu">list</span>(varbnames, varbnames) )</span>
<span id="cb307-14"><a href="week-06-principal-components-analysis.html#cb307-14" tabindex="-1"></a></span>
<span id="cb307-15"><a href="week-06-principal-components-analysis.html#cb307-15" tabindex="-1"></a>blood_sd <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.371</span>,    <span class="fl">41.253</span>, <span class="fl">1.935</span>,  <span class="fl">0.077</span>,  <span class="fl">0.071</span>,  <span class="fl">4.037</span>,  <span class="fl">2.732</span>,  <span class="fl">0.297</span>)</span>
<span id="cb307-16"><a href="week-06-principal-components-analysis.html#cb307-16" tabindex="-1"></a><span class="fu">names</span>(blood_sd) <span class="ot">&lt;-</span> varbnames</span></code></pre></div>
<p>Let’s print the correlation matrix to be sure it is what we expect:</p>
<div class="sourceCode" id="cb308"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb308-1"><a href="week-06-principal-components-analysis.html#cb308-1" tabindex="-1"></a>blood_corr</span></code></pre></div>
<pre><code>##        rblood  plate wblood   neut  lymph  bilir sodium potass
## rblood  1.000  0.290  0.202 -0.055 -0.105 -0.252 -0.229  0.058
## plate   0.290  1.000  0.415  0.285 -0.376 -0.349 -0.164 -0.129
## wblood  0.202  0.415  1.000  0.419 -0.521 -0.441 -0.145 -0.076
## neut   -0.055  0.285  0.419  1.000 -0.877 -0.076  0.023 -0.131
## lymph  -0.105 -0.376 -0.521 -0.877  1.000  0.206  0.034  0.151
## bilir  -0.252 -0.349 -0.441 -0.076  0.206  1.000  0.192  0.077
## sodium -0.229 -0.164 -0.145  0.023  0.034  0.192  1.000  0.423
## potass  0.058 -0.129 -0.076 -0.131  0.151  0.077  0.423  1.000</code></pre>
<p>Here is the vector of standard deviations, which are needed to calculate the covariance matrix, which we only look at for comparison purposes:</p>
<div class="sourceCode" id="cb310"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb310-1"><a href="week-06-principal-components-analysis.html#cb310-1" tabindex="-1"></a>blood_sd</span></code></pre></div>
<pre><code>## rblood  plate wblood   neut  lymph  bilir sodium potass 
##  0.371 41.253  1.935  0.077  0.071  4.037  2.732  0.297</code></pre>
<p>Given the standard deviations and the correlation matrix, we can get the covariance matrix using some matrix algebra. Recall that to get from covariance to correlation, we divide each cell by the product of the two standard-deviations corresponding to the row and column. Here, we’re going in the opposite direction, so we’re multiplying each cell by the product of the row and column standard deviations. <a href="#fn29" class="footnote-ref" id="fnref29"><sup>29</sup></a> Everitt and Hothorn are using the covariance matrix for demonstration of the difference between covariance and correlation—and of what not to do—so we normally wouldn’t care about this if we have the correlation matrix.</p>
<div class="sourceCode" id="cb312"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb312-1"><a href="week-06-principal-components-analysis.html#cb312-1" tabindex="-1"></a>blood_cov <span class="ot">&lt;-</span> <span class="fu">diag</span>(blood_sd) <span class="sc">%*%</span> blood_corr <span class="sc">%*%</span> <span class="fu">diag</span>(blood_sd)</span>
<span id="cb312-2"><a href="week-06-principal-components-analysis.html#cb312-2" tabindex="-1"></a><span class="fu">round</span>(blood_cov, <span class="dv">2</span> )</span></code></pre></div>
<pre><code>##       [,1]    [,2]  [,3]  [,4]  [,5]   [,6]   [,7]  [,8]
## [1,]  0.14    4.44  0.15  0.00  0.00  -0.38  -0.23  0.01
## [2,]  4.44 1701.81 33.13  0.91 -1.10 -58.12 -18.48 -1.58
## [3,]  0.15   33.13  3.74  0.06 -0.07  -3.44  -0.77 -0.04
## [4,]  0.00    0.91  0.06  0.01  0.00  -0.02   0.00  0.00
## [5,]  0.00   -1.10 -0.07  0.00  0.01   0.06   0.01  0.00
## [6,] -0.38  -58.12 -3.44 -0.02  0.06  16.30   2.12  0.09
## [7,] -0.23  -18.48 -0.77  0.00  0.01   2.12   7.46  0.34
## [8,]  0.01   -1.58 -0.04  0.00  0.00   0.09   0.34  0.09</code></pre>
<div id="pca-with-covariance-vs.-correlation" class="section level3 hasAnchor" number="7.1.1">
<h3><span class="header-section-number">7.1.1</span> PCA with Covariance vs. Correlation<a href="week-06-principal-components-analysis.html#pca-with-covariance-vs.-correlation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We can use a PCA on a covariance matrix, correlation matrix, or the raw data. The correlation matrix is better than the covariance matrix if the observed variables are not on the same scale (which is most of the time). Conducting a PCA on the raw data is useful if we want to save each case’s composite scores and use them later on, say, in a regression or a plot.</p>
<p>Here is the PCA on the <strong>covariance matrix</strong>, which is on the original scale of the variables. The results of this are reported on Page 67 of the chapter.</p>
<div class="sourceCode" id="cb314"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb314-1"><a href="week-06-principal-components-analysis.html#cb314-1" tabindex="-1"></a>blood_pcacov <span class="ot">&lt;-</span> <span class="fu">princomp</span>(<span class="at">covmat =</span> blood_cov)</span>
<span id="cb314-2"><a href="week-06-principal-components-analysis.html#cb314-2" tabindex="-1"></a><span class="fu">summary</span>(blood_pcacov, <span class="at">loadings =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<pre><code>## Importance of components:
##                            Comp.1      Comp.2     Comp.3      Comp.4      Comp.5       Comp.6
## Standard deviation     41.2877486 3.880212624 2.64197339 1.624583979 0.353951757 2.561722e-01
## Proportion of Variance  0.9856182 0.008705172 0.00403574 0.001525986 0.000072436 3.794288e-05
## Cumulative Proportion   0.9856182 0.994323381 0.99835912 0.999885108 0.999957544 9.999955e-01
##                              Comp.7       Comp.8
## Standard deviation     8.510631e-02 2.372715e-02
## Proportion of Variance 4.187837e-06 3.255049e-07
## Cumulative Proportion  9.999997e-01 1.000000e+00
## 
## Loadings:
##      Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8
## [1,]                              0.943  0.329              
## [2,]  0.999                                                 
## [3,]         0.192         0.981                            
## [4,]                                            0.758  0.650
## [5,]                                           -0.649  0.760
## [6,]        -0.961  0.195  0.191                            
## [7,]        -0.193 -0.979                                   
## [8,]                              0.329 -0.942</code></pre>
<p>In this output under the label <code>Loadings:</code>, the eigenvector coefficients with very low absolute values (less than 0.10, it seems) are left blank in the printout. These coefficients actually do exist, and we can see them if we ask for them using <code>blood_pcacov$loadings[1:64]</code>, where 64 is the number of cells in the eigenvector matrix (8 eigenvectors across 8 items).</p>
<p>What do you notice about the first component? How much of the total variance does it explain? Which observed variables have the largest coefficients? (Refer back to the standard deviations of the raw data and look at which variable seems to have a much larger scale, as reflected by the standard deviations.)</p>
<p>And, for comparison, here is the PCA on the <strong>correlation matrix</strong>, which is the preferred method.</p>
<div class="sourceCode" id="cb316"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb316-1"><a href="week-06-principal-components-analysis.html#cb316-1" tabindex="-1"></a>blood_pcacor <span class="ot">&lt;-</span> <span class="fu">princomp</span>(<span class="at">covmat =</span> blood_corr)</span>
<span id="cb316-2"><a href="week-06-principal-components-analysis.html#cb316-2" tabindex="-1"></a><span class="fu">summary</span>(blood_pcacor, <span class="at">loadings =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<pre><code>## Importance of components:
##                           Comp.1    Comp.2    Comp.3     Comp.4     Comp.5     Comp.6     Comp.7
## Standard deviation     1.6710100 1.2375848 1.1177138 0.88227419 0.78839505 0.69917350 0.66002394
## Proportion of Variance 0.3490343 0.1914520 0.1561605 0.09730097 0.07769584 0.06110545 0.05445395
## Cumulative Proportion  0.3490343 0.5404863 0.6966468 0.79394778 0.87164363 0.93274908 0.98720303
##                            Comp.8
## Standard deviation     0.31996216
## Proportion of Variance 0.01279697
## Cumulative Proportion  1.00000000
## 
## Loadings:
##        Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8
## rblood  0.194  0.417  0.400  0.652  0.175  0.363  0.176  0.102
## plate   0.400  0.154  0.168        -0.848 -0.230 -0.110       
## wblood  0.459         0.168 -0.274  0.251 -0.403  0.677       
## neut    0.430 -0.472 -0.171  0.169  0.118        -0.237  0.678
## lymph  -0.494  0.360        -0.180 -0.139 -0.136  0.157  0.724
## bilir  -0.319 -0.320 -0.277  0.633 -0.162 -0.384  0.377       
## sodium -0.177 -0.535  0.410 -0.163 -0.299  0.513  0.367       
## potass -0.171 -0.245  0.709         0.198 -0.469 -0.376</code></pre>
<p>Notice how the correlation matrix has a very different proportion of variance being explained by the first component and that several of the observed variables are contributing to that component. This is very different from what we observed with the covariance matrix. In a correlation matrix, all of the variables are treated equally, being placed on the same scale that is bound between -1 and +1, which in turn provides a more accurate estimate of the components.</p>
</div>
<div id="unscaled-eigenvectors" class="section level3 hasAnchor" number="7.1.2">
<h3><span class="header-section-number">7.1.2</span> Unscaled eigenvectors<a href="week-06-principal-components-analysis.html#unscaled-eigenvectors" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Note that what the <code>princomp()</code> function labels as “loadings” are actually the <strong><em>unscaled</em> eigenvectors</strong>. To get the <strong><em>rescaled</em> eigenvectors</strong>, which are more appropriate for interpreting the components, and which are what analysts from a factor-analysis tradition call <strong>loadings</strong>, we use the equation at the bottom of Page 70 in the chapter:</p>
<p><span class="math display">\[\mathbf{a}^*_i = \mathbf{a}_i\sqrt{\lambda}_i \]</span>
where <span class="math inline">\(i\)</span> refers to the component number—in this example we have eight possible components. Following Everitt and Hothorn’s labeling system, <span class="math inline">\(\mathbf{a}_i\)</span> represents the eigenvector for component <span class="math inline">\(i\)</span>, and the rescaled eigenvector has an asterisk. Also, the lowercase lambda, <span class="math inline">\(\lambda_i\)</span>, represents the eigenvalue of component <span class="math inline">\(i\)</span>. If we stuck these eight columns of vectors side-by-side in a matrix, we have a matrix of eigenvectors, <span class="math inline">\(\mathbf{A}\)</span> and <span class="math inline">\(\mathbf{A^*}\)</span>, for the unscaled and scaled eigenvectors respectively (described on pp. 70–71 in the chapter).</p>
<p>Here are the eight eigenvectors, as they’re reported in the <code>princomp()</code> function under each column, labeled by the component. (Again, these are <strong>not</strong> loadings—they’re unscaled eigenvectors.)</p>
<div class="sourceCode" id="cb318"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb318-1"><a href="week-06-principal-components-analysis.html#cb318-1" tabindex="-1"></a>blood_pcacor<span class="sc">$</span>loadings</span></code></pre></div>
<pre><code>## 
## Loadings:
##        Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8
## rblood  0.194  0.417  0.400  0.652  0.175  0.363  0.176  0.102
## plate   0.400  0.154  0.168        -0.848 -0.230 -0.110       
## wblood  0.459         0.168 -0.274  0.251 -0.403  0.677       
## neut    0.430 -0.472 -0.171  0.169  0.118        -0.237  0.678
## lymph  -0.494  0.360        -0.180 -0.139 -0.136  0.157  0.724
## bilir  -0.319 -0.320 -0.277  0.633 -0.162 -0.384  0.377       
## sodium -0.177 -0.535  0.410 -0.163 -0.299  0.513  0.367       
## potass -0.171 -0.245  0.709         0.198 -0.469 -0.376       
## 
##                Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8
## SS loadings     1.000  1.000  1.000  1.000  1.000  1.000  1.000  1.000
## Proportion Var  0.125  0.125  0.125  0.125  0.125  0.125  0.125  0.125
## Cumulative Var  0.125  0.250  0.375  0.500  0.625  0.750  0.875  1.000</code></pre>
<p>The eigenvalues are available from the output, as well. The <code>$sdev</code> includes the standard deviations of each component (based on the <strong>unscaled</strong> eigenvector coefficients). If we square those, we have the component’s variance, or eigenvalue. There are eight eigenvalues—one for each component and these represent the repackaged variances when we create these components from the variables.</p>
<div class="sourceCode" id="cb320"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb320-1"><a href="week-06-principal-components-analysis.html#cb320-1" tabindex="-1"></a>blood_pcacor<span class="sc">$</span>sdev<span class="sc">^</span><span class="dv">2</span></span></code></pre></div>
<pre><code>##    Comp.1    Comp.2    Comp.3    Comp.4    Comp.5    Comp.6    Comp.7    Comp.8 
## 2.7922743 1.5316161 1.2492841 0.7784077 0.6215668 0.4888436 0.4356316 0.1023758</code></pre>
<p>The components with higher eigenvalues explain more variability in the data. The first component, (which is the “principal” component that Pearson originally looked at), explains most of the variability in our data set.</p>
</div>
<div id="looking-at-the-properties-of-eigenvectors-and-eigenvalues" class="section level3 hasAnchor" number="7.1.3">
<h3><span class="header-section-number">7.1.3</span> Looking at the properties of eigenvectors and eigenvalues<a href="week-06-principal-components-analysis.html#looking-at-the-properties-of-eigenvectors-and-eigenvalues" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Our handy R calculator has done the work in obtaining the eigenvectors and eigenvalues for us. The eigenvectors are the weights, analogous to regression coefficients; the eigenvalues represent the variance of the component, with higher values representing a stronger component. To obtain the eigenvectors, two constraints had to be in place:</p>
<ol style="list-style-type: decimal">
<li>The sum of the squared coefficients is one, <span class="math inline">\(\mathbf{a}_i\prime\mathbf{a}_i = 1\)</span>.</li>
<li>The cross-products of each eigenvector is zero, <span class="math inline">\(\mathbf{a}_i\prime\mathbf{a}_j = 0 \text{, where } j &gt; i\)</span>.</li>
</ol>
<p>Let’s check this out:</p>
<p>Here’s our first eigenvector:</p>
<div class="sourceCode" id="cb322"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb322-1"><a href="week-06-principal-components-analysis.html#cb322-1" tabindex="-1"></a>a1 <span class="ot">&lt;-</span> blood_pcacor<span class="sc">$</span>loadings[, <span class="dv">1</span>]</span>
<span id="cb322-2"><a href="week-06-principal-components-analysis.html#cb322-2" tabindex="-1"></a>a1</span></code></pre></div>
<pre><code>##     rblood      plate     wblood       neut      lymph      bilir     sodium     potass 
##  0.1942203  0.4003625  0.4588793  0.4303359 -0.4937748 -0.3194549 -0.1768857 -0.1705160</code></pre>
<p>Does <span class="math inline">\(\mathbf{a}_1\prime\mathbf{a}_1 = 1\)</span>?</p>
<p>In other words, does</p>
<p><span class="math display">\[\begin{bmatrix}
  0.19 &amp;  0.40  &amp; 0.46 &amp;  0.43 &amp; -0.49 &amp; -0.32 &amp; -0.18 &amp; -0.17
\end{bmatrix}
\begin{bmatrix}
  0.19 \\  0.40  \\ 0.46 \\  0.43 \\ -0.49 \\ -0.32 \\ -0.18 \\ -0.17
\end{bmatrix} = 1 \text{ ?}\]</span></p>
<div class="sourceCode" id="cb324"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb324-1"><a href="week-06-principal-components-analysis.html#cb324-1" tabindex="-1"></a><span class="fu">t</span>(a1) <span class="sc">%*%</span> a1</span></code></pre></div>
<pre><code>##      [,1]
## [1,]    1</code></pre>
<p>It does.</p>
<p>Here’s our second eigenvector:</p>
<div class="sourceCode" id="cb326"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb326-1"><a href="week-06-principal-components-analysis.html#cb326-1" tabindex="-1"></a>a2 <span class="ot">&lt;-</span> blood_pcacor<span class="sc">$</span>loadings[, <span class="dv">2</span>]</span>
<span id="cb326-2"><a href="week-06-principal-components-analysis.html#cb326-2" tabindex="-1"></a>a2</span></code></pre></div>
<pre><code>##        rblood         plate        wblood          neut         lymph         bilir        sodium 
##  0.4171230843  0.1539289974 -0.0002984974 -0.4724424229  0.3604497505 -0.3201664742 -0.5352734751 
##        potass 
## -0.2452834631</code></pre>
<p>Does <span class="math inline">\(\mathbf{a}_2\prime\mathbf{a}_2 = 1\)</span>?</p>
<div class="sourceCode" id="cb328"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb328-1"><a href="week-06-principal-components-analysis.html#cb328-1" tabindex="-1"></a><span class="fu">t</span>(a2) <span class="sc">%*%</span> a2</span></code></pre></div>
<pre><code>##      [,1]
## [1,]    1</code></pre>
<p>It does. This also sums to 1.</p>
<p>Okay, how about their cross-products: Does</p>
<p><span class="math display">\[\begin{bmatrix}
  0.19 &amp;  0.40  &amp; 0.46 &amp;  0.43 &amp; -0.49 &amp; -0.32 &amp; -0.18 &amp; -0.17
\end{bmatrix}
\begin{bmatrix}
  0.42 \\  0.15 \\   0.00 \\  -0.47  \\  0.36 \\  -0.32 \\  -0.54 \\  -0.25
\end{bmatrix} = 0 \text{ ?}\]</span></p>
<div class="sourceCode" id="cb330"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb330-1"><a href="week-06-principal-components-analysis.html#cb330-1" tabindex="-1"></a><span class="fu">round</span>( <span class="fu">t</span>(a1) <span class="sc">%*%</span> a2 , <span class="dv">3</span>)</span></code></pre></div>
<pre><code>##      [,1]
## [1,]    0</code></pre>
<p>Indeed it does.</p>
<p>This latter condition ensures that the components are orthogonal—that is, unrelated.</p>
<p>We only checked the first two components, but we would find the same results with the rest.</p>
<div id="eigenvalues-add-up-to-total-observed-variance" class="section level4 hasAnchor" number="7.1.3.1">
<h4><span class="header-section-number">7.1.3.1</span> Eigenvalues add up to total observed variance<a href="week-06-principal-components-analysis.html#eigenvalues-add-up-to-total-observed-variance" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We can also see that the sum of the eigenvalues (<span class="math inline">\(\lambda_1\)</span> through <span class="math inline">\(\lambda_8\)</span>) equals the sum of the variances of the observed variables. In other words, does
<span class="math display">\[\sum{\lambda_i = s^2_1 + s^2_2 + \cdots + s^2_p} \text{ ?}\]</span>
In our case, we fit the PCA to the data’s correlation matrix, which includes <code>1</code> on the diagonal, meaning that the variables are scaled to have a variance (and standard deviation) of <code>1</code>. With that, the sum of our eigenvalues (before we reduce the number of components) should be the number of variables.</p>
<p>So we can ask whether the sum of the eigenvalues is 8, given that we have eight variables.</p>
<div class="sourceCode" id="cb332"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb332-1"><a href="week-06-principal-components-analysis.html#cb332-1" tabindex="-1"></a><span class="fu">sum</span>(blood_pcacor<span class="sc">$</span>sdev<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 8</code></pre>
<p>This demonstrates that the amount of variance is the same as it is in our original data, just repackaged.</p>
</div>
<div id="each-components-proportion-of-variance" class="section level4 hasAnchor" number="7.1.3.2">
<h4><span class="header-section-number">7.1.3.2</span> Each component’s proportion of variance<a href="week-06-principal-components-analysis.html#each-components-proportion-of-variance" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>What is the proportion of variance that each component explains? Given that we know the total variance, and each eigenvalue, we can use this code:</p>
<div class="sourceCode" id="cb334"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb334-1"><a href="week-06-principal-components-analysis.html#cb334-1" tabindex="-1"></a>props <span class="ot">&lt;-</span> blood_pcacor<span class="sc">$</span>sdev<span class="sc">^</span><span class="dv">2</span> <span class="sc">/</span> <span class="fu">sum</span>(blood_pcacor<span class="sc">$</span>sdev<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb334-2"><a href="week-06-principal-components-analysis.html#cb334-2" tabindex="-1"></a><span class="fu">round</span>(props, <span class="dv">3</span>)</span></code></pre></div>
<pre><code>## Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8 
##  0.349  0.191  0.156  0.097  0.078  0.061  0.054  0.013</code></pre>
<p>If we look up above to the results of <code>summary(blood_pcacor, loadings = TRUE)</code>, we see these same values reported in the <code>Proportion of Variance</code> row.</p>
</div>
<div id="rescaling-the-eigenvectors" class="section level4 hasAnchor" number="7.1.3.3">
<h4><span class="header-section-number">7.1.3.3</span> Rescaling the eigenvectors<a href="week-06-principal-components-analysis.html#rescaling-the-eigenvectors" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Let’s calculate the rescaled eigenvalues so they can be more easily interpreted. These coefficients represent the correlation between the observed variable and the component. These are what are outputted in SPSS and in the psych package’s <code>pca()</code> function, which we’ll use for our actual analysis. First, let’s calculate</p>
<p><span class="math display">\[\mathbf{a}^*_i = \mathbf{a}_i\sqrt{\lambda}_i \]</span></p>
<p>We could get the unscaled eigenvalues from the <code>princomp()</code> object using <code>blood_pcacor$loadings[1:64]</code> but that is less convenient (and not as easily reproducible in R code). Instead, we can use the <code>eigen()</code> function on the correlation matrix to get each component’s eigenvector and eigenvalue.</p>
<div class="sourceCode" id="cb336"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb336-1"><a href="week-06-principal-components-analysis.html#cb336-1" tabindex="-1"></a>vals.and.vects <span class="ot">&lt;-</span> <span class="fu">eigen</span>(blood_corr)</span>
<span id="cb336-2"><a href="week-06-principal-components-analysis.html#cb336-2" tabindex="-1"></a>vals.and.vects</span></code></pre></div>
<pre><code>## eigen() decomposition
## $values
## [1] 2.7922743 1.5316161 1.2492841 0.7784077 0.6215668 0.4888436 0.4356316 0.1023758
## 
## $vectors
##            [,1]          [,2]        [,3]        [,4]       [,5]        [,6]       [,7]        [,8]
## [1,] -0.1942203  0.4171230843  0.39976114  0.65159275  0.1752060  0.36281561  0.1763116 -0.10240413
## [2,] -0.4003625  0.1539289974  0.16772869  0.06371996 -0.8476003 -0.23041340 -0.1104646 -0.01017235
## [3,] -0.4588793 -0.0002984974  0.16777536 -0.27379988  0.2512311 -0.40295337  0.6769694 -0.05038622
## [4,] -0.4303359 -0.4724424229 -0.17128192  0.16914858  0.1177228  0.06459323 -0.2367450 -0.67792429
## [5,]  0.4937748  0.3604497505  0.08716408 -0.18037205 -0.1389990 -0.13572092  0.1572459 -0.72364606
## [6,]  0.3194549 -0.3201664742 -0.27661854  0.63331424 -0.1615438 -0.38374488  0.3765128  0.05214312
## [7,]  0.1768857 -0.5352734751  0.41027697 -0.16314269 -0.2988956  0.51279925  0.3670544 -0.01484605
## [8,]  0.1705160 -0.2452834631  0.70861094  0.08690517  0.1978511 -0.46913476 -0.3757113  0.02620828</code></pre>
<p>Let’s plug these into our equation, <span class="math inline">\(\mathbf{a}^*_i = \mathbf{a}_i\sqrt{\lambda}_i\)</span>. We could do each rescaled eigenvector at a time, or we could use this fancy matrix operation.</p>
<div class="sourceCode" id="cb338"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb338-1"><a href="week-06-principal-components-analysis.html#cb338-1" tabindex="-1"></a>loads <span class="ot">&lt;-</span> vals.and.vects<span class="sc">$</span>vectors <span class="sc">%*%</span> <span class="fu">diag</span>(<span class="fu">sqrt</span>(vals.and.vects<span class="sc">$</span>values))</span>
<span id="cb338-2"><a href="week-06-principal-components-analysis.html#cb338-2" tabindex="-1"></a><span class="fu">round</span>(loads, <span class="dv">2</span>)</span></code></pre></div>
<pre><code>##       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]
## [1,] -0.32  0.52  0.45  0.57  0.14  0.25  0.12 -0.03
## [2,] -0.67  0.19  0.19  0.06 -0.67 -0.16 -0.07  0.00
## [3,] -0.77  0.00  0.19 -0.24  0.20 -0.28  0.45 -0.02
## [4,] -0.72 -0.58 -0.19  0.15  0.09  0.05 -0.16 -0.22
## [5,]  0.83  0.45  0.10 -0.16 -0.11 -0.09  0.10 -0.23
## [6,]  0.53 -0.40 -0.31  0.56 -0.13 -0.27  0.25  0.02
## [7,]  0.30 -0.66  0.46 -0.14 -0.24  0.36  0.24  0.00
## [8,]  0.28 -0.30  0.79  0.08  0.16 -0.33 -0.25  0.01</code></pre>
<p>In this code, the <code>diag(sqrt(vals.and.vects$values))</code> is the square root of each component’s eigenvalue placed on the diagonal in a square matrix, which has zeros on the off-diagonal. With our data, the eigenvalue diagonal matrix (before square-rooting) looks like this, with capital lambda, <span class="math inline">\(\mathbf{\Lambda}\)</span>, being used to symbolize it given that each element on the diagonal is a little lambda <span class="math inline">\(\lambda_i\)</span>:</p>
<p><span class="math display">\[ \mathbf{\Lambda} =
\begin{bmatrix}
  2.79 &amp;  0  &amp; 0 &amp;  0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
  0 &amp;  1.53  &amp; 0 &amp;  0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
  0 &amp;  0  &amp; 1.25 &amp;  0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
  0 &amp;  0  &amp; 0 &amp;  0.78 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
  0 &amp;  0  &amp; 0 &amp;  0 &amp; 0.62 &amp; 0 &amp; 0 &amp; 0\\
  0 &amp;  0  &amp; 0 &amp;  0 &amp; 0 &amp; 0.49 &amp; 0 &amp; 0\\
  0 &amp;  0  &amp; 0 &amp;  0 &amp; 0 &amp; 0 &amp; 0.44 &amp; 0\\
  0 &amp;  0  &amp; 0 &amp;  0 &amp; 0 &amp; 0 &amp; 0 &amp; 0.10
\end{bmatrix} \]</span></p>
<p>This is also represented as <span class="math inline">\(\mathbf{\lambda} \mathbf{I}\)</span> because multiplying the vector of eigenvalues by a <span class="math inline">\(p \times p\)</span> identity matrix will give the same result as <code>diag(eigenvalues)</code>.</p>
<p>Again, we’re using the square roots of the diagonal, <code>diag(sqrt(vals.and.vects$values))</code>:
<span class="math display">\[ \begin{bmatrix}
  \sqrt{2.79} &amp;  0  &amp; 0 &amp;  0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
  0 &amp;  \sqrt{1.53}  &amp; 0 &amp;  0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
  0 &amp;  0  &amp; \sqrt{1.25} &amp;  0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
  0 &amp;  0  &amp; 0 &amp;  \sqrt{0.78} &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
  0 &amp;  0  &amp; 0 &amp;  0 &amp; \sqrt{0.62} &amp; 0 &amp; 0 &amp; 0\\
  0 &amp;  0  &amp; 0 &amp;  0 &amp; 0 &amp; \sqrt{0.49} &amp; 0 &amp; 0\\
  0 &amp;  0  &amp; 0 &amp;  0 &amp; 0 &amp; 0 &amp; \sqrt{0.44} &amp; 0\\
  0 &amp;  0  &amp; 0 &amp;  0 &amp; 0 &amp; 0 &amp; 0 &amp; \sqrt{0.10}
\end{bmatrix} \]</span></p>
<p>When we matrix-multiply the matrix of unscaled eigenvectors (<code>vals.and.vects$vectors</code>) by this diagonal matrix, each eigenvector cell is multiplied by its respective component’s <span class="math inline">\(\sqrt{\text{eigenvalue}}\)</span> and placed into its corresponding cell in the resulting matrix, which contains our rescaled eigenvectors (<span class="math inline">\(\mathbf{a}^*_i\)</span>), AKA “loadings”.</p>
<p>This is the set of coefficients that represents the correlation between the observed variable and the component.</p>
</div>
<div id="reproducing-the-covariance-or-correlation-matrix" class="section level4 hasAnchor" number="7.1.3.4">
<h4><span class="header-section-number">7.1.3.4</span> Reproducing the covariance (or correlation) matrix<a href="week-06-principal-components-analysis.html#reproducing-the-covariance-or-correlation-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We can reproduce the matrix from which we performed the principal components analysis on (in this case, the correlation matrix) using the equation at the top of Page 71 in the chapter:</p>
<p><span class="math display">\[\mathbf{S} =  \mathbf{A^*}\mathbf{A^*}^\prime \]</span></p>
<div class="sourceCode" id="cb340"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb340-1"><a href="week-06-principal-components-analysis.html#cb340-1" tabindex="-1"></a>A.star <span class="ot">&lt;-</span> loads</span>
<span id="cb340-2"><a href="week-06-principal-components-analysis.html#cb340-2" tabindex="-1"></a>R.reprod <span class="ot">&lt;-</span> A.star <span class="sc">%*%</span> <span class="fu">t</span>(A.star)</span>
<span id="cb340-3"><a href="week-06-principal-components-analysis.html#cb340-3" tabindex="-1"></a><span class="fu">round</span>( R.reprod, <span class="dv">3</span>)</span></code></pre></div>
<pre><code>##        [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]   [,8]
## [1,]  1.000  0.290  0.202 -0.055 -0.105 -0.252 -0.229  0.058
## [2,]  0.290  1.000  0.415  0.285 -0.376 -0.349 -0.164 -0.129
## [3,]  0.202  0.415  1.000  0.419 -0.521 -0.441 -0.145 -0.076
## [4,] -0.055  0.285  0.419  1.000 -0.877 -0.076  0.023 -0.131
## [5,] -0.105 -0.376 -0.521 -0.877  1.000  0.206  0.034  0.151
## [6,] -0.252 -0.349 -0.441 -0.076  0.206  1.000  0.192  0.077
## [7,] -0.229 -0.164 -0.145  0.023  0.034  0.192  1.000  0.423
## [8,]  0.058 -0.129 -0.076 -0.131  0.151  0.077  0.423  1.000</code></pre>
<p>We can get the <strong>residual covariance matrix</strong> from this by subtracting the reproduced covariance from the original covariance (or correlations in this case):</p>
<div class="sourceCode" id="cb342"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb342-1"><a href="week-06-principal-components-analysis.html#cb342-1" tabindex="-1"></a><span class="fu">round</span>( blood_corr <span class="sc">-</span> R.reprod, <span class="dv">3</span>)</span></code></pre></div>
<pre><code>##        rblood plate wblood neut lymph bilir sodium potass
## rblood      0     0      0    0     0     0      0      0
## plate       0     0      0    0     0     0      0      0
## wblood      0     0      0    0     0     0      0      0
## neut        0     0      0    0     0     0      0      0
## lymph       0     0      0    0     0     0      0      0
## bilir       0     0      0    0     0     0      0      0
## sodium      0     0      0    0     0     0      0      0
## potass      0     0      0    0     0     0      0      0</code></pre>
<p>This reproduced correlation is indeed the same as our raw correlation matrix. (If we had conducted the PCA on the covariance matrix, it would return the covariance matrix). This demonstrates that the linear recombination of the variables stil represents the covariance of the raw data.</p>
<p>But, <strong>what if we used only seven of those components instead of all eight</strong>? Our reproduced covariance or correlation matrix will be very similar, but not identical to, the original matrix because that last component was explaining some proportion of the variance. Now, we see some residuals. The seven components do not perfectly explain the data.</p>
<div class="sourceCode" id="cb344"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb344-1"><a href="week-06-principal-components-analysis.html#cb344-1" tabindex="-1"></a>R.reprod3 <span class="ot">&lt;-</span> A.star[,<span class="dv">1</span><span class="sc">:</span><span class="dv">7</span>] <span class="sc">%*%</span> <span class="fu">t</span>( (A.star)[,<span class="dv">1</span><span class="sc">:</span><span class="dv">7</span>] )</span>
<span id="cb344-2"><a href="week-06-principal-components-analysis.html#cb344-2" tabindex="-1"></a><span class="fu">round</span>( blood_corr <span class="sc">-</span> R.reprod3, <span class="dv">3</span>)</span></code></pre></div>
<pre><code>##        rblood plate wblood   neut  lymph  bilir sodium potass
## rblood  0.001 0.000  0.001  0.007  0.008 -0.001  0.000  0.000
## plate   0.000 0.000  0.000  0.001  0.001  0.000  0.000  0.000
## wblood  0.001 0.000  0.000  0.003  0.004  0.000  0.000  0.000
## neut    0.007 0.001  0.003  0.047  0.050 -0.004  0.001 -0.002
## lymph   0.008 0.001  0.004  0.050  0.054 -0.004  0.001 -0.002
## bilir  -0.001 0.000  0.000 -0.004 -0.004  0.000  0.000  0.000
## sodium  0.000 0.000  0.000  0.001  0.001  0.000  0.000  0.000
## potass  0.000 0.000  0.000 -0.002 -0.002  0.000  0.000  0.000</code></pre>
<p>When we reduce the data to fewer components, we’re seeking parsimony while capitalizing on the optimal linear combinations of the variables, but we are not perfectly explaining the data.</p>
</div>
<div id="summary-of-the-vectors-and-matrices" class="section level4 hasAnchor" number="7.1.3.5">
<h4><span class="header-section-number">7.1.3.5</span> Summary of the vectors and matrices<a href="week-06-principal-components-analysis.html#summary-of-the-vectors-and-matrices" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We have the following vectors and matrices:</p>
<blockquote>
<p><span class="math inline">\(\mathbf{a}_i\)</span> An eigenvector. <span class="math inline">\(\mathbf{a}_1\)</span> is eigenvector 1, <span class="math inline">\(\mathbf{a}_2\)</span> is eigenvector 2, and so forth</p>
</blockquote>
<blockquote>
<p><span class="math inline">\(\mathbf{A}\)</span> A matrix of eigenvectors. Column 1 is eigenvector 1. Column 2 is eigenvector 2, and so forth</p>
</blockquote>
<blockquote>
<p><span class="math inline">\(\mathbf{a}^*_i\)</span> A rescaled eigenvector, so we can interpret the coefficients as correlations (or so-called loadings)</p>
</blockquote>
<blockquote>
<p><span class="math inline">\(\mathbf{A}^*\)</span> A matrix of rescaled eigenvectors</p>
</blockquote>
<blockquote>
<p><span class="math inline">\(\mathbf{\lambda_i}\)</span> Lowercase lambda, the eigenvalue of Component <span class="math inline">\(i\)</span></p>
</blockquote>
<blockquote>
<p><span class="math inline">\(\mathbf{\Lambda}\)</span> Uppercase lambda, a diagonal matrix of eigenvalues, also represented as <span class="math inline">\(\mathbf{\lambda} \mathbf{I}\)</span></p>
</blockquote>
<blockquote>
<p><span class="math inline">\(\mathbf{S}\)</span> The covariance matrix of the observed data</p>
</blockquote>
<blockquote>
<p><span class="math inline">\(\mathbf{R}\)</span> The correlation matrix of the observed data</p>
</blockquote>
<p>It is worth noting that correlation, eigenvectors, and eigenvalues are all related:</p>
<blockquote>
<p>An eigenvalue matrix is related to the eigevectors and correlation matrix, <span class="math inline">\(\mathbf{\Lambda} = \mathbf{A}^\prime \mathbf{R} \mathbf{A}\)</span></p>
</blockquote>
<div class="sourceCode" id="cb346"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb346-1"><a href="week-06-principal-components-analysis.html#cb346-1" tabindex="-1"></a>Lambda <span class="ot">&lt;-</span> <span class="fu">t</span>(vals.and.vects<span class="sc">$</span>vectors) <span class="sc">%*%</span> blood_corr <span class="sc">%*%</span> vals.and.vects<span class="sc">$</span>vectors </span>
<span id="cb346-2"><a href="week-06-principal-components-analysis.html#cb346-2" tabindex="-1"></a><span class="fu">round</span>(Lambda, <span class="dv">2</span>)</span></code></pre></div>
<pre><code>##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]
## [1,] 2.79 0.00 0.00 0.00 0.00 0.00 0.00  0.0
## [2,] 0.00 1.53 0.00 0.00 0.00 0.00 0.00  0.0
## [3,] 0.00 0.00 1.25 0.00 0.00 0.00 0.00  0.0
## [4,] 0.00 0.00 0.00 0.78 0.00 0.00 0.00  0.0
## [5,] 0.00 0.00 0.00 0.00 0.62 0.00 0.00  0.0
## [6,] 0.00 0.00 0.00 0.00 0.00 0.49 0.00  0.0
## [7,] 0.00 0.00 0.00 0.00 0.00 0.00 0.44  0.0
## [8,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.1</code></pre>
<blockquote>
<p>A correlation is reproduced from the eigenvectors and eigenvalues, <span class="math inline">\(\mathbf{R} = \mathbf{A} \mathbf{\Lambda} \mathbf{A}^\prime\)</span>, or <span class="math inline">\(\mathbf{R} = (\mathbf{A} \mathbf{\Lambda^{1/2}})( \mathbf{\Lambda^{1/2}} \mathbf{A}^\prime)\)</span></p>
</blockquote>
<div class="sourceCode" id="cb348"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb348-1"><a href="week-06-principal-components-analysis.html#cb348-1" tabindex="-1"></a>R <span class="ot">&lt;-</span> vals.and.vects<span class="sc">$</span>vectors <span class="sc">%*%</span> Lambda <span class="sc">%*%</span> <span class="fu">t</span>(vals.and.vects<span class="sc">$</span>vectors) </span>
<span id="cb348-2"><a href="week-06-principal-components-analysis.html#cb348-2" tabindex="-1"></a><span class="fu">round</span>(R, <span class="dv">2</span>)</span></code></pre></div>
<pre><code>##       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]
## [1,]  1.00  0.29  0.20 -0.06 -0.11 -0.25 -0.23  0.06
## [2,]  0.29  1.00  0.42  0.29 -0.38 -0.35 -0.16 -0.13
## [3,]  0.20  0.42  1.00  0.42 -0.52 -0.44 -0.15 -0.08
## [4,] -0.06  0.28  0.42  1.00 -0.88 -0.08  0.02 -0.13
## [5,] -0.11 -0.38 -0.52 -0.88  1.00  0.21  0.03  0.15
## [6,] -0.25 -0.35 -0.44 -0.08  0.21  1.00  0.19  0.08
## [7,] -0.23 -0.16 -0.15  0.02  0.03  0.19  1.00  0.42
## [8,]  0.06 -0.13 -0.08 -0.13  0.15  0.08  0.42  1.00</code></pre>
</div>
</div>
<div id="using-the-psych-package" class="section level3 hasAnchor" number="7.1.4">
<h3><span class="header-section-number">7.1.4</span> Using the psych package<a href="week-06-principal-components-analysis.html#using-the-psych-package" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The psych package’s <code>pca()</code> function reports results that are ready for interpretation. It automatically uses the correlation matrix rather than the covariance matrix.</p>
<p>We specify the correlation matrix or the raw data, the number of factors we wish to extract, which in a preliminary analysis is the number of observed variables (8 in our example). In PCA, we can avoid rotating the solution for now.</p>
<div class="sourceCode" id="cb350"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb350-1"><a href="week-06-principal-components-analysis.html#cb350-1" tabindex="-1"></a><span class="fu">library</span>(psych)</span>
<span id="cb350-2"><a href="week-06-principal-components-analysis.html#cb350-2" tabindex="-1"></a>pc_psy <span class="ot">&lt;-</span> psych<span class="sc">::</span><span class="fu">pca</span>(blood_corr,   <span class="co"># can also use the raw data as input.</span></span>
<span id="cb350-3"><a href="week-06-principal-components-analysis.html#cb350-3" tabindex="-1"></a>              <span class="at">nfactors =</span> <span class="dv">8</span>, </span>
<span id="cb350-4"><a href="week-06-principal-components-analysis.html#cb350-4" tabindex="-1"></a>              <span class="at">rotate =</span> <span class="st">&#39;none&#39;</span>)</span>
<span id="cb350-5"><a href="week-06-principal-components-analysis.html#cb350-5" tabindex="-1"></a><span class="fu">print</span>(pc_psy,<span class="at">digits=</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## Principal Components Analysis
## Call: principal(r = r, nfactors = nfactors, residuals = residuals, 
##     rotate = rotate, n.obs = n.obs, covar = covar, scores = scores, 
##     missing = missing, impute = impute, oblique.scores = oblique.scores, 
##     method = method, use = use, cor = cor, correct = 0.5, weight = NULL)
## Standardized loadings (pattern matrix) based upon correlation matrix
##          PC1   PC2   PC3   PC4   PC5   PC6   PC7   PC8 h2       u2 com
## rblood  0.32 -0.52  0.45  0.57 -0.14 -0.25  0.12  0.03  1 -4.4e-16 4.2
## plate   0.67 -0.19  0.19  0.06  0.67  0.16 -0.07  0.00  1  1.1e-15 2.5
## wblood  0.77  0.00  0.19 -0.24 -0.20  0.28  0.45  0.02  1  1.4e-15 2.5
## neut    0.72  0.58 -0.19  0.15 -0.09 -0.05 -0.16  0.22  1  2.0e-15 2.6
## lymph  -0.83 -0.45  0.10 -0.16  0.11  0.09  0.10  0.23  1  6.7e-16 2.0
## bilir  -0.53  0.40 -0.31  0.56  0.13  0.27  0.25 -0.02  1  1.8e-15 4.5
## sodium -0.30  0.66  0.46 -0.14  0.24 -0.36  0.24  0.00  1  1.8e-15 3.7
## potass -0.28  0.30  0.79  0.08 -0.16  0.33 -0.25 -0.01  1  2.8e-15 2.4
## 
##                        PC1  PC2  PC3  PC4  PC5  PC6  PC7  PC8
## SS loadings           2.79 1.53 1.25 0.78 0.62 0.49 0.44 0.10
## Proportion Var        0.35 0.19 0.16 0.10 0.08 0.06 0.05 0.01
## Cumulative Var        0.35 0.54 0.70 0.79 0.87 0.93 0.99 1.00
## Proportion Explained  0.35 0.19 0.16 0.10 0.08 0.06 0.05 0.01
## Cumulative Proportion 0.35 0.54 0.70 0.79 0.87 0.93 0.99 1.00
## 
## Mean item complexity =  3
## Test of the hypothesis that 8 components are sufficient.
## 
## The root mean square of the residuals (RMSR) is  0 
## 
## Fit based upon off diagonal values = 1</code></pre>
<p>The output under the label <code>Standardized loadings</code> is the same reweighted eigenvector matrix, <span class="math inline">\(\mathbf{A^*}\)</span>, we calculated above. Again, these represent the correlations between each observed variable and each component.</p>
</div>
<div id="determining-how-many-components-to-retain" class="section level3 hasAnchor" number="7.1.5">
<h3><span class="header-section-number">7.1.5</span> Determining how many components to retain<a href="week-06-principal-components-analysis.html#determining-how-many-components-to-retain" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>One of the most valuable reasons for using PCA is for dimension reduction. There are several different methods for determining how many components to retain. Some analysts set the threshold to be sure that they have enough components to explain some pre-specified proportion of variance, such as 80%. Some use Kaiser’s rule of retaining components of values of at least <span class="math inline">\(1.00\)</span>, though Joliffe <span class="citation">(<a href="#ref-jolliffe_discarding_1972">1972</a>)</span> suggested a threshold of <span class="math inline">\(0.70\)</span>. Others <span class="citation">(<a href="#ref-cattell_scree_1966">Cattell 1966</a>)</span> suggested using a scree plot to see where the elbow in the curve is, though this may be more appropriate for factor analysis rather than PCA. Another is the parallel test. And still another is the Hull test, which can be used if the PCA is on the data set rather than a correlation matrix.</p>
<div id="kaisers-rule" class="section level4 hasAnchor" number="7.1.5.1">
<h4><span class="header-section-number">7.1.5.1</span> 1. Kaiser’s rule<a href="week-06-principal-components-analysis.html#kaisers-rule" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The eigenvalue-greater-than-one approach, also called the K1 criterion named after the Kaiser’s <span class="citation">(<a href="#ref-kaiser_index_1974">1974</a>)</span> work, can be used in PCA to determine the number of factors to retain.</p>
<div class="sourceCode" id="cb352"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb352-1"><a href="week-06-principal-components-analysis.html#cb352-1" tabindex="-1"></a>pc_psy<span class="sc">$</span>loadings</span></code></pre></div>
<pre><code>## 
## Loadings:
##        PC1    PC2    PC3    PC4    PC5    PC6    PC7    PC8   
## rblood  0.325 -0.516  0.447  0.575 -0.138 -0.254  0.116       
## plate   0.669 -0.191  0.187         0.668  0.161              
## wblood  0.767         0.188 -0.242 -0.198  0.282  0.447       
## neut    0.719  0.585 -0.191  0.149               -0.156  0.217
## lymph  -0.825 -0.446        -0.159  0.110         0.104  0.232
## bilir  -0.534  0.396 -0.309  0.559  0.127  0.268  0.249       
## sodium -0.296  0.662  0.459 -0.144  0.236 -0.359  0.242       
## potass -0.285  0.304  0.792        -0.156  0.328 -0.248       
## 
##                  PC1   PC2   PC3   PC4   PC5   PC6   PC7   PC8
## SS loadings    2.792 1.532 1.249 0.778 0.622 0.489 0.436 0.102
## Proportion Var 0.349 0.191 0.156 0.097 0.078 0.061 0.054 0.013
## Cumulative Var 0.349 0.540 0.697 0.794 0.872 0.933 0.987 1.000</code></pre>
<p>Based on this criterion, we would retain three components, as the eigenvalues, listed in the <code>SS loadings</code> row are above 1.00 in the first three components. If we used 80% of the variance explained as a criterion instead of Kaiser’s rule, we would look at the <code>pca()</code> output at the <code>Cumulative Proportion</code> line and see that we’d need more at least five components, as Component 4 explains 79% of the variance. It’s borderline, but them’s the rules we would have set for ourselves.</p>
</div>
<div id="scree-plot" class="section level4 hasAnchor" number="7.1.5.2">
<h4><span class="header-section-number">7.1.5.2</span> 2. Scree plot<a href="week-06-principal-components-analysis.html#scree-plot" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Let’s generate a scree plot. We can use this to not only see those components with eigenvalues that exceed 1.00 but we can see where the plot levels off.</p>
<p>Though there are packages that can do this for us, we can do this with ggplot.<a href="#fn30" class="footnote-ref" id="fnref30"><sup>30</sup></a> First, we create a data frame that includes the component number and the eigenvalue. For this, we can get the eigenvalues from the <code>psych::pca()</code> outputted object with the <code>$values</code>.</p>
<div class="sourceCode" id="cb354"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb354-1"><a href="week-06-principal-components-analysis.html#cb354-1" tabindex="-1"></a>vals.and.vects <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">Component.number =</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(pc_psy<span class="sc">$</span>values), </span>
<span id="cb354-2"><a href="week-06-principal-components-analysis.html#cb354-2" tabindex="-1"></a>                             <span class="at">Eigenvalue =</span> pc_psy<span class="sc">$</span>values)</span>
<span id="cb354-3"><a href="week-06-principal-components-analysis.html#cb354-3" tabindex="-1"></a><span class="co"># Commented out here is the corresponding code if we used the object outputted </span></span>
<span id="cb354-4"><a href="week-06-principal-components-analysis.html#cb354-4" tabindex="-1"></a><span class="co"># from the `princomp()` function.</span></span>
<span id="cb354-5"><a href="week-06-principal-components-analysis.html#cb354-5" tabindex="-1"></a><span class="co"># vals.and.vects &lt;- data.frame(Component.number = 1:length(blood_pcacor$sdev^2), </span></span>
<span id="cb354-6"><a href="week-06-principal-components-analysis.html#cb354-6" tabindex="-1"></a><span class="co">#                    Eigenvalue    = blood_pcacor$sdev^2)</span></span>
<span id="cb354-7"><a href="week-06-principal-components-analysis.html#cb354-7" tabindex="-1"></a><span class="fu">ggplot</span>(vals.and.vects, <span class="fu">aes</span>(<span class="at">x =</span> Component.number, <span class="at">y =</span> Eigenvalue ) ) <span class="sc">+</span></span>
<span id="cb354-8"><a href="week-06-principal-components-analysis.html#cb354-8" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb354-9"><a href="week-06-principal-components-analysis.html#cb354-9" tabindex="-1"></a>  <span class="fu">geom_point</span>()</span></code></pre></div>
<p><img src="multivariate_stats_files/figure-html/unnamed-chunk-175-1.png" width="672" /></p>
<p>With this plot, we seek to find where the slope changes. The first component is clearly present, as the slope is much steeper from the first to second component than it is from the second to third component, whose slope resembles the rest of the scree. In other words, if we drew a line of best for all of the eigenvalues from Component 2 through 8, such a line would have the points close to it. However, we might decide to retain three components because the eigenvalues after Component 3 seem to flatten out. This is a good example of when this eyeballing technique is not so straightforward.</p>
</div>
<div id="parallel-analysis" class="section level4 hasAnchor" number="7.1.5.3">
<h4><span class="header-section-number">7.1.5.3</span> 3. Parallel analysis<a href="week-06-principal-components-analysis.html#parallel-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The parallel analysis <span class="citation">(<a href="#ref-horn_rationale_1965">Horn 1965</a>)</span> provides a method for comparing our data to randomly generated data sets having the same number of cases and observed variables. After ranking these randomly generated data from low to high, the procedure can estimate the eigenvalue at each <span class="math inline">\(n\)</span>th component at the 95 percentile level. We compare the eigenvalues calculated from our observed data with those of the randomly generated data. If with a given model, such as one with <span class="math inline">\(n\)</span> components, 95% of the randomly drawn samples have eigenvalues that <strong>are lower than</strong> what we observe in our data, we can assume that the component structure in our data lies outside that 0-to-95 percent range (in other words, our extreme eigenvalues do not likely to occur by random luck). The last component to meet this criterion is the number of components to retain. Up to that point, the components explain more of the variance than would occur by chance at the 95% cutoff.</p>
<p>We can use the <code>parallel()</code> function from the nFactors package <span class="citation">(<a href="#ref-R-nFactors">Raiche and Magis 2025</a>)</span> to perform a parallel analysis. The parallel analysis takes random draws of data and estimates the eigenvalues that would occur randomly in each random draw. The mean of the random draws is reported, along with the 95 percentile. We want to use the 95 percentile.</p>
<p>The first step is to compute the eigenvalues, which we did above with the scree plot.</p>
<div class="sourceCode" id="cb355"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb355-1"><a href="week-06-principal-components-analysis.html#cb355-1" tabindex="-1"></a><span class="fu">library</span>(nFactors)</span>
<span id="cb355-2"><a href="week-06-principal-components-analysis.html#cb355-2" tabindex="-1"></a><span class="co"># help(package=&quot;nFactors&quot;)</span></span>
<span id="cb355-3"><a href="week-06-principal-components-analysis.html#cb355-3" tabindex="-1"></a>n_i  <span class="ot">&lt;-</span> <span class="dv">72</span> <span class="co"># The number of cases in our data</span></span>
<span id="cb355-4"><a href="week-06-principal-components-analysis.html#cb355-4" tabindex="-1"></a>n_p <span class="ot">&lt;-</span> <span class="fu">ncol</span>(blood_corr) <span class="co"># The number of variables in our data</span></span>
<span id="cb355-5"><a href="week-06-principal-components-analysis.html#cb355-5" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)            <span class="co"># To reproduce our randomly generated results.</span></span>
<span id="cb355-6"><a href="week-06-principal-components-analysis.html#cb355-6" tabindex="-1"></a>Eigs <span class="ot">&lt;-</span> pc_psy<span class="sc">$</span>values    <span class="co"># The eigenvalues</span></span>
<span id="cb355-7"><a href="week-06-principal-components-analysis.html#cb355-7" tabindex="-1"></a>n_components  <span class="ot">&lt;-</span> <span class="fu">length</span>(Eigs) <span class="co"># number of components</span></span>
<span id="cb355-8"><a href="week-06-principal-components-analysis.html#cb355-8" tabindex="-1"></a>paral <span class="ot">&lt;-</span> <span class="fu">parallel</span>(<span class="at">subject =</span> n_i,  <span class="co"># The number of cases in our data</span></span>
<span id="cb355-9"><a href="week-06-principal-components-analysis.html#cb355-9" tabindex="-1"></a>                      <span class="at">var =</span> n_p,  <span class="co"># The number of variables in our data</span></span>
<span id="cb355-10"><a href="week-06-principal-components-analysis.html#cb355-10" tabindex="-1"></a>                      <span class="at">rep =</span> <span class="dv">100</span>,</span>
<span id="cb355-11"><a href="week-06-principal-components-analysis.html#cb355-11" tabindex="-1"></a>                 <span class="at">quantile =</span> .<span class="dv">95</span>,</span>
<span id="cb355-12"><a href="week-06-principal-components-analysis.html#cb355-12" tabindex="-1"></a>                   <span class="at">model  =</span> <span class="st">&quot;components&quot;</span>)</span>
<span id="cb355-13"><a href="week-06-principal-components-analysis.html#cb355-13" tabindex="-1"></a></span>
<span id="cb355-14"><a href="week-06-principal-components-analysis.html#cb355-14" tabindex="-1"></a>ParallelAna <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">Ncomponent  =</span> <span class="dv">1</span><span class="sc">:</span>n_components,</span>
<span id="cb355-15"><a href="week-06-principal-components-analysis.html#cb355-15" tabindex="-1"></a>                           Eigs,</span>
<span id="cb355-16"><a href="week-06-principal-components-analysis.html#cb355-16" tabindex="-1"></a>                           <span class="at">RandEigM =</span> paral<span class="sc">$</span>eigen<span class="sc">$</span>mevpea,</span>
<span id="cb355-17"><a href="week-06-principal-components-analysis.html#cb355-17" tabindex="-1"></a>                           <span class="at">RandEig95=</span> paral<span class="sc">$</span>eigen<span class="sc">$</span>qevpea)</span>
<span id="cb355-18"><a href="week-06-principal-components-analysis.html#cb355-18" tabindex="-1"></a>ParallelAna <span class="ot">&lt;-</span> <span class="fu">round</span>(ParallelAna, <span class="dv">3</span>)</span>
<span id="cb355-19"><a href="week-06-principal-components-analysis.html#cb355-19" tabindex="-1"></a>ParallelAna</span></code></pre></div>
<pre><code>##   Ncomponent  Eigs RandEigM RandEig95
## 1          1 2.792    1.527     1.707
## 2          2 1.532    1.315     1.454
## 3          3 1.249    1.159     1.265
## 4          4 0.778    1.033     1.122
## 5          5 0.622    0.919     0.980
## 6          6 0.489    0.803     0.882
## 7          7 0.436    0.688     0.778
## 8          8 0.102    0.556     0.660</code></pre>
<p>Based on these results, it looks like we should retain two components, as explained below.</p>
<p>First, it is worth mentioning that because this operation is based on random draws of the data, the point estimate and 95th percentile estimate will differ if we performed this operation again. In our code, we used the <code>set.seed()</code> function to make this particular result reproducible. We can remove that line of code or change the seed values and observe different results.<a href="#fn31" class="footnote-ref" id="fnref31"><sup>31</sup></a></p>
<p>In our output, we can use the rightmost column and identify at what point 95% of the randomly drawn data’s eigenvalues exceed our reduced-eigenvalue estimates. We see that it is at component number <span class="math inline">\(3\)</span> that our observed reduced eigenvalue (<span class="math inline">\(1.249\)</span>) is exceeded by the randomly generated eigenvalue (<span class="math inline">\(1.265\)</span>). Based on this, we step down to one component below this and decide we should retain <span class="math inline">\(2\)</span> components.</p>
<p><strong>How many should we retain?</strong> The parallel analysis might make the most sense with these data, as it is compares random draws. With this, we might retain two components.</p>
</div>
</div>
<div id="reduced-pca-model" class="section level3 hasAnchor" number="7.1.6">
<h3><span class="header-section-number">7.1.6</span> Reduced PCA model<a href="week-06-principal-components-analysis.html#reduced-pca-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb357"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb357-1"><a href="week-06-principal-components-analysis.html#cb357-1" tabindex="-1"></a><span class="fu">library</span>(psych)</span>
<span id="cb357-2"><a href="week-06-principal-components-analysis.html#cb357-2" tabindex="-1"></a>pc_psy <span class="ot">&lt;-</span> psych<span class="sc">::</span><span class="fu">pca</span>(blood_corr, </span>
<span id="cb357-3"><a href="week-06-principal-components-analysis.html#cb357-3" tabindex="-1"></a>              <span class="at">nfactors =</span> <span class="dv">2</span>, </span>
<span id="cb357-4"><a href="week-06-principal-components-analysis.html#cb357-4" tabindex="-1"></a>              <span class="at">rotate =</span> <span class="st">&#39;none&#39;</span>)</span>
<span id="cb357-5"><a href="week-06-principal-components-analysis.html#cb357-5" tabindex="-1"></a><span class="fu">print</span>(pc_psy, <span class="at">digits =</span> <span class="dv">3</span>)</span></code></pre></div>
<pre><code>## Principal Components Analysis
## Call: principal(r = r, nfactors = nfactors, residuals = residuals, 
##     rotate = rotate, n.obs = n.obs, covar = covar, scores = scores, 
##     missing = missing, impute = impute, oblique.scores = oblique.scores, 
##     method = method, use = use, cor = cor, correct = 0.5, weight = NULL)
## Standardized loadings (pattern matrix) based upon correlation matrix
##           PC1    PC2    h2    u2  com
## rblood  0.325 -0.516 0.372 0.628 1.68
## plate   0.669 -0.191 0.484 0.516 1.16
## wblood  0.767  0.000 0.588 0.412 1.00
## neut    0.719  0.585 0.859 0.141 1.92
## lymph  -0.825 -0.446 0.880 0.120 1.54
## bilir  -0.534  0.396 0.442 0.558 1.85
## sodium -0.296  0.662 0.526 0.474 1.38
## potass -0.285  0.304 0.173 0.827 1.99
## 
##                         PC1   PC2
## SS loadings           2.792 1.532
## Proportion Var        0.349 0.191
## Cumulative Var        0.349 0.540
## Proportion Explained  0.646 0.354
## Cumulative Proportion 0.646 1.000
## 
## Mean item complexity =  1.6
## Test of the hypothesis that 2 components are sufficient.
## 
## The root mean square of the residuals (RMSR) is  0.127 
## 
## Fit based upon off diagonal values = 0.823</code></pre>
</div>
<div id="interpretation-1" class="section level3 hasAnchor" number="7.1.7">
<h3><span class="header-section-number">7.1.7</span> Interpretation<a href="week-06-principal-components-analysis.html#interpretation-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The pysch output is formatted for factor-analysis, which is closely related to PCA but used for a a different purpose (EFA is for estimating latent variables). Here, we have the first two components, <code>PC1</code> and <code>PC2</code>, the communalities <code>h2</code>, the unique variances, <code>u2</code>, and the complexity, <code>com</code>. These were not explained in our reading but will be in the explanation about exploratory factor analysis. Each observed variable’s communality is the proportion of its variance that is shared with the two factors. If we sum the squared coefficients, we get the communality; for example, with <code>rblood</code>, <span class="math inline">\(.325^2 + (-.516)^2 = .372\)</span>. The unique variance is what is not explained by the shared variance; with this example, it is <span class="math inline">\(1 - .3718 = .628\)</span>. The complexity refers to how strongly the item loads on other components. The last observed variable, potassium, seems to be evenly shared across two components; it’s complexity is <code>1.99</code> and we can see that indeed its loadings on PC1 and PC2 are nearly equal in strength. It is easier to interpret models that have low complexity.</p>
<p>These coefficients (or “loadings” to some researchers) are unrotated. In this data set, it seems that the first component is due to a large part from the variability in the lymphocites variable, (-825) as well as to the white blood cell counts, neutrophil levels, platelet counts, bilirubin levels, and to a small extent red blood cell counts. The component seems to be explained in one direction by lymphocyte levels and bilirubin levels and in the opposite direction from the others.<a href="#fn32" class="footnote-ref" id="fnref32"><sup>32</sup></a></p>
<p>The second component seems to be an indicator of levels of sodium, neutrophil, red blood cells, lymphocytes, bilirubin, and to a small extent, potassium, with different signs among these. Interpreting these is not easy because the components do not seem to parsimoniously explain the observed variables. When this occurs, many analysts will rotate the solution so that the components more easily align with the variables; others will disagree and state that this changes the structure.</p>
<div id="plot" class="section level4 hasAnchor" number="7.1.7.1">
<h4><span class="header-section-number">7.1.7.1</span> Plot<a href="week-06-principal-components-analysis.html#plot" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Let’s create a plot of the component scores.</p>
<p>To prepare for this, let’s save the rescaled eigenvector matrix (the loadings). We’ll use the <code>cbind()</code> function on the <code>$loadings</code> part of the outputted object from our PCA model.</p>
<div class="sourceCode" id="cb359"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb359-1"><a href="week-06-principal-components-analysis.html#cb359-1" tabindex="-1"></a>A <span class="ot">&lt;-</span> <span class="fu">cbind</span>(pc_psy<span class="sc">$</span>loadings) </span>
<span id="cb359-2"><a href="week-06-principal-components-analysis.html#cb359-2" tabindex="-1"></a>A</span></code></pre></div>
<pre><code>##               PC1           PC2
## rblood  0.3245441 -0.5162251871
## plate   0.6690097 -0.1905001868
## wblood  0.7667919  0.0003694159
## neut    0.7190955  0.5846875596
## lymph  -0.8251026 -0.4460871309
## bilir  -0.5338122  0.3962331606
## sodium -0.2955777  0.6624463145
## potass -0.2849339  0.3035590847</code></pre>
<div class="sourceCode" id="cb361"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb361-1"><a href="week-06-principal-components-analysis.html#cb361-1" tabindex="-1"></a>A <span class="sc">%&gt;%</span> </span>
<span id="cb361-2"><a href="week-06-principal-components-analysis.html#cb361-2" tabindex="-1"></a>  <span class="fu">data.frame</span>(.) <span class="sc">%&gt;%</span> </span>
<span id="cb361-3"><a href="week-06-principal-components-analysis.html#cb361-3" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> PC1, <span class="at">y =</span> PC2)) <span class="sc">+</span></span>
<span id="cb361-4"><a href="week-06-principal-components-analysis.html#cb361-4" tabindex="-1"></a>  <span class="fu">geom_vline</span>( <span class="fu">aes</span>(<span class="at">xintercept =</span> <span class="dv">0</span>) ) <span class="sc">+</span></span>
<span id="cb361-5"><a href="week-06-principal-components-analysis.html#cb361-5" tabindex="-1"></a>  <span class="fu">geom_hline</span>( <span class="fu">aes</span>(<span class="at">yintercept =</span> <span class="dv">0</span>) ) <span class="sc">+</span></span>
<span id="cb361-6"><a href="week-06-principal-components-analysis.html#cb361-6" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb361-7"><a href="week-06-principal-components-analysis.html#cb361-7" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">limits=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb361-8"><a href="week-06-principal-components-analysis.html#cb361-8" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="at">limits=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb361-9"><a href="week-06-principal-components-analysis.html#cb361-9" tabindex="-1"></a>  <span class="fu">geom_text</span>(<span class="at">hjust=</span><span class="sc">-</span>.<span class="dv">2</span>, <span class="at">vjust=</span><span class="dv">0</span>, <span class="at">label =</span> <span class="fu">rownames</span>(A)) <span class="sc">+</span></span>
<span id="cb361-10"><a href="week-06-principal-components-analysis.html#cb361-10" tabindex="-1"></a>  <span class="fu">labs</span>( <span class="at">title =</span> <span class="st">&quot;Unrotated PCA Solution&quot;</span>)</span></code></pre></div>
<p><img src="multivariate_stats_files/figure-html/unnamed-chunk-180-1.png" width="672" /></p>
<p>This kind of plot helps us to see which variables are more strongly explaining which component. Those closest to the horizontal axis and farther away from the origin are more strongly contributing to Component 1. Those closer to the vertical axis and farther away from the origin more strongly explain Component 2, which in this case does not seem to include many; sodium is the strongest one with Component 2.</p>
</div>
</div>
<div id="interpretation-of-the-rotated-solution" class="section level3 hasAnchor" number="7.1.8">
<h3><span class="header-section-number">7.1.8</span> Interpretation of the rotated solution<a href="week-06-principal-components-analysis.html#interpretation-of-the-rotated-solution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Some researchers caution against rotating the solution, as it changes structure. Others argue for it, as it helps us to explain the components. We specify <code>rotate = varimax</code> for varimax rotation so that the PCA solution remains orthogonal; that is, so that the components do not correlate with each other but rather explain unrelated features of the combination of variables.<a href="#fn33" class="footnote-ref" id="fnref33"><sup>33</sup></a> This type of orthogonal rotation is probably fine with data that are assumed to exist in the world rather. With data that are based on social or psychological constructions, such as academic proficiency, orthogonal rotation makes less sense.</p>
<div class="sourceCode" id="cb362"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb362-1"><a href="week-06-principal-components-analysis.html#cb362-1" tabindex="-1"></a><span class="fu">library</span>(psych)</span>
<span id="cb362-2"><a href="week-06-principal-components-analysis.html#cb362-2" tabindex="-1"></a>pc_psy_rot <span class="ot">&lt;-</span> psych<span class="sc">::</span><span class="fu">pca</span>(blood_corr, </span>
<span id="cb362-3"><a href="week-06-principal-components-analysis.html#cb362-3" tabindex="-1"></a>              <span class="at">nfactors =</span> <span class="dv">2</span>, </span>
<span id="cb362-4"><a href="week-06-principal-components-analysis.html#cb362-4" tabindex="-1"></a>              <span class="at">rotate =</span> <span class="st">&#39;varimax&#39;</span>)</span>
<span id="cb362-5"><a href="week-06-principal-components-analysis.html#cb362-5" tabindex="-1"></a><span class="fu">print</span>(pc_psy_rot, <span class="at">digits =</span> <span class="dv">3</span>)</span></code></pre></div>
<pre><code>## Principal Components Analysis
## Call: principal(r = r, nfactors = nfactors, residuals = residuals, 
##     rotate = rotate, n.obs = n.obs, covar = covar, scores = scores, 
##     missing = missing, impute = impute, oblique.scores = oblique.scores, 
##     method = method, use = use, cor = cor, correct = 0.5, weight = NULL)
## Standardized loadings (pattern matrix) based upon correlation matrix
##           RC1    RC2    h2    u2  com
## rblood  0.003 -0.610 0.372 0.628 1.00
## plate   0.467 -0.515 0.484 0.516 1.98
## wblood  0.651 -0.405 0.588 0.412 1.67
## neut    0.919  0.116 0.859 0.141 1.03
## lymph  -0.936  0.058 0.880 0.120 1.01
## bilir  -0.244  0.619 0.442 0.558 1.30
## sodium  0.099  0.719 0.526 0.474 1.04
## potass -0.081  0.408 0.173 0.827 1.08
## 
##                         RC1   RC2
## SS loadings           2.440 1.884
## Proportion Var        0.305 0.235
## Cumulative Var        0.305 0.540
## Proportion Explained  0.564 0.436
## Cumulative Proportion 0.564 1.000
## 
## Mean item complexity =  1.3
## Test of the hypothesis that 2 components are sufficient.
## 
## The root mean square of the residuals (RMSR) is  0.127 
## 
## Fit based upon off diagonal values = 0.823</code></pre>
<p>In this rotated structure, we see it is easier to give a label to the first component as being explained by lymphocyte and neutrophil levels (in opposite directions), and white blood cell and platelet counts.</p>
<div class="sourceCode" id="cb364"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb364-1"><a href="week-06-principal-components-analysis.html#cb364-1" tabindex="-1"></a>A_rot <span class="ot">&lt;-</span> <span class="fu">cbind</span>(pc_psy_rot<span class="sc">$</span>loadings) </span>
<span id="cb364-2"><a href="week-06-principal-components-analysis.html#cb364-2" tabindex="-1"></a></span>
<span id="cb364-3"><a href="week-06-principal-components-analysis.html#cb364-3" tabindex="-1"></a>A_rot <span class="sc">%&gt;%</span> </span>
<span id="cb364-4"><a href="week-06-principal-components-analysis.html#cb364-4" tabindex="-1"></a>  <span class="fu">data.frame</span>(.) <span class="sc">%&gt;%</span> </span>
<span id="cb364-5"><a href="week-06-principal-components-analysis.html#cb364-5" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> RC1, <span class="at">y =</span> RC2)) <span class="sc">+</span></span>
<span id="cb364-6"><a href="week-06-principal-components-analysis.html#cb364-6" tabindex="-1"></a>  <span class="fu">geom_vline</span>( <span class="fu">aes</span>(<span class="at">xintercept =</span> <span class="dv">0</span>) ) <span class="sc">+</span></span>
<span id="cb364-7"><a href="week-06-principal-components-analysis.html#cb364-7" tabindex="-1"></a>  <span class="fu">geom_hline</span>( <span class="fu">aes</span>(<span class="at">yintercept =</span> <span class="dv">0</span>) ) <span class="sc">+</span></span>
<span id="cb364-8"><a href="week-06-principal-components-analysis.html#cb364-8" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb364-9"><a href="week-06-principal-components-analysis.html#cb364-9" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">limits=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb364-10"><a href="week-06-principal-components-analysis.html#cb364-10" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="at">limits=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb364-11"><a href="week-06-principal-components-analysis.html#cb364-11" tabindex="-1"></a>  <span class="fu">geom_text</span>(<span class="at">hjust=</span><span class="sc">-</span>.<span class="dv">2</span>, <span class="at">vjust=</span><span class="dv">0</span>, <span class="at">label =</span> <span class="fu">rownames</span>(A)) <span class="sc">+</span></span>
<span id="cb364-12"><a href="week-06-principal-components-analysis.html#cb364-12" tabindex="-1"></a>  <span class="fu">labs</span>( <span class="at">title =</span> <span class="st">&quot;Rotated PCA Solution&quot;</span>)</span></code></pre></div>
<p><img src="multivariate_stats_files/figure-html/unnamed-chunk-182-1.png" width="672" />
In this plot, the variables all remain in same space relative to each other. It is the component axes that have moved. In comparing this to the unrotated solution, it looks like we rotated the axes about 45 degrees.</p>
<p>We see that with the rotated solution, the axes of the components have been moved closer to the observed variables, thereby reducing their complexity and making it easier to associate some variables with individual components. This comes at a cost, however, as we also see that now, compared to our unrotated model, the platelet and white-blood cell variables are now somewhere in between the two components. Their complexities are now high whereas before in the unrotated model, they were low because they were both close to the first component axis (the horizontal axis). Whether to rotate or not is a decision made by the researcher. In the social sciences, we tend to rotate the components, though in the social sciences, we should not usually use PCA because we tend to think more about latent constructs, which are best estimated using EFA.</p>
</div>
</div>
<div id="example-two" class="section level2 hasAnchor" number="7.2">
<h2><span class="header-section-number">7.2</span> Example Two<a href="week-06-principal-components-analysis.html#example-two" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Everitt and Hothorn use a data set called <code>headsize</code> which I copied and pasted and saved as a csv file. Each case in this data set is a family and the two observed variables being analyzed are <code>head1</code> and <code>head2</code> for the head size of the first and second born male child in the family.</p>
<p>This provides an example of how we can perform PCA on a data set instead of the correlation matrix. It also allows for us to see how we can obtain a component score for each observation. There are several ways to calculate component scores (unfortunately).</p>
<div class="sourceCode" id="cb365"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb365-1"><a href="week-06-principal-components-analysis.html#cb365-1" tabindex="-1"></a>headsize <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;headsize.csv&quot;</span>)</span>
<span id="cb365-2"><a href="week-06-principal-components-analysis.html#cb365-2" tabindex="-1"></a>head_dat <span class="ot">&lt;-</span> headsize <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">select</span>(head1, head2)</span></code></pre></div>
<p>Let’s try the <code>prcomp()</code> function for the PCA. This is similar to the <code>princomp()</code> function, which they used in the chapter with these data, but it’s easier to use its output.</p>
<div id="pca-on-unstandardized-scores" class="section level3 hasAnchor" number="7.2.1">
<h3><span class="header-section-number">7.2.1</span> PCA on Unstandardized Scores<a href="week-06-principal-components-analysis.html#pca-on-unstandardized-scores" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Here’s the PCA Everitt and Hothorn conducted, which was on the raw data and in the original scale. Normally, we do <strong>not</strong> perform PCA on the unstandardized scale of the data because, as we saw above, variables with larger scales are given more weight.<a href="#fn34" class="footnote-ref" id="fnref34"><sup>34</sup></a></p>
<div class="sourceCode" id="cb366"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb366-1"><a href="week-06-principal-components-analysis.html#cb366-1" tabindex="-1"></a>head_pca <span class="ot">&lt;-</span> <span class="fu">prcomp</span>(<span class="sc">~</span> head1 <span class="sc">+</span> head2, </span>
<span id="cb366-2"><a href="week-06-principal-components-analysis.html#cb366-2" tabindex="-1"></a>                 <span class="at">data  =</span> head_dat,</span>
<span id="cb366-3"><a href="week-06-principal-components-analysis.html#cb366-3" tabindex="-1"></a>                 <span class="at">retx  =</span> <span class="cn">FALSE</span>, <span class="co"># Not rotating</span></span>
<span id="cb366-4"><a href="week-06-principal-components-analysis.html#cb366-4" tabindex="-1"></a>                 <span class="at">scale =</span> <span class="cn">FALSE</span>) <span class="co"># Using raw data scores instead of standardized scores.</span></span>
<span id="cb366-5"><a href="week-06-principal-components-analysis.html#cb366-5" tabindex="-1"></a>head_pca</span></code></pre></div>
<pre><code>## Standard deviations (1, .., p=2):
## [1] 12.952459  5.322951
## 
## Rotation (n x k) = (2 x 2):
##              PC1        PC2
## head1 -0.6929858  0.7209512
## head2 -0.7209512 -0.6929858</code></pre>
<div class="sourceCode" id="cb368"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb368-1"><a href="week-06-principal-components-analysis.html#cb368-1" tabindex="-1"></a><span class="fu">summary</span>(head_pca)</span></code></pre></div>
<pre><code>## Importance of components:
##                            PC1    PC2
## Standard deviation     12.9525 5.3230
## Proportion of Variance  0.8555 0.1445
## Cumulative Proportion   0.8555 1.0000</code></pre>
<p>The eigenvalues of the PCA on the non-standardized scores are <span class="math inline">\(12.9525^2\)</span> and <span class="math inline">\(5.3230^2\)</span>, or <span class="math inline">\(167.767\)</span> and <span class="math inline">\(28.334\)</span> respectively.</p>
<div id="hand-calculating-component-scores-from-this-pca" class="section level4 hasAnchor" number="7.2.1.1">
<h4><span class="header-section-number">7.2.1.1</span> Hand calculating component scores from this PCA<a href="week-06-principal-components-analysis.html#hand-calculating-component-scores-from-this-pca" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Following the explanation in Everitt and Horthorn, we can calculate each family’s score on each of the two components after centering their observed scores on the variables. For example, each family’s observed center score on <code>head1</code> is their deviation score, <span class="math inline">\(x_{i1} - \bar{x}_1\)</span>; on <code>head1</code>, it is <span class="math inline">\(x_{i2} - \bar{x}_2\)</span>.</p>
<p><span class="math display">\[ C_{i1} = 0.693(x_{i1} - \bar{x}_1) + .721(x_{i2} - \bar{x}_2) \]</span>
<span class="math display">\[ C_{i2} = .721(x_{i1} - \bar{x}_1) - .693(x_{i2} - \bar{x}_2) \]</span></p>
<p>We can do this in tidyverse:</p>
<div class="sourceCode" id="cb370"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb370-1"><a href="week-06-principal-components-analysis.html#cb370-1" tabindex="-1"></a>head_dat <span class="ot">&lt;-</span> head_dat <span class="sc">%&gt;%</span></span>
<span id="cb370-2"><a href="week-06-principal-components-analysis.html#cb370-2" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">C1.cent =</span> <span class="fl">0.693</span> <span class="sc">*</span>(head1 <span class="sc">-</span> <span class="fu">mean</span>(head1)) <span class="sc">+</span> .<span class="dv">721</span><span class="sc">*</span>(head2 <span class="sc">-</span> <span class="fu">mean</span>(head2)),</span>
<span id="cb370-3"><a href="week-06-principal-components-analysis.html#cb370-3" tabindex="-1"></a>         <span class="at">C2.cent =</span> <span class="fl">0.721</span> <span class="sc">*</span>(head1 <span class="sc">-</span> <span class="fu">mean</span>(head1)) <span class="sc">-</span> .<span class="dv">693</span><span class="sc">*</span>(head2 <span class="sc">-</span> <span class="fu">mean</span>(head2)) )</span>
<span id="cb370-4"><a href="week-06-principal-components-analysis.html#cb370-4" tabindex="-1"></a><span class="fu">head</span>(head_dat) <span class="co"># PCA can be very ... heady `groan() = TRUE`.</span></span></code></pre></div>
<pre><code>##   head1 head2  C1.cent C2.cent
## 1   191   179   0.1694   7.161
## 2   195   201  18.8034  -5.201
## 3   181   185  -2.4346  -4.207
## 4   183   188   1.1144  -4.844
## 5   176   171 -15.9936   1.890
## 6   208   192  21.3234  10.409</code></pre>
<p>A more reproducible way to do this same calculation is with the matrix of eigenvectors and the raw scores. Also, whereas above, we calculated the centered scores by hand, we can use the <code>scale()</code> function in Base R to get centered scores, with the arguments <code>center = TRUE</code> and <code>scale = FALSE</code>. (If we set <code>scale = TRUE</code>, we get standardized scores.) We also need to save the eigenvectors as a matrix using <code>A &lt;- cbind(head_pca$rotation)</code>. Then we can matrix multiply the matrix of centered scores by the matrix of eigenvectors, <span class="math inline">\(\mathbf{A}\)</span></p>
<div class="sourceCode" id="cb372"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb372-1"><a href="week-06-principal-components-analysis.html#cb372-1" tabindex="-1"></a>head_centrd <span class="ot">&lt;-</span> <span class="fu">cbind</span>( <span class="fu">scale</span>(head_dat[ , <span class="fu">c</span>(<span class="st">&quot;head1&quot;</span>, <span class="st">&quot;head2&quot;</span>)], </span>
<span id="cb372-2"><a href="week-06-principal-components-analysis.html#cb372-2" tabindex="-1"></a>                            <span class="at">center =</span> <span class="cn">TRUE</span>, </span>
<span id="cb372-3"><a href="week-06-principal-components-analysis.html#cb372-3" tabindex="-1"></a>                            <span class="at">scale  =</span> <span class="cn">FALSE</span>) )</span>
<span id="cb372-4"><a href="week-06-principal-components-analysis.html#cb372-4" tabindex="-1"></a>A <span class="ot">&lt;-</span> <span class="fu">cbind</span>(head_pca<span class="sc">$</span>rotation)</span>
<span id="cb372-5"><a href="week-06-principal-components-analysis.html#cb372-5" tabindex="-1"></a>head_dat[, <span class="fu">c</span>(<span class="st">&quot;C1.cent&quot;</span>, <span class="st">&quot;C2.cent&quot;</span>)] <span class="ot">&lt;-</span> head_centrd <span class="sc">%*%</span> A </span>
<span id="cb372-6"><a href="week-06-principal-components-analysis.html#cb372-6" tabindex="-1"></a><span class="fu">head</span>(head_dat)</span></code></pre></div>
<pre><code>##   head1 head2     C1.cent   C2.cent
## 1   191   179  -0.1695614  7.160674
## 2   195   201 -18.8024312 -5.201210
## 3   181   185   2.4345897 -4.206753
## 4   183   188  -1.1142355 -4.843808
## 5   176   171  15.9928357  1.890292
## 6   208   192 -21.3226862 10.408028</code></pre>
<p>We learned that the eigenvalues are the variances of the composite variables. Let’s look into this.</p>
<div class="sourceCode" id="cb374"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb374-1"><a href="week-06-principal-components-analysis.html#cb374-1" tabindex="-1"></a>head_dat <span class="sc">%&gt;%</span></span>
<span id="cb374-2"><a href="week-06-principal-components-analysis.html#cb374-2" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="at">varC1 =</span> <span class="fu">var</span>(C1.cent),</span>
<span id="cb374-3"><a href="week-06-principal-components-analysis.html#cb374-3" tabindex="-1"></a>            <span class="at">varC2 =</span> <span class="fu">var</span>(C2.cent))</span></code></pre></div>
<pre><code>##      varC1    varC2
## 1 167.7662 28.33381</code></pre>
<p>These are the same as the square of the standard deviations reported in the output, <span class="math inline">\(12.9525^2\)</span> and <span class="math inline">\(5.3230^2\)</span>, resulting in the two eigenvalues, <span class="math inline">\(167.767\)</span> and <span class="math inline">\(28.334\)</span>.</p>
</div>
</div>
<div id="pca-on-the-standardized-scores" class="section level3 hasAnchor" number="7.2.2">
<h3><span class="header-section-number">7.2.2</span> PCA on the Standardized Scores<a href="week-06-principal-components-analysis.html#pca-on-the-standardized-scores" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We almost always conduct a PCA on the standardized observed data rather than on the unstandardized data because it ensures that some variables, with excessive variance, do not take up all of the variance in the PCA solution.</p>
<p>Let’s take a look at the standardized observed scores.</p>
<div class="sourceCode" id="cb376"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb376-1"><a href="week-06-principal-components-analysis.html#cb376-1" tabindex="-1"></a>stdz_scores <span class="ot">&lt;-</span> <span class="fu">scale</span>(head_dat[ ,<span class="fu">c</span>(<span class="st">&quot;head1&quot;</span>, <span class="st">&quot;head2&quot;</span>)], <span class="at">scale =</span> <span class="cn">TRUE</span>)</span>
<span id="cb376-2"><a href="week-06-principal-components-analysis.html#cb376-2" tabindex="-1"></a><span class="fu">head</span>(stdz_scores)</span></code></pre></div>
<pre><code>##           head1      head2
## [1,]  0.5408822 -0.4820596
## [2,]  0.9506414  1.7091204
## [3,] -0.4835159  0.1155349
## [4,] -0.2786363  0.4143322
## [5,] -0.9957149 -1.2788523
## [6,]  2.2823588  0.8127286</code></pre>
<p>These scores, by definition, have a mean of zero and a standard deviation of one.</p>
<p>We can also recognize that (a) the correlation and covariance matrices are the same when we have standardized data, and (b) the <code>eigen()</code> function on this covariance matrix returns the same eigenvalues and eigenvectors as we will observe in our output below. This simply confirms that a PCA on the standardized observed scores will yield the same PCA results as a PCA on the correlation matrix.</p>
<div class="sourceCode" id="cb378"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb378-1"><a href="week-06-principal-components-analysis.html#cb378-1" tabindex="-1"></a><span class="fu">cor</span>(stdz_scores) </span></code></pre></div>
<pre><code>##           head1     head2
## head1 1.0000000 0.7107518
## head2 0.7107518 1.0000000</code></pre>
<div class="sourceCode" id="cb380"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb380-1"><a href="week-06-principal-components-analysis.html#cb380-1" tabindex="-1"></a><span class="fu">cov</span>(stdz_scores)</span></code></pre></div>
<pre><code>##           head1     head2
## head1 1.0000000 0.7107518
## head2 0.7107518 1.0000000</code></pre>
<div class="sourceCode" id="cb382"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb382-1"><a href="week-06-principal-components-analysis.html#cb382-1" tabindex="-1"></a><span class="fu">eigen</span>(<span class="fu">cov</span>(stdz_scores))</span></code></pre></div>
<pre><code>## eigen() decomposition
## $values
## [1] 1.7107518 0.2892482
## 
## $vectors
##           [,1]       [,2]
## [1,] 0.7071068 -0.7071068
## [2,] 0.7071068  0.7071068</code></pre>
<p>Let’s now go back to our <code>prcomp()</code> function and now use <code>scale = TRUE</code> in the function, which does the scaling for us. We can compare this output with the output above with the PCA on the standardized scores.</p>
<div class="sourceCode" id="cb384"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb384-1"><a href="week-06-principal-components-analysis.html#cb384-1" tabindex="-1"></a>pc_out <span class="ot">&lt;-</span> <span class="fu">prcomp</span>(<span class="sc">~</span> head1 <span class="sc">+</span> head2, </span>
<span id="cb384-2"><a href="week-06-principal-components-analysis.html#cb384-2" tabindex="-1"></a>                 <span class="at">data  =</span> head_dat,</span>
<span id="cb384-3"><a href="week-06-principal-components-analysis.html#cb384-3" tabindex="-1"></a>                 <span class="at">retx  =</span> <span class="cn">FALSE</span>, <span class="co"># Not rotating</span></span>
<span id="cb384-4"><a href="week-06-principal-components-analysis.html#cb384-4" tabindex="-1"></a>                 <span class="at">scale =</span> <span class="cn">TRUE</span>)  <span class="co"># Standardize scores, then perform PCA.</span></span>
<span id="cb384-5"><a href="week-06-principal-components-analysis.html#cb384-5" tabindex="-1"></a>pc_out</span></code></pre></div>
<pre><code>## Standard deviations (1, .., p=2):
## [1] 1.307957 0.537818
## 
## Rotation (n x k) = (2 x 2):
##              PC1        PC2
## head1 -0.7071068  0.7071068
## head2 -0.7071068 -0.7071068</code></pre>
<div class="sourceCode" id="cb386"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb386-1"><a href="week-06-principal-components-analysis.html#cb386-1" tabindex="-1"></a><span class="fu">summary</span>(pc_out)</span></code></pre></div>
<pre><code>## Importance of components:
##                           PC1    PC2
## Standard deviation     1.3080 0.5378
## Proportion of Variance 0.8554 0.1446
## Cumulative Proportion  0.8554 1.0000</code></pre>
<p>The proportion of variance of each component is the same as it was with the previous model with unstandardized data. As we expect, however, the standard deviations of the components, which is the square root of the eigenvalues, differ from those of the unstandardized model. The eigenvalues of the PCA on the standardized scores are <span class="math inline">\(1.3080^2\)</span> and <span class="math inline">\(0.5378^2\)</span>, or <span class="math inline">\(1.711\)</span> and <span class="math inline">\(0.289\)</span>. These are the same as those we saw with the <code>eigen()</code> function on the covariance matrix.</p>
<div id="calculating-component-scores" class="section level4 hasAnchor" number="7.2.2.1">
<h4><span class="header-section-number">7.2.2.1</span> Calculating component scores<a href="week-06-principal-components-analysis.html#calculating-component-scores" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>First let’s save and look at the unscaled eigenvectors from the standardized data.</p>
<div class="sourceCode" id="cb388"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb388-1"><a href="week-06-principal-components-analysis.html#cb388-1" tabindex="-1"></a>A <span class="ot">&lt;-</span> <span class="fu">cbind</span>(pc_out<span class="sc">$</span>rotation)</span>
<span id="cb388-2"><a href="week-06-principal-components-analysis.html#cb388-2" tabindex="-1"></a>A <span class="co"># The two eigenvectors</span></span></code></pre></div>
<pre><code>##              PC1        PC2
## head1 -0.7071068  0.7071068
## head2 -0.7071068 -0.7071068</code></pre>
<p>Here are the eigenvalues:</p>
<div class="sourceCode" id="cb390"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb390-1"><a href="week-06-principal-components-analysis.html#cb390-1" tabindex="-1"></a>pc_out<span class="sc">$</span>sdev<span class="sc">^</span><span class="dv">2</span></span></code></pre></div>
<pre><code>## [1] 1.7107518 0.2892482</code></pre>
<p>Here are the component scores from this model:</p>
<div class="sourceCode" id="cb392"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb392-1"><a href="week-06-principal-components-analysis.html#cb392-1" tabindex="-1"></a>head_dat[, <span class="fu">c</span>(<span class="st">&quot;C1.stdz&quot;</span>, <span class="st">&quot;C2.stdz&quot;</span>)] <span class="ot">&lt;-</span> stdz_scores <span class="sc">%*%</span> A </span>
<span id="cb392-2"><a href="week-06-principal-components-analysis.html#cb392-2" tabindex="-1"></a><span class="fu">head</span>( head_dat )</span></code></pre></div>
<pre><code>##   head1 head2     C1.cent   C2.cent     C1.stdz    C2.stdz
## 1   191   179  -0.1695614  7.160674 -0.04159384  0.7233291
## 2   195   201 -18.8024312 -5.201210 -1.88073559 -0.5363257
## 3   181   185   2.4345897 -4.206753  0.26020181 -0.4235929
## 4   183   188  -1.1142355 -4.843808 -0.09595153 -0.4900027
## 5   176   171  15.9928357  1.890292  1.60836191  0.2002084
## 6   208   192 -21.3226862 10.408028 -2.18855730  1.0391855</code></pre>
<p>Again, we see that the sample variances of the component scores are the same as the eigenvalues.</p>
<div class="sourceCode" id="cb394"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb394-1"><a href="week-06-principal-components-analysis.html#cb394-1" tabindex="-1"></a>head_dat <span class="sc">%&gt;%</span></span>
<span id="cb394-2"><a href="week-06-principal-components-analysis.html#cb394-2" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="at">varC1 =</span> <span class="fu">var</span>(C1.stdz),</span>
<span id="cb394-3"><a href="week-06-principal-components-analysis.html#cb394-3" tabindex="-1"></a>            <span class="at">varC2 =</span> <span class="fu">var</span>(C2.stdz))</span></code></pre></div>
<pre><code>##      varC1     varC2
## 1 1.710752 0.2892482</code></pre>
</div>
<div id="using-the-rescaled-eigenvectors-to-interpret-the-components" class="section level4 hasAnchor" number="7.2.2.2">
<h4><span class="header-section-number">7.2.2.2</span> Using the rescaled eigenvectors to interpret the components<a href="week-06-principal-components-analysis.html#using-the-rescaled-eigenvectors-to-interpret-the-components" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>To get the rescaled eigevectors (or “loadings”), we first get the square root of the eigenvalues. Here are the eigenvalues, again, but this time we’ll place them in a diagonal matrix, <span class="math inline">\(\mathbf{L}\)</span>, which has zeros on the off diagonal because we can then use this for matrix multiplication.</p>
<div class="sourceCode" id="cb396"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb396-1"><a href="week-06-principal-components-analysis.html#cb396-1" tabindex="-1"></a>L <span class="ot">&lt;-</span> <span class="fu">diag</span>(pc_out<span class="sc">$</span>sdev<span class="sc">^</span><span class="dv">2</span>) <span class="co">#Eigenvalues</span></span>
<span id="cb396-2"><a href="week-06-principal-components-analysis.html#cb396-2" tabindex="-1"></a>L <span class="co"># We&#39;ll use &quot;L&quot; for capital Lambda. This is the eigenvalue matrix</span></span></code></pre></div>
<pre><code>##          [,1]      [,2]
## [1,] 1.710752 0.0000000
## [2,] 0.000000 0.2892482</code></pre>
<p>Here are the scaled eigenvectors, with elements interpreted as correlations with component.<a href="#fn35" class="footnote-ref" id="fnref35"><sup>35</sup></a></p>
<div class="sourceCode" id="cb398"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb398-1"><a href="week-06-principal-components-analysis.html#cb398-1" tabindex="-1"></a>A_star <span class="ot">&lt;-</span> A <span class="sc">%*%</span> (<span class="fu">sqrt</span>(L)) </span>
<span id="cb398-2"><a href="week-06-principal-components-analysis.html#cb398-2" tabindex="-1"></a><span class="fu">round</span>(A_star, <span class="dv">3</span>) </span></code></pre></div>
<pre><code>##         [,1]  [,2]
## head1 -0.925  0.38
## head2 -0.925 -0.38</code></pre>
<p>Component 1 has to do with head size. Component 2 is something else, with two opposing forces, maybe something to do with head shape. That’s about as much interpretation we can do with this very simple data set.</p>
</div>
</div>
<div id="let-the-psych-package-do-the-work" class="section level3 hasAnchor" number="7.2.3">
<h3><span class="header-section-number">7.2.3</span> Let the Psych Package Do the Work<a href="week-06-principal-components-analysis.html#let-the-psych-package-do-the-work" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Having done all that work, let’s use the psych package and let it report the rescaled eigenvalues (loadings). This, by default, uses the correlation matrix rather than the covariance matrix.</p>
<p>We can also add a <code>scores = TRUE</code> argument to obtain the component scores; these component scores have been rescaled to be standardized. There seems to be a lot of standardization going on here—we’ve standardized the observed variables and now we’re standardizing the component scores, which is really for aiding interpretation because component scores themselves have no intrinsic scale; by standardizing them, we can at least interpret the component scores as standard-deviation units away from the mean.</p>
<div class="sourceCode" id="cb400"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb400-1"><a href="week-06-principal-components-analysis.html#cb400-1" tabindex="-1"></a><span class="fu">library</span>(psych)</span>
<span id="cb400-2"><a href="week-06-principal-components-analysis.html#cb400-2" tabindex="-1"></a>pc_psy <span class="ot">&lt;-</span> psych<span class="sc">::</span><span class="fu">pca</span>(head_dat[, <span class="fu">c</span>(<span class="st">&quot;head1&quot;</span>, <span class="st">&quot;head2&quot;</span>)], </span>
<span id="cb400-3"><a href="week-06-principal-components-analysis.html#cb400-3" tabindex="-1"></a>              <span class="at">nfactors =</span> <span class="dv">2</span>, </span>
<span id="cb400-4"><a href="week-06-principal-components-analysis.html#cb400-4" tabindex="-1"></a>              <span class="at">rotate =</span> <span class="st">&#39;none&#39;</span>,</span>
<span id="cb400-5"><a href="week-06-principal-components-analysis.html#cb400-5" tabindex="-1"></a>              <span class="at">scores =</span> <span class="cn">TRUE</span>)</span>
<span id="cb400-6"><a href="week-06-principal-components-analysis.html#cb400-6" tabindex="-1"></a><span class="fu">print</span>(pc_psy, <span class="at">digits =</span> <span class="dv">3</span>)</span></code></pre></div>
<pre><code>## Principal Components Analysis
## Call: principal(r = r, nfactors = nfactors, residuals = residuals, 
##     rotate = rotate, n.obs = n.obs, covar = covar, scores = scores, 
##     missing = missing, impute = impute, oblique.scores = oblique.scores, 
##     method = method, use = use, cor = cor, correct = 0.5, weight = NULL)
## Standardized loadings (pattern matrix) based upon correlation matrix
##         PC1   PC2 h2 u2  com
## head1 0.925 -0.38  1  0 1.33
## head2 0.925  0.38  1  0 1.33
## 
##                         PC1   PC2
## SS loadings           1.711 0.289
## Proportion Var        0.855 0.145
## Cumulative Var        0.855 1.000
## Proportion Explained  0.855 0.145
## Cumulative Proportion 0.855 1.000
## 
## Mean item complexity =  1.3
## Test of the hypothesis that 2 components are sufficient.
## 
## The root mean square of the residuals (RMSR) is  0 
##  with the empirical chi square  0  with prob &lt;  NA 
## 
## Fit based upon off diagonal values = 1</code></pre>
<p>We see the loadings are the rescaled eigenvectors, which are what we want. They can be interpreted as the correlation of the observed variable with the component.</p>
<div id="view-the-component-scores" class="section level4 hasAnchor" number="7.2.3.1">
<h4><span class="header-section-number">7.2.3.1</span> View the component scores<a href="week-06-principal-components-analysis.html#view-the-component-scores" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We can obtain the component scores for each case from the outputted object. They are a matrix of however many components we have, which we can save to our data:</p>
<div class="sourceCode" id="cb402"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb402-1"><a href="week-06-principal-components-analysis.html#cb402-1" tabindex="-1"></a>head_dat[, <span class="fu">c</span>(<span class="st">&quot;C1.zscore&quot;</span>, <span class="st">&quot;C2.zscore&quot;</span>) ]  <span class="ot">&lt;-</span> pc_psy<span class="sc">$</span>scores</span></code></pre></div>
<p>We can verify that these are indeed standardized component scores.</p>
<div class="sourceCode" id="cb403"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb403-1"><a href="week-06-principal-components-analysis.html#cb403-1" tabindex="-1"></a>head_dat <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">select</span>(C1.zscore, C2.zscore) <span class="sc">%&gt;%</span> </span>
<span id="cb403-2"><a href="week-06-principal-components-analysis.html#cb403-2" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="at">MC1 =</span> <span class="fu">round</span>( <span class="fu">mean</span>(C1.zscore), <span class="dv">2</span>),</span>
<span id="cb403-3"><a href="week-06-principal-components-analysis.html#cb403-3" tabindex="-1"></a>            <span class="at">MC2 =</span> <span class="fu">round</span>( <span class="fu">mean</span>(C2.zscore), <span class="dv">2</span>),</span>
<span id="cb403-4"><a href="week-06-principal-components-analysis.html#cb403-4" tabindex="-1"></a>            <span class="at">varC1 =</span> <span class="fu">var</span>(C1.zscore),</span>
<span id="cb403-5"><a href="week-06-principal-components-analysis.html#cb403-5" tabindex="-1"></a>            <span class="at">varC2 =</span> <span class="fu">var</span>(C2.zscore))</span></code></pre></div>
<pre><code>##   MC1 MC2 varC1 varC2
## 1   0   0     1     1</code></pre>
<p>We also see that these scores correlate perfectly with the other component scores we calculated earlier from the coefficients. The difference in sign is because the <code>prcomp()</code> function arbitrarily assigns the sign. We also see that the two component scores, no matter how we calculate them, are orthogonal to each other.</p>
<div class="sourceCode" id="cb405"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb405-1"><a href="week-06-principal-components-analysis.html#cb405-1" tabindex="-1"></a><span class="fu">round</span>( <span class="fu">cor</span>(head_dat[, <span class="fu">c</span>(<span class="st">&quot;C1.cent&quot;</span>, <span class="st">&quot;C2.cent&quot;</span>, <span class="st">&quot;C1.stdz&quot;</span>, <span class="st">&quot;C2.stdz&quot;</span>, </span>
<span id="cb405-2"><a href="week-06-principal-components-analysis.html#cb405-2" tabindex="-1"></a><span class="st">&quot;C1.zscore&quot;</span>, <span class="st">&quot;C2.zscore&quot;</span>) ] ) , <span class="dv">2</span>)</span></code></pre></div>
<pre><code>##           C1.cent C2.cent C1.stdz C2.stdz C1.zscore C2.zscore
## C1.cent      1.00    0.00    1.00    0.01     -1.00     -0.01
## C2.cent      0.00    1.00   -0.01    1.00      0.01     -1.00
## C1.stdz      1.00   -0.01    1.00    0.00     -1.00      0.00
## C2.stdz      0.01    1.00    0.00    1.00      0.00     -1.00
## C1.zscore   -1.00    0.01   -1.00    0.00      1.00      0.00
## C2.zscore   -0.01   -1.00    0.00   -1.00      0.00      1.00</code></pre>
</div>
</div>
</div>
<div id="heptathlon-example" class="section level2 hasAnchor" number="7.3">
<h2><span class="header-section-number">7.3</span> Heptathlon example<a href="week-06-principal-components-analysis.html#heptathlon-example" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>These are Olympic heptathlon data. Let’s use this as an example for a PCA. The steps in a PCA usually include the following, depending on our purpose:</p>
<ol style="list-style-type: decimal">
<li>Perform data cleaning, to ensure the variables are in the logical direction and so forth</li>
<li>Assessing the assumptions of linearity and multivariate normality and absence of outliers</li>
<li>Fit the PCA model</li>
<li>Determine the number of components to retain</li>
<li>Refit the PCA model, using rotation if that is acceptable in your field</li>
<li>Interpret the components</li>
<li>Save the component scores if the intent is to use them for future analyses.</li>
</ol>
<div id="data-and-data-cleaning" class="section level3 hasAnchor" number="7.3.1">
<h3><span class="header-section-number">7.3.1</span> Data and data cleaning<a href="week-06-principal-components-analysis.html#data-and-data-cleaning" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb407"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb407-1"><a href="week-06-principal-components-analysis.html#cb407-1" tabindex="-1"></a>hepdat <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;heptathlon.csv&quot;</span>)</span>
<span id="cb407-2"><a href="week-06-principal-components-analysis.html#cb407-2" tabindex="-1"></a><span class="fu">str</span>(hepdat)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    25 obs. of  9 variables:
##  $ athlete : chr  &quot;Joyner-Kersee (USA)&quot; &quot;John (GDR)&quot; &quot;Behmer (GDR)&quot; &quot;Sablovskaite (URS)&quot; ...
##  $ hurdles : num  12.7 12.8 13.2 13.6 13.5 ...
##  $ highjump: num  1.86 1.8 1.83 1.8 1.74 1.83 1.8 1.8 1.83 1.77 ...
##  $ shot    : num  15.8 16.2 14.2 15.2 14.8 ...
##  $ run200m : num  22.6 23.6 23.1 23.9 23.9 ...
##  $ longjump: num  7.27 6.71 6.68 6.25 6.32 6.33 6.37 6.47 6.11 6.28 ...
##  $ javelin : num  45.7 42.6 44.5 42.8 47.5 ...
##  $ run800m : num  129 126 124 132 128 ...
##  $ score   : int  7291 6897 6858 6540 6540 6411 6351 6297 6252 6252 ...</code></pre>
<p>Everitt and Hothorn (2011) reversed the scores of the events in which a lower score is a better score to make the direction of the scores consistent in terms of athletic performance.</p>
<div class="sourceCode" id="cb409"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb409-1"><a href="week-06-principal-components-analysis.html#cb409-1" tabindex="-1"></a>dat <span class="ot">&lt;-</span> hepdat <span class="sc">%&gt;%</span> </span>
<span id="cb409-2"><a href="week-06-principal-components-analysis.html#cb409-2" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">hurdles =</span> <span class="fu">max</span>(hurdles) <span class="sc">-</span> hurdles,</span>
<span id="cb409-3"><a href="week-06-principal-components-analysis.html#cb409-3" tabindex="-1"></a>         <span class="at">run200m =</span> <span class="fu">max</span>(run200m) <span class="sc">-</span> run200m,</span>
<span id="cb409-4"><a href="week-06-principal-components-analysis.html#cb409-4" tabindex="-1"></a>         <span class="at">run800m =</span> <span class="fu">max</span>(run800m) <span class="sc">-</span> run800m)</span></code></pre></div>
<p>I’m also going to save the variable names in case I need them because I’m too lazy to type them out.</p>
<div class="sourceCode" id="cb410"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb410-1"><a href="week-06-principal-components-analysis.html#cb410-1" tabindex="-1"></a>varbnames <span class="ot">&lt;-</span> <span class="fu">names</span>(dat)[<span class="dv">2</span><span class="sc">:</span><span class="dv">8</span>]</span></code></pre></div>
</div>
<div id="examine-assumption-of-linearity" class="section level3 hasAnchor" number="7.3.2">
<h3><span class="header-section-number">7.3.2</span> Examine assumption of linearity<a href="week-06-principal-components-analysis.html#examine-assumption-of-linearity" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We can generate a scatterplot matrix using this simple code on the data with our variables.</p>
<div class="sourceCode" id="cb411"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb411-1"><a href="week-06-principal-components-analysis.html#cb411-1" tabindex="-1"></a><span class="fu">plot</span>(dat[, varbnames])</span></code></pre></div>
<p><img src="multivariate_stats_files/figure-html/unnamed-chunk-204-1.png" width="672" /></p>
<p>The variables do not seem to show non-linear relationships. However, in many cells, except for in the javelin and shot-put events, there seems to be a single case that is an outlier. This might be the same athlete across events. I would guess that this athlete had a leg injury that prevented her from performing well in the events that required running.</p>
</div>
<div id="multivariate-normality-and-outliers" class="section level3 hasAnchor" number="7.3.3">
<h3><span class="header-section-number">7.3.3</span> Multivariate normality and outliers<a href="week-06-principal-components-analysis.html#multivariate-normality-and-outliers" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s examine the Mahalanobis distances to see if we can identify this and any other outlying cases:</p>
<div class="sourceCode" id="cb412"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb412-1"><a href="week-06-principal-components-analysis.html#cb412-1" tabindex="-1"></a>varbs <span class="ot">&lt;-</span> <span class="fu">cbind</span>(dat[, varbnames])</span>
<span id="cb412-2"><a href="week-06-principal-components-analysis.html#cb412-2" tabindex="-1"></a>distances <span class="ot">&lt;-</span> <span class="fu">mahalanobis</span>(varbs, </span>
<span id="cb412-3"><a href="week-06-principal-components-analysis.html#cb412-3" tabindex="-1"></a>                         <span class="at">center =</span> <span class="fu">colMeans</span>(varbs), </span>
<span id="cb412-4"><a href="week-06-principal-components-analysis.html#cb412-4" tabindex="-1"></a>                         <span class="at">cov =</span> <span class="fu">cov</span>(varbs))</span>
<span id="cb412-5"><a href="week-06-principal-components-analysis.html#cb412-5" tabindex="-1"></a>dat<span class="sc">$</span>distances <span class="ot">&lt;-</span> distances</span>
<span id="cb412-6"><a href="week-06-principal-components-analysis.html#cb412-6" tabindex="-1"></a></span>
<span id="cb412-7"><a href="week-06-principal-components-analysis.html#cb412-7" tabindex="-1"></a><span class="co"># Let&#39;s check for those who exceed the p &lt; .001 criterion:</span></span>
<span id="cb412-8"><a href="week-06-principal-components-analysis.html#cb412-8" tabindex="-1"></a>dat<span class="sc">$</span>p <span class="ot">&lt;-</span> <span class="fu">pchisq</span>(distances,</span>
<span id="cb412-9"><a href="week-06-principal-components-analysis.html#cb412-9" tabindex="-1"></a>                <span class="at">df =</span> (<span class="fu">ncol</span>(varbs)<span class="sc">-</span><span class="dv">1</span>), <span class="co"># df is number of variables - 1</span></span>
<span id="cb412-10"><a href="week-06-principal-components-analysis.html#cb412-10" tabindex="-1"></a>                <span class="at">lower.tail =</span> <span class="cn">FALSE</span>)</span>
<span id="cb412-11"><a href="week-06-principal-components-analysis.html#cb412-11" tabindex="-1"></a></span>
<span id="cb412-12"><a href="week-06-principal-components-analysis.html#cb412-12" tabindex="-1"></a>dat<span class="sc">$</span>outlier <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(dat<span class="sc">$</span>p <span class="sc">&lt;</span> .<span class="dv">001</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb412-13"><a href="week-06-principal-components-analysis.html#cb412-13" tabindex="-1"></a><span class="co"># Temporarily sort the data by p-values and outlier status and get first n rows.</span></span>
<span id="cb412-14"><a href="week-06-principal-components-analysis.html#cb412-14" tabindex="-1"></a>dat <span class="sc">%&gt;%</span> <span class="fu">arrange</span>(<span class="fu">desc</span>(outlier), p) <span class="sc">%&gt;%</span> <span class="fu">slice_head</span>(<span class="at">n =</span> <span class="dv">5</span>) </span></code></pre></div>
<pre><code>##               athlete hurdles highjump  shot run200m longjump javelin run800m score distances
## 1         Launa (PNG)    0.00     1.50 11.78    0.45     4.88   46.38    0.00  4566 18.828022
## 2 Joyner-Kersee (USA)    3.73     1.86 15.80    4.05     7.27   45.66   34.92  7291 10.153130
## 3        Yuping (CHN)    2.49     1.86 14.21    1.61     6.40   38.60   16.76  6087 10.101416
## 4         Hagger (GB)    2.95     1.80 12.75    1.14     6.34   35.76   24.95  5975  9.526282
## 5      Scheider (SWI)    2.57     1.86 11.58    1.74     6.05   47.50   28.50  6137  9.179544
##             p outlier
## 1 0.004464155       0
## 2 0.118349731       0
## 3 0.120445638       0
## 4 0.146071466       0
## 5 0.163729573       0</code></pre>
<div class="sourceCode" id="cb414"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb414-1"><a href="week-06-principal-components-analysis.html#cb414-1" tabindex="-1"></a><span class="fu">library</span>(car)</span>
<span id="cb414-2"><a href="week-06-principal-components-analysis.html#cb414-2" tabindex="-1"></a>car<span class="sc">::</span><span class="fu">qqPlot</span>(distances, </span>
<span id="cb414-3"><a href="week-06-principal-components-analysis.html#cb414-3" tabindex="-1"></a>            <span class="at">distribution =</span> <span class="st">&quot;chisq&quot;</span>, <span class="at">df =</span> <span class="fu">mean</span>(distances), </span>
<span id="cb414-4"><a href="week-06-principal-components-analysis.html#cb414-4" tabindex="-1"></a>            <span class="at">lwd =</span> <span class="dv">1</span>, </span>
<span id="cb414-5"><a href="week-06-principal-components-analysis.html#cb414-5" tabindex="-1"></a>            <span class="at">grid =</span> <span class="cn">FALSE</span>, </span>
<span id="cb414-6"><a href="week-06-principal-components-analysis.html#cb414-6" tabindex="-1"></a>            <span class="at">main =</span> <span class="st">&quot;Multi-normal Q-Q Plot on Residuals&quot;</span>, </span>
<span id="cb414-7"><a href="week-06-principal-components-analysis.html#cb414-7" tabindex="-1"></a>            <span class="at">xlab =</span> <span class="fu">expression</span>(chi<span class="sc">^</span><span class="dv">2</span> <span class="sc">*</span> <span class="st">&quot; quantiles&quot;</span>), </span>
<span id="cb414-8"><a href="week-06-principal-components-analysis.html#cb414-8" tabindex="-1"></a>            <span class="at">ylab =</span> <span class="fu">expression</span>(<span class="st">&quot;Mahalanobis distances &quot;</span><span class="sc">^</span><span class="dv">2</span>))</span></code></pre></div>
<p><img src="multivariate_stats_files/figure-html/unnamed-chunk-205-1.png" width="672" /></p>
<pre><code>## [1] 25  8</code></pre>
<p>We see that the 25th row in the data frame includes a case that is far above (or below) the rest. Let’s examine this row:</p>
<div class="sourceCode" id="cb416"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb416-1"><a href="week-06-principal-components-analysis.html#cb416-1" tabindex="-1"></a>dat[<span class="dv">25</span>, ]</span></code></pre></div>
<pre><code>##        athlete hurdles highjump  shot run200m longjump javelin run800m score distances           p
## 25 Launa (PNG)       0      1.5 11.78    0.45     4.88   46.38       0  4566  18.82802 0.004464155
##    outlier
## 25       0</code></pre>
<p>It appears that the athlete from Papua New Guinea was the outlier, though not enough to merit removal because she was within the error band and did not have a statistically significant Mahalanobis distance from the rest. However, let’s follow the text’s example and remove this observation from the data. We can compare the results with and without this athlete in a sensitivity analysis if we were doing a complete analysis.</p>
<div class="sourceCode" id="cb418"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb418-1"><a href="week-06-principal-components-analysis.html#cb418-1" tabindex="-1"></a>dat <span class="ot">&lt;-</span> dat[<span class="sc">!</span>dat<span class="sc">$</span>athlete <span class="sc">==</span> <span class="st">&quot;Launa (PNG)&quot;</span>, ]</span></code></pre></div>
<p>We should also perform a test of multivariate normality. With this small data set, it will likely not result in a statistically significant difference. Let’s get to the analysis and pretend we had done that part.</p>
</div>
<div id="re-examine-linearity-and-normality" class="section level3 hasAnchor" number="7.3.4">
<h3><span class="header-section-number">7.3.4</span> Re-examine linearity and normality<a href="week-06-principal-components-analysis.html#re-examine-linearity-and-normality" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb419"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb419-1"><a href="week-06-principal-components-analysis.html#cb419-1" tabindex="-1"></a><span class="fu">plot</span>(dat[, varbnames], <span class="at">main =</span> <span class="st">&quot;After removal of multivariate outlier&quot;</span>)</span></code></pre></div>
<p><img src="multivariate_stats_files/figure-html/unnamed-chunk-208-1.png" width="672" /></p>
<p>The scatterplots show that the removal of this outlier resulted in scatterplots with data that are dispersed across the ranges of the variables. Linearity seems to hold.</p>
<div class="sourceCode" id="cb420"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb420-1"><a href="week-06-principal-components-analysis.html#cb420-1" tabindex="-1"></a>varbs <span class="ot">&lt;-</span> <span class="fu">cbind</span>(dat[, varbnames])</span>
<span id="cb420-2"><a href="week-06-principal-components-analysis.html#cb420-2" tabindex="-1"></a>distances <span class="ot">&lt;-</span> <span class="fu">mahalanobis</span>(varbs, </span>
<span id="cb420-3"><a href="week-06-principal-components-analysis.html#cb420-3" tabindex="-1"></a>                         <span class="at">center =</span> <span class="fu">colMeans</span>(varbs), </span>
<span id="cb420-4"><a href="week-06-principal-components-analysis.html#cb420-4" tabindex="-1"></a>                         <span class="at">cov =</span> <span class="fu">cov</span>(varbs))</span>
<span id="cb420-5"><a href="week-06-principal-components-analysis.html#cb420-5" tabindex="-1"></a>dat<span class="sc">$</span>distances <span class="ot">&lt;-</span> distances</span>
<span id="cb420-6"><a href="week-06-principal-components-analysis.html#cb420-6" tabindex="-1"></a></span>
<span id="cb420-7"><a href="week-06-principal-components-analysis.html#cb420-7" tabindex="-1"></a><span class="fu">library</span>(car)</span>
<span id="cb420-8"><a href="week-06-principal-components-analysis.html#cb420-8" tabindex="-1"></a>car<span class="sc">::</span><span class="fu">qqPlot</span>(distances, </span>
<span id="cb420-9"><a href="week-06-principal-components-analysis.html#cb420-9" tabindex="-1"></a>            <span class="at">distribution =</span> <span class="st">&quot;chisq&quot;</span>, <span class="at">df =</span> <span class="fu">mean</span>(distances), </span>
<span id="cb420-10"><a href="week-06-principal-components-analysis.html#cb420-10" tabindex="-1"></a>            <span class="at">lwd =</span> <span class="dv">1</span>, </span>
<span id="cb420-11"><a href="week-06-principal-components-analysis.html#cb420-11" tabindex="-1"></a>            <span class="at">grid =</span> <span class="cn">FALSE</span>, </span>
<span id="cb420-12"><a href="week-06-principal-components-analysis.html#cb420-12" tabindex="-1"></a>            <span class="at">main =</span> <span class="st">&quot;After removal of multivariate outlier&quot;</span>, </span>
<span id="cb420-13"><a href="week-06-principal-components-analysis.html#cb420-13" tabindex="-1"></a>            <span class="at">xlab =</span> <span class="fu">expression</span>(chi<span class="sc">^</span><span class="dv">2</span> <span class="sc">*</span> <span class="st">&quot; quantiles&quot;</span>), </span>
<span id="cb420-14"><a href="week-06-principal-components-analysis.html#cb420-14" tabindex="-1"></a>            <span class="at">ylab =</span> <span class="fu">expression</span>(<span class="st">&quot;Mahalanobis distances &quot;</span><span class="sc">^</span><span class="dv">2</span>))</span></code></pre></div>
<p><img src="multivariate_stats_files/figure-html/unnamed-chunk-209-1.png" width="672" /></p>
<pre><code>## [1]  8 15</code></pre>
<p>The Q-Q plot of the multivariate distances suggested that the cases are arguably not violating the assumption of multivariate normality.</p>
</div>
<div id="fit-the-initial-pca" class="section level3 hasAnchor" number="7.3.5">
<h3><span class="header-section-number">7.3.5</span> Fit the initial PCA<a href="week-06-principal-components-analysis.html#fit-the-initial-pca" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb422"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb422-1"><a href="week-06-principal-components-analysis.html#cb422-1" tabindex="-1"></a>fit.pca <span class="ot">&lt;-</span> psych<span class="sc">::</span><span class="fu">pca</span>(dat[ ,varbnames], </span>
<span id="cb422-2"><a href="week-06-principal-components-analysis.html#cb422-2" tabindex="-1"></a>                      <span class="at">nfactors =</span> <span class="fu">length</span>(varbnames),</span>
<span id="cb422-3"><a href="week-06-principal-components-analysis.html#cb422-3" tabindex="-1"></a>                      <span class="at">rotate =</span> <span class="st">&quot;none&quot;</span> )</span>
<span id="cb422-4"><a href="week-06-principal-components-analysis.html#cb422-4" tabindex="-1"></a><span class="fu">print</span>( fit.pca, <span class="at">digits  =</span> <span class="dv">2</span>)</span></code></pre></div>
<pre><code>## Principal Components Analysis
## Call: principal(r = r, nfactors = nfactors, residuals = residuals, 
##     rotate = rotate, n.obs = n.obs, covar = covar, scores = scores, 
##     missing = missing, impute = impute, oblique.scores = oblique.scores, 
##     method = method, use = use, cor = cor, correct = 0.5, weight = NULL)
## Standardized loadings (pattern matrix) based upon correlation matrix
##           PC1   PC2   PC3   PC4   PC5   PC6   PC7 h2      u2 com
## hurdles  0.94 -0.05 -0.16 -0.03 -0.11 -0.29 -0.02  1 3.3e-16 1.3
## highjump 0.65  0.62 -0.19  0.38  0.04  0.03  0.09  1 1.7e-15 2.9
## shot     0.84  0.02 -0.14 -0.37  0.37  0.03  0.06  1 1.2e-15 1.9
## run200m  0.89 -0.18  0.12 -0.16 -0.34  0.11  0.12  1 1.6e-15 1.6
## longjump 0.94  0.02 -0.25  0.01 -0.07  0.13 -0.20  1 1.2e-15 1.3
## javelin  0.50  0.31  0.80 -0.04  0.04 -0.02 -0.06  1 5.6e-16 2.1
## run800m  0.63 -0.62  0.18  0.39  0.17  0.02  0.02  1 4.4e-16 3.0
## 
##                        PC1  PC2  PC3  PC4  PC5  PC6  PC7
## SS loadings           4.32 0.90 0.83 0.47 0.30 0.11 0.07
## Proportion Var        0.62 0.13 0.12 0.07 0.04 0.02 0.01
## Cumulative Var        0.62 0.75 0.86 0.93 0.97 0.99 1.00
## Proportion Explained  0.62 0.13 0.12 0.07 0.04 0.02 0.01
## Cumulative Proportion 0.62 0.75 0.86 0.93 0.97 0.99 1.00
## 
## Mean item complexity =  2
## Test of the hypothesis that 7 components are sufficient.
## 
## The root mean square of the residuals (RMSR) is  0 
##  with the empirical chi square  0  with prob &lt;  NA 
## 
## Fit based upon off diagonal values = 1</code></pre>
</div>
<div id="determine-the-number-of-components-to-retain" class="section level3 hasAnchor" number="7.3.6">
<h3><span class="header-section-number">7.3.6</span> Determine the number of components to retain<a href="week-06-principal-components-analysis.html#determine-the-number-of-components-to-retain" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb424"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb424-1"><a href="week-06-principal-components-analysis.html#cb424-1" tabindex="-1"></a>fit.pca<span class="sc">$</span>Vaccounted</span></code></pre></div>
<pre><code>##                             PC1       PC2       PC3        PC4        PC5        PC6         PC7
## SS loadings           4.3236422 0.8989944 0.8297417 0.46675769 0.29832218 0.11387578 0.068666022
## Proportion Var        0.6176632 0.1284278 0.1185345 0.06667967 0.04261745 0.01626797 0.009809432
## Cumulative Var        0.6176632 0.7460909 0.8646255 0.93130515 0.97392260 0.99019057 1.000000000
## Proportion Explained  0.6176632 0.1284278 0.1185345 0.06667967 0.04261745 0.01626797 0.009809432
## Cumulative Proportion 0.6176632 0.7460909 0.8646255 0.93130515 0.97392260 0.99019057 1.000000000</code></pre>
<p>We see that the first variable explained 62% of the variance in the seven events. the second and third components explained an additional 13% and 12% each, whereas the fourth component only explained about 7% of the variation. With this, it may be worthwhile to retain the first three components. Let’s look at the eigenvalues, the scree plot, and the parallel analysis results.</p>
<p>Looking at the output, where it says <code>SS loadings</code>, we can see the eigenvalues. Based on this solution, if we were to follow the Kaiser rule of eigenvalues <span class="math inline">\(\ge 1\)</span>, a single component stands out above the rest. We wanted, we could verify this with the <code>eigen()</code> function.</p>
<div class="sourceCode" id="cb426"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb426-1"><a href="week-06-principal-components-analysis.html#cb426-1" tabindex="-1"></a><span class="fu">eigen</span>(<span class="fu">cor</span>(dat[ ,varbnames]))</span></code></pre></div>
<pre><code>## eigen() decomposition
## $values
## [1] 4.32364217 0.89899445 0.82974172 0.46675769 0.29832218 0.11387578 0.06866602
## 
## $vectors
##            [,1]        [,2]       [,3]        [,4]        [,5]        [,6]        [,7]
## [1,] -0.4503876  0.05772161 -0.1739345 -0.04840598 -0.19889364  0.84665086 -0.06961672
## [2,] -0.3145115 -0.65133162 -0.2088272  0.55694554  0.07076358 -0.09007544  0.33155910
## [3,] -0.4024884 -0.02202088 -0.1534709 -0.54826705  0.67166466 -0.09886359  0.22904298
## [4,] -0.4270860  0.18502783  0.1301287 -0.23095946 -0.61781764 -0.33279359  0.46971934
## [5,] -0.4509639 -0.02492486 -0.2697589  0.01468275 -0.12151793 -0.38294411 -0.74940781
## [6,] -0.2423079 -0.32572229  0.8806995 -0.06024757  0.07874396  0.07193437 -0.21108138
## [7,] -0.3029068  0.65650503  0.1930020  0.57418128  0.31880178 -0.05217664  0.07718616</code></pre>
<p>To continue the exercise, we can also examine the scree plot and conduct a parallel analysis. Before, we used the <code>parallel()</code> function from the <code>nFactors</code> package to conduct the parallel analysis. That is a better method for reporting, as we can be certain about the 95% cutoff. Another, much easier, option is to use the <code>fa.parallel()</code> function from the psych package. Because we’re using principal components analysis (not factor analysis), we need to specify the model using the argument <code>fa = "pc"</code>. We’ll also specify the number of factors as the same as the number fo variables.</p>
<div class="sourceCode" id="cb428"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb428-1"><a href="week-06-principal-components-analysis.html#cb428-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)   <span class="co"># To reproduce our randomly generated results.</span></span>
<span id="cb428-2"><a href="week-06-principal-components-analysis.html#cb428-2" tabindex="-1"></a>para.psych <span class="ot">&lt;-</span> psych<span class="sc">::</span><span class="fu">fa.parallel</span>(dat[,varbnames], </span>
<span id="cb428-3"><a href="week-06-principal-components-analysis.html#cb428-3" tabindex="-1"></a>                   <span class="at">fa =</span> <span class="st">&quot;pc&quot;</span>,</span>
<span id="cb428-4"><a href="week-06-principal-components-analysis.html#cb428-4" tabindex="-1"></a>                   <span class="at">nfactors =</span> <span class="fu">length</span>(varbnames))</span></code></pre></div>
<p><img src="multivariate_stats_files/figure-html/unnamed-chunk-213-1.png" width="672" /></p>
<pre><code>## Parallel analysis suggests that the number of factors =  NA  and the number of components =  1</code></pre>
<p>These results clearly indicate that we can reduce the number of dimensions from seven to 1. To be sure, we should examine the 95 percentile with nFactors package’s <code>parallel()</code> function results.</p>
<div class="sourceCode" id="cb430"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb430-1"><a href="week-06-principal-components-analysis.html#cb430-1" tabindex="-1"></a><span class="fu">library</span>(nFactors)</span>
<span id="cb430-2"><a href="week-06-principal-components-analysis.html#cb430-2" tabindex="-1"></a>n_i  <span class="ot">&lt;-</span> <span class="fu">nrow</span>(dat) <span class="co"># The number of cases in our data</span></span>
<span id="cb430-3"><a href="week-06-principal-components-analysis.html#cb430-3" tabindex="-1"></a>n_p <span class="ot">&lt;-</span> <span class="fu">length</span>(varbnames) <span class="co"># The number of variables in our data</span></span>
<span id="cb430-4"><a href="week-06-principal-components-analysis.html#cb430-4" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">456</span>)            <span class="co"># To reproduce our randomly generated results.</span></span>
<span id="cb430-5"><a href="week-06-principal-components-analysis.html#cb430-5" tabindex="-1"></a>Eigs <span class="ot">&lt;-</span> fit.pca<span class="sc">$</span>values    <span class="co"># The eigenvalues</span></span>
<span id="cb430-6"><a href="week-06-principal-components-analysis.html#cb430-6" tabindex="-1"></a>n_components  <span class="ot">&lt;-</span> <span class="fu">length</span>(Eigs) <span class="co"># number of components</span></span>
<span id="cb430-7"><a href="week-06-principal-components-analysis.html#cb430-7" tabindex="-1"></a>paral <span class="ot">&lt;-</span> <span class="fu">parallel</span>(<span class="at">subject =</span> n_i,  <span class="co"># The number of cases in our data</span></span>
<span id="cb430-8"><a href="week-06-principal-components-analysis.html#cb430-8" tabindex="-1"></a>                      <span class="at">var =</span> n_p,  <span class="co"># The number of variables in our data</span></span>
<span id="cb430-9"><a href="week-06-principal-components-analysis.html#cb430-9" tabindex="-1"></a>                      <span class="at">rep =</span> <span class="dv">100</span>,</span>
<span id="cb430-10"><a href="week-06-principal-components-analysis.html#cb430-10" tabindex="-1"></a>                 <span class="at">quantile =</span> .<span class="dv">95</span>,</span>
<span id="cb430-11"><a href="week-06-principal-components-analysis.html#cb430-11" tabindex="-1"></a>                   <span class="at">model  =</span> <span class="st">&quot;components&quot;</span>)</span>
<span id="cb430-12"><a href="week-06-principal-components-analysis.html#cb430-12" tabindex="-1"></a></span>
<span id="cb430-13"><a href="week-06-principal-components-analysis.html#cb430-13" tabindex="-1"></a>ParallelAna <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">Ncomponent  =</span> <span class="dv">1</span><span class="sc">:</span>n_components,</span>
<span id="cb430-14"><a href="week-06-principal-components-analysis.html#cb430-14" tabindex="-1"></a>                           Eigs,</span>
<span id="cb430-15"><a href="week-06-principal-components-analysis.html#cb430-15" tabindex="-1"></a>                           <span class="at">RandEigM =</span> paral<span class="sc">$</span>eigen<span class="sc">$</span>mevpea,</span>
<span id="cb430-16"><a href="week-06-principal-components-analysis.html#cb430-16" tabindex="-1"></a>                           <span class="at">RandEig95=</span> paral<span class="sc">$</span>eigen<span class="sc">$</span>qevpea)</span>
<span id="cb430-17"><a href="week-06-principal-components-analysis.html#cb430-17" tabindex="-1"></a>ParallelAna <span class="ot">&lt;-</span> <span class="fu">round</span>(ParallelAna, <span class="dv">3</span>)</span>
<span id="cb430-18"><a href="week-06-principal-components-analysis.html#cb430-18" tabindex="-1"></a>ParallelAna</span></code></pre></div>
<pre><code>##   Ncomponent  Eigs RandEigM RandEig95
## 1          1 4.324    1.857     2.242
## 2          2 0.899    1.447     1.639
## 3          3 0.830    1.161     1.313
## 4          4 0.467    0.940     1.063
## 5          5 0.298    0.748     0.873
## 6          6 0.114    0.528     0.680
## 7          7 0.069    0.318     0.505</code></pre>
<p>This code here might be easier to read, though less reproducible. I also changed the repetitions to 1000 to get a more stable result (though the conclusion is the same).</p>
<div class="sourceCode" id="cb432"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb432-1"><a href="week-06-principal-components-analysis.html#cb432-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">994</span>)</span>
<span id="cb432-2"><a href="week-06-principal-components-analysis.html#cb432-2" tabindex="-1"></a>paral <span class="ot">&lt;-</span> nFactors<span class="sc">::</span><span class="fu">parallel</span>(<span class="at">subject =</span> <span class="dv">24</span>,  <span class="co"># The number of cases</span></span>
<span id="cb432-3"><a href="week-06-principal-components-analysis.html#cb432-3" tabindex="-1"></a>                      <span class="at">var =</span> <span class="dv">7</span>,             <span class="co"># The number of variables</span></span>
<span id="cb432-4"><a href="week-06-principal-components-analysis.html#cb432-4" tabindex="-1"></a>                      <span class="at">rep =</span> <span class="dv">1000</span>,</span>
<span id="cb432-5"><a href="week-06-principal-components-analysis.html#cb432-5" tabindex="-1"></a>                 <span class="at">quantile =</span> .<span class="dv">95</span>,</span>
<span id="cb432-6"><a href="week-06-principal-components-analysis.html#cb432-6" tabindex="-1"></a>                   <span class="at">model  =</span> <span class="st">&quot;components&quot;</span>)</span>
<span id="cb432-7"><a href="week-06-principal-components-analysis.html#cb432-7" tabindex="-1"></a></span>
<span id="cb432-8"><a href="week-06-principal-components-analysis.html#cb432-8" tabindex="-1"></a>ParallelAna <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">Ncomponent  =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">7</span>, <span class="co"># We have 7 components</span></span>
<span id="cb432-9"><a href="week-06-principal-components-analysis.html#cb432-9" tabindex="-1"></a>                           Eigs,</span>
<span id="cb432-10"><a href="week-06-principal-components-analysis.html#cb432-10" tabindex="-1"></a>                           <span class="at">RandEigM =</span> paral<span class="sc">$</span>eigen<span class="sc">$</span>mevpea,</span>
<span id="cb432-11"><a href="week-06-principal-components-analysis.html#cb432-11" tabindex="-1"></a>                           <span class="at">RandEig95=</span> paral<span class="sc">$</span>eigen<span class="sc">$</span>qevpea)</span>
<span id="cb432-12"><a href="week-06-principal-components-analysis.html#cb432-12" tabindex="-1"></a>ParallelAna <span class="ot">&lt;-</span> <span class="fu">round</span>(ParallelAna, <span class="dv">3</span>)</span>
<span id="cb432-13"><a href="week-06-principal-components-analysis.html#cb432-13" tabindex="-1"></a>ParallelAna</span></code></pre></div>
<pre><code>##   Ncomponent  Eigs RandEigM RandEig95
## 1          1 4.324    1.862     2.242
## 2          2 0.899    1.446     1.683
## 3          3 0.830    1.165     1.330
## 4          4 0.467    0.938     1.075
## 5          5 0.298    0.734     0.880
## 6          6 0.114    0.540     0.696
## 7          7 0.069    0.315     0.513</code></pre>
<p>This result does suggest that at the second component, the randomly generated eigenvalue at the 95 percentile (1.639) exceeds that of our second observed eigenvalue of 0.899.</p>
</div>
<div id="refit-the-pca-model-using-rotation-if-that-is-acceptable-in-your-field" class="section level3 hasAnchor" number="7.3.7">
<h3><span class="header-section-number">7.3.7</span> Refit the PCA model, using rotation if that is acceptable in your field<a href="week-06-principal-components-analysis.html#refit-the-pca-model-using-rotation-if-that-is-acceptable-in-your-field" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s refit the PCA model to have a single component. If we had a solution with more than two components, we could use rotation, such as with <code>rotate = "varimax"</code>. Here, rotation does not matter because there is a single component.</p>
<div class="sourceCode" id="cb434"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb434-1"><a href="week-06-principal-components-analysis.html#cb434-1" tabindex="-1"></a>fit.pca1 <span class="ot">&lt;-</span> psych<span class="sc">::</span><span class="fu">pca</span>(dat[ ,varbnames], </span>
<span id="cb434-2"><a href="week-06-principal-components-analysis.html#cb434-2" tabindex="-1"></a>                      <span class="at">nfactors =</span> <span class="dv">1</span>,</span>
<span id="cb434-3"><a href="week-06-principal-components-analysis.html#cb434-3" tabindex="-1"></a>                      <span class="at">rotate =</span> <span class="st">&quot;varimax&quot;</span>,</span>
<span id="cb434-4"><a href="week-06-principal-components-analysis.html#cb434-4" tabindex="-1"></a>                      <span class="at">scores =</span> <span class="cn">TRUE</span>)</span>
<span id="cb434-5"><a href="week-06-principal-components-analysis.html#cb434-5" tabindex="-1"></a><span class="fu">print</span>( fit.pca1, <span class="at">digits  =</span> <span class="dv">3</span>)</span></code></pre></div>
<pre><code>## Principal Components Analysis
## Call: principal(r = r, nfactors = nfactors, residuals = residuals, 
##     rotate = rotate, n.obs = n.obs, covar = covar, scores = scores, 
##     missing = missing, impute = impute, oblique.scores = oblique.scores, 
##     method = method, use = use, cor = cor, correct = 0.5, weight = NULL)
## Standardized loadings (pattern matrix) based upon correlation matrix
##            PC1    h2    u2 com
## hurdles  0.937 0.877 0.123   1
## highjump 0.654 0.428 0.572   1
## shot     0.837 0.700 0.300   1
## run200m  0.888 0.789 0.211   1
## longjump 0.938 0.879 0.121   1
## javelin  0.504 0.254 0.746   1
## run800m  0.630 0.397 0.603   1
## 
##                  PC1
## SS loadings    4.324
## Proportion Var 0.618
## 
## Mean item complexity =  1
## Test of the hypothesis that 1 component is sufficient.
## 
## The root mean square of the residuals (RMSR) is  0.099 
##  with the empirical chi square  9.812  with prob &lt;  0.776 
## 
## Fit based upon off diagonal values = 0.97</code></pre>
</div>
<div id="interpret-the-components" class="section level3 hasAnchor" number="7.3.8">
<h3><span class="header-section-number">7.3.8</span> Interpret the components<a href="week-06-principal-components-analysis.html#interpret-the-components" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>After examining the size of the eigenvalues, the scree plot, and the parallel test, we concluded that the eight variables can be reduced to a single dimension, which explains 62% of the variability in the observed variables. It appears as though all of the events contributed to this component. Particularly influential were the long jump and hurdles, with coefficients exceeding .90. The 200 meter race and the shot put also had strong influence on the component, with coefficients in the .80 to .89 range. The Javelin contributed least strongly to the component score.</p>
</div>
<div id="saving-the-scores" class="section level3 hasAnchor" number="7.3.9">
<h3><span class="header-section-number">7.3.9</span> Saving the scores<a href="week-06-principal-components-analysis.html#saving-the-scores" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We had asked for <code>scores = TRUE</code> when fitting the one-component model. We can attach the scores to the data frame.</p>
<div class="sourceCode" id="cb436"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb436-1"><a href="week-06-principal-components-analysis.html#cb436-1" tabindex="-1"></a>dat<span class="sc">$</span>C.score <span class="ot">&lt;-</span> fit.pca1<span class="sc">$</span>scores</span></code></pre></div>
<p>Let’s merge the scores into the dat dataframe so we can compare the component score with the scores allotted in the competition.</p>
<div class="sourceCode" id="cb437"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb437-1"><a href="week-06-principal-components-analysis.html#cb437-1" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">left_join</span>(dat, hepdat)</span></code></pre></div>
<p>Let’s examine the correlation between the component score and the observed score from the competition:</p>
<div class="sourceCode" id="cb438"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb438-1"><a href="week-06-principal-components-analysis.html#cb438-1" tabindex="-1"></a><span class="fu">cor</span>(dat[ , <span class="fu">c</span>(<span class="st">&quot;C.score&quot;</span>, <span class="st">&quot;score&quot;</span>)] )</span></code></pre></div>
<pre><code>##           C.score     score
## C.score 1.0000000 0.9931168
## score   0.9931168 1.0000000</code></pre>
<p>We have good evidence to support the assertion that the final score used in the competition reflected the principal sources of variability in among the seven events.</p>
</div>
</div>
<div id="other-example-from-later-in-the-chapter-not-assigned" class="section level2 hasAnchor" number="7.4">
<h2><span class="header-section-number">7.4</span> Other example from later in the chapter (not assigned)<a href="week-06-principal-components-analysis.html#other-example-from-later-in-the-chapter-not-assigned" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="usairpollution-data" class="section level3 hasAnchor" number="7.4.1">
<h3><span class="header-section-number">7.4.1</span> USairpollution Data<a href="week-06-principal-components-analysis.html#usairpollution-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb440"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb440-1"><a href="week-06-principal-components-analysis.html#cb440-1" tabindex="-1"></a>USair <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;USairpollution.csv&quot;</span>)</span>
<span id="cb440-2"><a href="week-06-principal-components-analysis.html#cb440-2" tabindex="-1"></a><span class="fu">str</span>(USair)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    41 obs. of  8 variables:
##  $ city   : chr  &quot;Albany&quot; &quot;Albuquerque&quot; &quot;Atlanta&quot; &quot;Baltimore&quot; ...
##  $ SO2    : int  46 11 24 47 11 31 110 23 65 26 ...
##  $ temp   : num  47.6 56.8 61.5 55 47.1 55.2 50.6 54 49.7 51.5 ...
##  $ manu   : int  44 46 368 625 391 35 3344 462 1007 266 ...
##  $ popul  : int  116 244 497 905 463 71 3369 453 751 540 ...
##  $ wind   : num  8.8 8.9 9.1 9.6 12.4 6.5 10.4 7.1 10.9 8.6 ...
##  $ precip : num  33.36 7.77 48.34 41.31 36.11 ...
##  $ predays: int  135 58 115 111 166 148 122 132 155 134 ...</code></pre>
<div class="sourceCode" id="cb442"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb442-1"><a href="week-06-principal-components-analysis.html#cb442-1" tabindex="-1"></a>air <span class="ot">&lt;-</span> USair <span class="sc">%&gt;%</span> </span>
<span id="cb442-2"><a href="week-06-principal-components-analysis.html#cb442-2" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">precip =</span> <span class="fu">max</span>(precip) <span class="sc">-</span> precip) <span class="co"># Reverse scoring precipitation</span></span></code></pre></div>
</div>
</div>
<div id="another-resource" class="section level2 hasAnchor" number="7.5">
<h2><span class="header-section-number">7.5</span> Another resource<a href="week-06-principal-components-analysis.html#another-resource" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>There are other resources on principal components analysis. One site I found particularly useful in visualizing the <a href="http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/112-pca-principal-component-analysis-essentials/#pca-data-format">components is on STHDA</a>.</p>
<p>If you find other resources to share with the class, please do!</p>
</div>
<div id="references" class="section level2 hasAnchor" number="7.6">
<h2><span class="header-section-number">7.6</span> References<a href="week-06-principal-components-analysis.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h2>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-cattell_scree_1966" class="csl-entry">
Cattell, Raymond B. 1966. <span>“The Scree Test for the Number of Factors.”</span> <em>Multivariate Behavioral Research</em> 1 (2): 245–76. <a href="https://doi.org/10.1207/s15327906mbr0102_10">https://doi.org/10.1207/s15327906mbr0102_10</a>.
</div>
<div id="ref-everitt_introduction_2011" class="csl-entry">
Everitt, Brian, and Torsten Hothorn. 2011. <em>An Introduction to Applied Multivariate Analysis with r</em>. Use r! New York: Springer.
</div>
<div id="ref-horn_rationale_1965" class="csl-entry">
Horn, John L. 1965. <span>“A Rationale and Test for the Number of Factors in Factor Analysis.”</span> <em>Psychometrika</em> 30 (2): 179–85. <a href="https://doi.org/10.1007/BF02289447">https://doi.org/10.1007/BF02289447</a>.
</div>
<div id="ref-jolliffe_discarding_1972" class="csl-entry">
Jolliffe, I. T. 1972. <span>“Discarding Variables in a Principal Component Analysis. I: Artificial Data.”</span> <em>Journal of the Royal Statistical Society. Series C (Applied Statistics)</em> 21 (2): 160–73. <a href="https://doi.org/10.2307/2346488">https://doi.org/10.2307/2346488</a>.
</div>
<div id="ref-kaiser_index_1974" class="csl-entry">
Kaiser, Henry F. 1974. <span>“An Index of Factorial Simplicity.”</span> <em>Psychometrika</em> 39 (1): 31–36. <a href="https://doi.org/10.1007/BF02291575">https://doi.org/10.1007/BF02291575</a>.
</div>
<div id="ref-R-nFactors" class="csl-entry">
Raiche, Gilles, and David Magis. 2025. <em>nFactors: Parallel Analysis and Other Non Graphical Solutions to the Cattell Scree Test</em>. <a href="https://doi.org/10.32614/CRAN.package.nFactors">https://doi.org/10.32614/CRAN.package.nFactors</a>.
</div>
<div id="ref-R-psych" class="csl-entry">
Revelle, William. 2025. <em>Psych: Procedures for Psychological, Psychometric, and Personality Research</em>. <a href="https://personality-project.org/r/psych/">https://personality-project.org/r/psych/</a>.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="28">
<li id="fn28"><p>The psych package also uses <code>principal()</code> for the same thing, as <code>pca()</code> wraps <code>principal()</code> into it.<a href="week-06-principal-components-analysis.html#fnref28" class="footnote-back">↩︎</a></p></li>
<li id="fn29"><p>We can also be lazy and use the <code>cor_to_cov()</code> function from the <code>correlation</code> package <span class="citation">(<a href="#ref-R-correlation"><strong>R-correlation?</strong></a>)</span>.<a href="week-06-principal-components-analysis.html#fnref29" class="footnote-back">↩︎</a></p></li>
<li id="fn30"><p>There are packages we can use for this; for example, the <code>factoextra</code> package <span class="citation">(<a href="#ref-R-factoextra"><strong>R-factoextra?</strong></a>)</span> has the <code>fviz_screeplot()</code> function.<a href="week-06-principal-components-analysis.html#fnref30" class="footnote-back">↩︎</a></p></li>
<li id="fn31"><p>Strictly speaking, we should only run this procedure one time—or with a single random seed so we can reproduce our one procedure—to avoid phishing for the desired number of components.<a href="week-06-principal-components-analysis.html#fnref31" class="footnote-back">↩︎</a></p></li>
<li id="fn32"><p>I’m sure I got some of those incorrect as I only have the labels that were presented in Everitt and Hothorn’s (2011) chapter.<a href="week-06-principal-components-analysis.html#fnref32" class="footnote-back">↩︎</a></p></li>
<li id="fn33"><p>There are other non-orthogonal rotations, which we will use in EFA.<a href="week-06-principal-components-analysis.html#fnref33" class="footnote-back">↩︎</a></p></li>
<li id="fn34"><p>Some functions, such as the psych pacakge’s <code>pca()</code> function will report PCA results based on the standardized scores by default.<a href="week-06-principal-components-analysis.html#fnref34" class="footnote-back">↩︎</a></p></li>
<li id="fn35"><p>We are doing double duty in this demonstration by squaring and then rooting but it is to demonstrate that there is a thing called the eigenvalue matrix, which has eigenvalues along the diagonal.<a href="week-06-principal-components-analysis.html#fnref35" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="week-05-discriminant-function-analysis.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="week-07-exploratory-factor-analysis-part-1.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": null,
    "text": null
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
