[["hotellings-t2.html", "2 Hotelling’s \\(T^2\\) 2.1 Within-group covariance matrices 2.2 SSCP matrices 2.3 Conducting the Test using Functions", " 2 Hotelling’s \\(T^2\\) Here is our data set, again. Let’s make the grp variable a factor, representing two different types of treatment, behavior modification and cognitive method. The dependent variables are sa (client satisfaction) and csa (client self acceptance). dat &lt;- data.frame( id = 1:9, grp = factor(c(rep(&quot;behavmod&quot;, 3), rep(&quot;cogmethod&quot;,6))), sa = c(1, 3, 2, 4, 6, 6, 5, 5, 4), csa = c(3, 7, 2, 6, 8, 8,10,10, 6)) Table 2.1: Our data set id grp sa csa 1 behavmod 1 3 2 behavmod 3 7 3 behavmod 2 2 4 cogmethod 4 6 5 cogmethod 6 8 6 cogmethod 6 8 7 cogmethod 5 10 8 cogmethod 5 10 9 cogmethod 4 6 2.1 Within-group covariance matrices In ANOVA, we are often interested in partitioning the variance into the between-groups variance and the within-groups variance. The between-groups variance represents how much the groups differ from each other on the variable of interest whereas the within-group variance, or residual variance, represents how much of the variance our statistical model does not explain. In MANOVA, we use covariance matrices for this same approach. In our example, we have two dependent variables, sa and csa, and one independent variable, grp. Let’s obtain the within-group covariance. Basically, we’re splitting the data by group and then obtaining the covariance matrix: # Separately by group: (cov_W1 &lt;- cov(subset(dat, grp == &quot;behavmod&quot;)[,c(&quot;sa&quot;, &quot;csa&quot;)]) ) ## sa csa ## sa 1 2 ## csa 2 7 (cov_W2 &lt;- cov(subset(dat, grp == &quot;cogmethod&quot;)[,c(&quot;sa&quot;, &quot;csa&quot;)]) ) ## sa csa ## sa 0.8 0.8 ## csa 0.8 3.2 2.2 SSCP matrices In ANOVA, the sum of the squared deviations (or sums of squares, SS, for short) is often used. The SS is the numerator in the calculation of the variance, \\(\\frac{\\Sigma(y_i-\\bar{y})^2}{n-1}\\), so if our computer program outputs the variance, we can get the SS by multiplying the variance by the denominator (the degrees of freedom), \\(SS_y = \\frac{\\Sigma(y_i-\\bar{y})^2}{n-1} (n-1)\\) In MANOVA, we use the matrix version of SS, which is the sum-of-squares-and-cross-products (SSCP) matrix. With matrices, we can multiply each element by a single number (also called a scalar) and get a new matrix. Here, we’re multiplying each group’s within-group variance by its degrees of freedom (which is \\(n-1\\) for each group) to get each group’s SSCP matrix.1 n_1 &lt;- nrow(subset(dat, grp == &quot;behavmod&quot;)) n_2 &lt;- nrow(subset(dat, grp == &quot;cogmethod&quot;)) (SSCP_W1 &lt;- cov_W1 * (n_1 - 1) ) ## sa csa ## sa 2 4 ## csa 4 14 (SSCP_W2 &lt;- cov_W2 * (n_2 - 1) ) ## sa csa ## sa 4 4 ## csa 4 16 \\[\\mathbf{W_1} = \\begin{bmatrix} 2.00&amp; 4.00 \\\\ 4.00&amp;14.00 \\\\\\end{bmatrix}\\] \\[\\mathbf{W_2} = \\begin{bmatrix} 4.00&amp; 4.00 \\\\ 4.00&amp;16.00 \\\\\\end{bmatrix}\\] These are the same within-group matrices reported on p. 148 of Pituch and Stevens (2016). The pooled within SSCP is the sum of these two matrices, which is in the SPSS output and labeled as error SSCP: SSCP_W1 + SSCP_W2 ## sa csa ## sa 6 8 ## csa 8 30 In MANOVA, the error term is the pooled within-Covariance matrix, which is the SSCP divided by the degrees of freedom, as displayed on p. 149 of Pituch and Stevens (2016). (S &lt;- (SSCP_W1 + SSCP_W2) / (n_1 + n_2 - 2) ) ## sa csa ## sa 0.8571429 1.142857 ## csa 1.1428571 4.285714 In matrix format, and rounded to two decimals, it is displayed like this: \\[\\mathbf{S} = \\begin{bmatrix}0.86&amp;1.14 \\\\1.14&amp;4.29 \\\\\\end{bmatrix}\\] This error matrix represents the noise in the signal-to-noise ratio that is used in a test statistic, such as in a t-test in univariate tests. In other words, it is the denominator. In our example, we have two groups in the independent variable and two dependent variables. The signal is the difference between the two groups on their mean scores on each of the two dependent variables, sa and csa. However, unlike the critical ratio in univariate tests, we cannot have a matrix in the denominator because we cannot divide by a matrix. Instead, we use the inverse and use matrix multiplication. Pituch and Stevens (p. 145) illustrate the relationship between the univariate t-test and Hotelling’s \\(T^2\\), where the univariate t statistic is calculated as \\[t = \\frac{\\bar{y}_1 - \\bar{y}_2}{\\sqrt{\\frac{(n_1-1)s^2_1 + (n_2-1)s^2_2 }{n_1 + n_2 - 2} }\\left( \\frac{1}{n_1} + \\frac{1}{n_2}\\right)} \\text{,} \\] which can be algebraically manipulated to this, after it is squared: \\[t^2 = \\frac{n_1n_2}{n_1 + n_2}(\\bar{y}_1 - \\bar{y}_2)(s^2)^-1(\\bar{y}_1 - \\bar{y}_2)\\] This allows us to see how the univariate test is similar to its multivariate counterpart, Hotelling’s \\(T^2\\): \\[T^2 = \\frac{n_1n_2}{n_1 + n_2}(\\mathbf{\\bar{y}_1 - \\bar{y}_2)^\\prime{}}\\mathbf{S^{-1}}(\\mathbf{\\bar{y}_1 - \\bar{y}_2)}\\] which is more succinctly presented as \\[T^2 = k\\mathbf{d}^{\\prime}\\mathbf{S^{-1}}\\mathbf{d}\\] library(tidyverse) m_bygrp &lt;- dat %&gt;% group_by(grp) %&gt;% summarize(m_y1 = mean(sa), m_y2 = mean(csa) ) m_bygrp ## # A tibble: 2 × 3 ## grp m_y1 m_y2 ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 behavmod 2 4 ## 2 cogmethod 5 8 # using a fancy function to transpose the table # install.packages(&quot;sjmisc&quot;) library(sjmisc) m_bygrp &lt;- m_bygrp %&gt;% rotate_df(cn = TRUE) %&gt;% mutate(m_diff = behavmod - cogmethod) k &lt;- (n_1*n_2)/(n_1 + n_2) d &lt;- m_bygrp$m_diff d # vector of mean differences ## [1] -3 -4 We can rely on software to calculate the inverse of the within-variance matrix: solve(S) ## sa csa ## sa 1.8103448 -0.4827586 ## csa -0.4827586 0.3620690 \\[\\mathbf{S^{-1}} = \\begin{bmatrix} 1.81&amp;-0.48 \\\\-0.48&amp; 0.36 \\\\\\end{bmatrix}\\] Here is our calculation of Hotelling’s \\(T^2\\) test statistic:2 T_sq &lt;- k * t(d) %*% solve(S) %*% d T_sq ## [,1] ## [1,] 21 We can use Hotelling’s formula for the exact F from a \\(T^2\\) to estimate the F test statistic, which we can subsequently test against our critical F with \\(df_1 = p\\) (i.e., the number of dependent variables), and \\(df_2 = N - p - 1\\). \\[F = \\frac{n_1 + n_2 - p - 1}{(n_1 + n_2 - 2)p}T^2 = 21\\] p &lt;- 2 # number of DVs ( F_exact &lt;- (n_1 + n_2 - p - 1) / ((n_1 + n_2 - 2)*p) * T_sq ) ## [,1] ## [1,] 9 (F_crit &lt;- qf(p = .05, # alpha for critical value of F df1 = p, df2 = (n_1 + n_2 - p - 1), lower.tail = FALSE)) ## [1] 5.143253 F_exact &gt; F_crit # If TRUE, then we can reject null hypothesis at alpha. ## [,1] ## [1,] TRUE 2.3 Conducting the Test using Functions To avoid making coding mistakes with all of those calculations, we can use functions to conduct the tests. Here is one specifically for Hotelling’s \\(T^2\\) from the Hotelling package and another using the manova() function in Base R.3 # install.packages(&quot;Hotelling&quot;) library(Hotelling) # saving the dependent variable data as a matrix dvs &lt;- as.matrix(dat[ , c(&quot;sa&quot;,&quot;csa&quot;)] ) hot_mod &lt;- hotelling.test(x = dvs[1:3, ], y = dvs[4:9, ] ) hot_mod ## Test stat: 21 ## Numerator df: 2 ## Denominator df: 6 ## P-value: 0.01562 dvs &lt;- as.matrix(dat[ , c(&quot;sa&quot;,&quot;csa&quot;)] ) man_mod &lt;- manova(dvs ~ grp, data = dat) summary(man_mod, test = &quot;Hotelling&quot; ) ## Df Hotelling-Lawley approx F num Df den Df Pr(&gt;F) ## grp 1 3 9 2 6 0.01563 * ## Residuals 7 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 There are four test statistics we could report from a MANOVA, Pillais’s trace, Wilks’ lambda, Hotelling’s trace, and Roy’s largest root. Because we’re only examining two groups, all of the tests yield the same statistical significance and statistical conclusion. Typically Wilks’ lambda is reported. Because we’re interested in Hotelling’s \\(T^2\\), we can report the Hotelling’s trace. The test-statistic is \\(3\\), which differs from the Hotelling’s \\(T^2\\) test statistic of 21, but which is directly related to it by a factor of \\(N - k\\), where \\(N\\) is the total sample size and \\(k\\) is the number of groups. With our example, given our test statistic of Hotelling-Lawley \\(= 3\\), Hotelling’s \\(T^2 = 3(9 - 2) = 21\\). In R, the nrow() function can be used to count the number of observations in the data frame.↩︎ In R, we use %*% for matrix multiplication.↩︎ The hotelling.test() function has a var.equal = argument, which is handy if we do not meet the assumption of equal variances and we want to use robust SE for a more cautious test of statistical significance (set var.equal = FALSE). ↩︎ "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
