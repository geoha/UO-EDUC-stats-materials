[["index.html", "Guidebook for Using R in Applied Multivariate Statistics Preface", " Guidebook for Using R in Applied Multivariate Statistics George M. Harrison Winter 2023 Preface This book is a guide for using R and for understanding the procedures in EDUC 644 Applied Multivariate Statistics, which was last offered in 2023. It uses data from the course readings and provides additional instruction in terminology and in planning analyses. Topics include MANOVA, discriminant function analysis, principal components analysis, exploratory factor analysis, and confirmatory factor analysis. "],["terminology-and-matrix-manipulations-used-in-multivariate-statistics.html", "1 Terminology and Matrix Manipulations Used in Multivariate Statistics 1.1 Scalars 1.2 Vectors 1.3 Matrices 1.4 Symmetrical Matrices 1.5 Transposed Vectors and Matrices 1.6 Matrix Addition and Subtraction 1.7 Operations on Matrices with a Scalar 1.8 Matrix Multiplication 1.9 Inverse of a Matrix 1.10 Matrix determinants, as generalized variance 1.11 Eigenvectors and Eigenvalues 1.12 Summary 1.13 Optional: SPSS and R Code to Manipulate Matrices", " 1 Terminology and Matrix Manipulations Used in Multivariate Statistics In this class and beyond, you will encounter published work that assumes you have some knowledge of the mathematical operations that are used in multivariate statistics. This is where matrix algebra comes in. MANOVA, discriminant function analysis, principal components analysis, exploratory &amp; confirmatory factor analyses (and their extensions) all use matrix algebra. Unfortunately, matrix algebra is completely new to most students in education and the social sciences, which poses a small challenge for us to overcome before we delve into the content of our class. Our purpose is not to study math but to understand the terminology and operations well enough to learn about the fundamental procedures used in multivariate statistics. This in turn serves to help us read and critique existing work and make informed decisions about what options to select while conducting our own multivariate analyses. With this, our goal is to understand matrix algebra at least enough to understand (a) the mathematical symbols used in the equations involved in applied multivariate statistics, (b) some of the fundamental operations used in multivariate statistics, and (c) the software output from a multivariate model procedure. We’ll address some types of numeric structures of data used in matrix algebra: scalars, vectors, matrices, and diagonals. Along the way, we’ll look at some operations, such as addition and subtraction, multiplication, and the matrix-algebra version of division, which is called inversion. We’ll also address get introduced to the determinant, which is a single number that represents the generalized variance of a multivariate matrix of data. At the very end, there is some sample R and SPSS code that can be used to conduct some of these matrix operations. 1.1 Scalars One thing statisticians like to do is use obscure terminology for simple things. The purpose of this is to ensure the rest of us feel incompetent when we encounter these terms—okay, this may not be the real purpose. Back to the point, here: What is a scalar? A scalar is a single number. Why is it called a scalar? Because we can use it to rescale other numbers. For example, if we multiply a number by a scalar that is less than 1, it shrinks it; if we multiply by a scalar greater than 1, it expands it. Negative scalars rescale a number in the opposite direction. When we work with matrices, we can rescale them using scalars. 1.2 Vectors A vector is a series of numbers. The most familiar example is a column of data in a data set—that’s a vector. For example, if we have nine people’s scores on a variable intended to measure their satisfaction with their counseling treatment, we can represent it as a vector: \\[\\mathbf{a} = \\begin{bmatrix} 1 \\\\ 3 \\\\ 2 \\\\ 4 \\\\ 6 \\\\ 6 \\\\ 5 \\\\ 5 \\\\ 4 \\end{bmatrix}\\] When we display a vector or a matrix, we use brackets around it, as is done above. Vectors are typically symbolized as boldface and lower case. This vector is labeled as \\(\\mathbf{a}\\), arbitrarily, but we could also give it a meaningful label, such as \\(\\mathbf{sa}\\), which is the name of this column of data in our data set, which we’ll see in a minute. 1.3 Matrices 1.3.1 Data Files as Matrices We can think of our typical data file as being made up of several vectors side by side each other, as columns. We can also think of a data file as a data matrix.1 Here is a tiny fictitious data set from Pituch and Stevens (2016, Ch. 4) containing nine counseling patients, three of whom are in one group of treatment (receiving a behavior-modification intervention), with the remaining 6 in a second group (receiving cognitive intervention). Each person is then measured on two continuous outcome variables: client satisfaction, sa, and client self-acceptance, csa. Our data set includes nine observations and three variables, two of which are the dependent variables we just addressed, and one of which is a grouping variable, grp. I suppose we could count the id column as a variable, too, but we typically do not. Table 1.1: Our data set id grp sa csa 1 1 1 3 2 1 3 7 3 1 2 2 4 2 4 6 5 2 6 8 6 2 6 8 7 2 5 10 8 2 5 10 9 2 4 6 A data set is a type of matrix—it is an \\(N \\times p\\) (said as “n by p”) data matrix, with N being the number of rows and p being the number of columns.2 \\[\\mathbf{X} = \\begin{bmatrix}1&amp;1&amp; 3 \\\\1&amp;3&amp; 7 \\\\1&amp;2&amp; 2 \\\\2&amp;4&amp; 6 \\\\2&amp;6&amp; 8 \\\\2&amp;6&amp; 8 \\\\2&amp;5&amp;10 \\\\2&amp;5&amp;10 \\\\2&amp;4&amp; 6 \\\\\\end{bmatrix}\\] Sometimes, we hear the term order of a matrix, which is another way to say the number of rows and number of columns; the order of this matrix is \\(9 \\times 3\\) (said as “9 by 3”).3 Each cell in a matrix or vector is called an element. The coordinate of an element is labeled based on its row and column location, in that order. For example, cell \\(X[3,2]\\) is the third row and second column, which is a value of \\(2\\) in our data matrix, corresponding with Person 3’s score on the sa variable. 1.4 Symmetrical Matrices Whereas data matrices are usually rectangular (with more N than p), variance-covariance and sum-of-squares-and-cross-products matrices are symmetrical, taking the shape of a square matrix. That is, they have the same number of columns and rows. Their numbers on each side of the off-diagonal are mirrored. 1.4.1 Variance-covariance Matrices One example of a symmetrical matrix that we often encounter in statistics (and measurement) is the variance-covariance matrix. Each row and each column are the variables, just as we see in a correlation matrix. This is of the order \\(p \\times p\\); for example, if we have two variables, it is a \\(2 \\times 2\\) matrix. A \\(3 \\times 3\\) covariance is structured like this: \\[\\mathbf{S} = \\begin{bmatrix} s^2_1 \\quad s_{1,2} \\quad s_{1,3} \\\\[.5em] s_{2,1} \\quad s^2_2 \\quad s_{2,3} \\\\[.5em] s_{3,1} \\quad s_{3,2} \\quad s^2_3 \\end{bmatrix}\\] The variances are along the diagonal and the covariances on the off-diagonal. Here is another way to write this same matrix: \\[\\mathbf{S} = \\begin{bmatrix} Var(y_1) \\quad Cov(y_1,y_2), \\quad Cov(y_1,y_3) \\\\[1em] Cov(y_2,y_1) \\quad Var(y_2) \\quad Cov(y_2,y_3) \\\\[1em] Cov(y_3,y_1) \\quad Cov(y_3, y_2) \\quad Var(y_3) \\quad \\end{bmatrix}\\] The symbol \\(\\mathbf{S}\\) is usually used to indicate the observed variance-covariance matrix.4 If we have a correlation matrix, it is usually represented as \\(\\mathbf{R}\\): \\[\\mathbf{R} = \\begin{bmatrix} 1 \\quad r_{1,2} \\quad r_{1,3} \\\\[.5em] r_{2,1} \\quad 1 \\quad r_{2,3} \\\\[.5em] r_{3,1} \\quad r_{3,2} \\quad 1 \\end{bmatrix}\\] Matrices are typically symbolized as boldface and upper case. In this course, we will see matrices labeled as \\(\\mathbf{R}\\), \\(\\mathbf{S}\\), \\(\\mathbf{SSCP}\\), \\(\\mathbf{B}\\), \\(\\mathbf{W}\\), \\(\\mathbf{D}\\), and \\(\\mathbf{C}\\), and some of the more terrifying Greek-symbol matrices used in factor analysis, \\(\\mathbf{\\Lambda}\\), \\(\\mathbf{\\Sigma}\\), \\(\\mathbf{\\Phi}\\), and \\(\\mathbf{\\Theta}\\). 1.4.2 Greek letters We will sometimes encounter two versions of the variance-covariance matrix, \\(\\mathbf{S}\\) and \\(\\mathbf{\\Sigma}\\) (uppercase sigma). Just as is the custom in univariate statistics, Greek letters are often used for the population. For instance, here is the corresponding population variance-covariance matrix for \\(\\mathbf{S}\\): \\[\\mathbf{\\Sigma} = \\begin{bmatrix} \\sigma^2_1 \\quad \\sigma_{1,2} \\quad \\sigma_{1,3} \\\\[.5em] \\sigma_{2,1}\\ \\quad \\sigma^2_2 \\quad \\sigma_{2,3} \\\\[.5em] \\sigma_{3,1} \\quad \\sigma_{3,1} \\quad \\sigma^2_3 \\end{bmatrix}\\] Greek symbols invite extra cognitive load in our learning of stats for those of us who do not speak Greek or who have not worked in statistics for very long. Here’s a general pattern that might help you remember how this works: Frequently, the Greek letters we use for a population statistic have the same initial phonetic sound as their sample counterparts. Here are some examples: the population mean is represented as \\(\\mu\\) (pronounced “moo” or sometimes “myoo”), the population standard deviation (which is the square root of the variance) is represented as \\(\\sigma\\) (sigma), The population correlation, which is \\(r\\) for the sample, is represented as \\(\\rho\\) (rho, as in “row” your boat). Above, we have a boldfaced uppercase Greek letter sigma (\\(\\mathbf{\\Sigma}\\)) representing the population variance-covariance matrix and we have lowercase sigmas representing the variances (\\(\\sigma^2_{i}\\)) and covariances (\\(\\sigma_{i,j}\\)) for each variable or pair of variables. The corresponding sample matrix will be labeled as \\(\\mathbf{S}\\) (or sometimes capital sigma hat, \\(\\mathbf{\\hat{\\Sigma}}\\), to represent an estimate of the population matrix) and will have \\(s_i\\) and \\(s_{i,j}\\) (or \\(\\hat{\\sigma^2}_i\\) and \\(\\hat{\\sigma}_{i,j}\\)) in the elements of the matrix. We’ll see this general pattern with other matrices. For example, in discussions about principal components analysis, we’ll see an uppercase lambda (\\(\\Lambda\\)) as a matrix label, with lowercase lambdas (\\(\\mathbf{\\lambda_i}\\)) along the diagonal. We might see some authors label this same matrix as \\(\\mathbf{L}\\) instead of \\(\\mathbf{\\Lambda}\\). 1.4.3 Example variance-covariance matrix Let’s look at a variance-covariance matrix of our two dependent variables, sa and csa, from our example data set. The variance is on the diagonal, which is \\(3.00\\) for sa and \\(7.75\\) for csa. The covariance between the two variables is on the off diagonal and is identical, \\(4.00\\) in this case, because the covariance of sa with csa (Cell 1,2) is the same as the covariance of csa with sa (Cell 2,1). In matrix format, it looks like this: \\[\\mathbf{S} = \\begin{bmatrix}3.00&amp;4.00 \\\\4.00&amp;7.75 \\\\\\end{bmatrix}\\] The term variance-covariance matrix is long, so people usually use the term covariance matrix. This is logical because the covariance of a variable with itself is actually computationally the same as the variance.5 1.4.4 The diagonal and trace Above, we used the term diagonal a few times. The diagonal of a square matrix includes the series of elements from the top left corner to the bottom right corner. Here, the diagonal of our matrix \\(\\mathbf{S}\\) is \\[\\text{diag}(\\mathbf{S}) = \\text{diag}\\begin{bmatrix}3.00&amp;4.00 \\\\4.00&amp;7.75 \\\\\\end{bmatrix} = \\begin{bmatrix}3.00&amp;7.75 \\end{bmatrix}\\] The diagonal of a covariance matrix includes all the variances. The off-diagonal includes all the covariances. The trace of a matrix is the sum of the elements in the diagonal: \\[\\text{tr}(\\mathbf{S}) = \\text{tr}\\begin{bmatrix}3.00&amp;4.00 \\\\4.00&amp;7.75 \\\\\\end{bmatrix} = 10.75\\] We encounter the diagonal and trace in the explanations about the multivariate computation, particularly in principal components analysis. 1.4.5 An identity matrix An identity matrix has all ones along the diagonal and zeros in the off-diagonal, here is a \\(3 \\times 3\\) identity matrix: \\[\\mathbf{I} = \\begin{bmatrix}1&amp;0&amp;0 \\\\0&amp;1&amp;0 \\\\0&amp;0&amp;1 \\\\\\end{bmatrix}\\] 1.5 Transposed Vectors and Matrices When we see a vector in our multivariate equations, we usually assume it is arranged as a vertical string of numbers. This is how we presented our vector earlier (here, we’ll label it \\(\\mathbf{sa}\\), whereas before, it was \\(\\mathbf{a}\\)): \\[\\mathbf{sa}= \\begin{bmatrix}1\\\\3\\\\2\\\\4\\\\6\\\\6\\\\5\\\\5\\\\4 \\end{bmatrix}\\] If we walk over and kicked that vector on its side, we transpose that vector. It’s now a horizontal vector. \\[\\mathbf{sa^{\\prime}} = \\begin{bmatrix}1&amp;3&amp;2&amp;4&amp;6&amp;6&amp;5&amp;5&amp;4 \\end{bmatrix}\\] In our course readings, we will see a prime, \\(\\mathbf{\\prime}\\), next to a vector or matrix to indicate that it is transposed; for example, the transpose of \\(\\mathbf{S}\\) is symbolized as \\(\\mathbf{S^\\prime}\\). Many other authors instead use a superscript T to indicate transpose: \\(\\mathbf{S}^\\intercal\\). If we transposed the \\(\\mathbf{S}\\) matrix above, we would be interchanging the rows and columns, so the first column in \\(\\mathbf{S}\\) becomes the first row in \\(\\mathbf{S^\\prime}\\), the second column in \\(\\mathbf{S}\\) becomes the second row in \\(\\mathbf{S^\\prime}\\), and so forth. If we transposed our data matrix, for some reason, it would look like this: \\[\\text{Transpose of } \\begin{bmatrix}1&amp;1&amp; 3 \\\\1&amp;3&amp; 7 \\\\1&amp;2&amp; 2 \\\\2&amp;4&amp; 6 \\\\2&amp;6&amp; 8 \\\\2&amp;6&amp; 8 \\\\2&amp;5&amp;10 \\\\2&amp;5&amp;10 \\\\2&amp;4&amp; 6 \\\\\\end{bmatrix} = \\begin{bmatrix} 1&amp; 1&amp; 1&amp; 2&amp; 2&amp; 2&amp; 2&amp; 2&amp; 2 \\\\ 1&amp; 3&amp; 2&amp; 4&amp; 6&amp; 6&amp; 5&amp; 5&amp; 4 \\\\ 3&amp; 7&amp; 2&amp; 6&amp; 8&amp; 8&amp;10&amp;10&amp; 6 \\\\\\end{bmatrix}\\] 1.6 Matrix Addition and Subtraction We can add two matrices together if they are the same order (recall that order refers to the number of rows and columns). For example, if we added \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\), we can get matrix \\(\\mathbf{C}\\). \\[\\mathbf{A} = \\begin{bmatrix}2&amp;4 \\\\3&amp;5 \\\\\\end{bmatrix}\\] \\[\\mathbf{B} = \\begin{bmatrix} 6&amp;10 \\\\ 8&amp; 2 \\\\\\end{bmatrix}\\] \\[\\mathbf{C}= \\begin{bmatrix}2&amp;4 \\\\3&amp;5 \\\\\\end{bmatrix} + \\begin{bmatrix} 6&amp;10 \\\\ 8&amp; 2 \\\\\\end{bmatrix} = \\begin{bmatrix} 8&amp;14 \\\\11&amp; 7 \\\\\\end{bmatrix}\\] This is because \\(2 + 6 = 8\\) for Cell [1,1] of \\(\\mathbf{C}\\), \\(3 + 8 = 11\\) for the Cell [1,2], \\(4 + 10 = 14\\) for the Cell [2,1], and \\(5 + 2 = 7\\) for the Cell [2,2]. The same procedure works with subtraction. \\[\\begin{bmatrix}2&amp;4 \\\\3&amp;5 \\\\\\end{bmatrix} - \\begin{bmatrix} 6&amp;10 \\\\ 8&amp; 2 \\\\\\end{bmatrix} = \\begin{bmatrix}-4&amp;-6 \\\\-5&amp; 3 \\\\\\end{bmatrix}\\] 1.7 Operations on Matrices with a Scalar We can also perform operations on a matrix with a scalar. We could, for instance, let’s take a vector (a one column matrix) and subtract a scalar from it. For example, let’s subtract the mean of sa (which is 4) from each score, to get a vector of deviation scores (or centered scores). \\[\\begin{bmatrix}1\\\\3\\\\2\\\\4\\\\6\\\\6\\\\5\\\\5\\\\4 \\end{bmatrix} - 4 = \\begin{bmatrix}-3\\\\-1\\\\-2\\\\ 0\\\\ 2\\\\ 2\\\\ 1\\\\ 1\\\\ 0 \\end{bmatrix}\\] Similarly, we can multiply by a scalar. For example, we can multiply \\(\\mathbf{B}\\) by 2: \\[2\\mathbf{B}= \\begin{bmatrix} 6&amp;10 \\\\ 8&amp; 2 \\\\\\end{bmatrix} \\times 2 = \\begin{bmatrix}12&amp;20 \\\\16&amp; 4 \\\\\\end{bmatrix}\\] 1.7.1 Sum-of-square-and-cross-product (SSCP) matrices In ANOVA, the sum of the squared deviations (or sums of squares, SS, for short) is often used. The SS is the numerator in the calculation of the variance, \\[\\frac{\\Sigma(y_i-\\bar{y})^2}{n-1}\\] This is useful because if our computer program outputs the variance, we can get the SS by multiplying the variance by the denominator (the degrees of freedom), \\[SS_y = \\frac{\\Sigma(y_i-\\bar{y})^2}{n-1} (n-1)\\] In MANOVA, we use the matrix version of SS, which is the sum-of-squares-and-cross-products (SSCP) matrix. With matrices, we can multiply each element by a single number (again, called a scalar) and get a new matrix.6 Here, we’re multiplying the variance by its degrees of freedom (which is \\(n-1\\)) to get the SSCP matrix. \\[\\mathbf{SSCP} = \\begin{bmatrix}3.00&amp;4.00 \\\\4.00&amp;7.75 \\\\\\end{bmatrix} \\times 8 = \\begin{bmatrix}24.00&amp;32.00 \\\\32.00&amp;62.00 \\\\\\end{bmatrix}\\] 1.7.2 Operations on elements of a matrix: Example with a correlation matrix Similar to scalar addition and subtraction, we can also perform operations on individual elements of a matrix. We might use this to get a correlation matrix from a covariance matrix. The formula for correlation between two variables is \\(r_{x,y} = \\frac{cov(x,y)} {s_xs_y}\\). We can extend this to multiple elements in a covariance matrix to calculate its corresponding correlation matrix. Here’s our covariance matrix from our data set: \\[\\mathbf{S} = \\begin{bmatrix}3.00&amp;4.00 \\\\4.00&amp;7.75 \\\\\\end{bmatrix}\\] The standard deviation of each of the two variables is the square root of its variance: \\[s_{y1} = \\sqrt{3.00} = 1.73\\] \\[s_{y2} = \\sqrt{7.75} = 2.78\\] Divide each element of our \\(\\mathbf{S}\\) matrix by the product of the standard deviations of the variables on the row and column. If this is on the diagonal, this is the same as the square.7 \\[\\mathbf{R} = \\begin{bmatrix} \\frac{3.00}{1.73^2} \\quad \\frac{4.00}{1.73 \\times2.78} \\\\[.5em] \\frac{4.00}{1.73 \\times2.78} \\quad \\frac{7.75}{2.78^2} \\end{bmatrix} = \\begin{bmatrix}1.00&amp;0.83 \\\\0.83&amp;1.00 \\\\\\end{bmatrix}\\] 1.8 Matrix Multiplication Matrix multiplication is very different from multiplying a matrix by a scalar or performing individual operations on the elements. When we multiply two matrices together, we multiply the elements of the row of the first matrix by the elements of the column of the second matrix and sum them up to a single number, which populates a single cell in the resulting matrix. We then multiply the elements of the next row in the first matrix by the elements in the first column in the second matrix and populate the next cell in the resulting matrix, and so forth. Let’s use a bare-bones example to illustrate this, with \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\), which we had used above with matrix addition: \\[\\mathbf{AB} = \\begin{bmatrix}2&amp;4 \\\\3&amp;5 \\\\\\end{bmatrix} \\begin{bmatrix} 6&amp;10 \\\\ 8&amp; 2 \\\\\\end{bmatrix} = \\begin{bmatrix}44&amp;28 \\\\58&amp;40 \\\\\\end{bmatrix}\\] The first cell of \\(\\mathbf{AB}\\) is calculated as \\((2 \\times 6) + (4 \\times 8) = 44\\). In this procedure, the \\(2\\) is from Cell \\([1,1]\\) of the first matrix, the \\(6\\) is from Cell \\([1,1]\\) of the second matrix. Then, the \\(4\\) is from Cell \\([1,2]\\) of the first matrix, while the \\(8\\) is from Cell \\([2,1]\\) of the second matrix. The sum of these products, \\(44\\), goes in Cell \\([1,1]\\) of matrix \\(\\mathbf{AB}\\). Row 2 and Column 1 of \\(\\mathbf{AB}\\) is calculated as \\((3 \\times 6) + (5 \\times 8) = 58\\). Row 1 and Column 2 of \\(\\mathbf{AB}\\) is calculated as \\((2 \\times 10) + (4 \\times 2) = 28\\). Row 2 and Column 2 of \\(\\mathbf{AB}\\) is calculated as \\((3 \\times 10) + (5 \\times 2) = 40\\). This procedure only succeeds when the number of columns in the first matrix is the same as the number of rows in second matrix. 1.8.1 Another example of matrix multiplication, to get the SSCP Another way to get the \\(\\mathbf{SSCP}\\) matrix is to first calculate the deviation scores, \\(x_{ij} - \\bar{x}_j\\), as we did above when we subtracted the mean (4) from the vector sa. If we did this for the other vector, csa, we can then stick them together into a matrix of deviation scores. Then, we can use matrix multiplication of the transpose of this matrix with itself. Arbitrarily, we’ll label this matrix of deviation scores as \\(\\mathbf{C}\\).8 \\[\\mathbf{SSCP} = \\mathbf{C^{\\prime}C}\\] \\[\\mathbf{SSCP} = \\begin{bmatrix}-3.00&amp;-1.00&amp;-2.00&amp; 0.00&amp; 2.00&amp; 2.00&amp; 1.00&amp; 1.00&amp; 0.00 \\\\-3.67&amp; 0.33&amp;-4.67&amp;-0.67&amp; 1.33&amp; 1.33&amp; 3.33&amp; 3.33&amp;-0.67 \\\\\\end{bmatrix} \\begin{bmatrix}-3.00&amp;-3.67 \\\\-1.00&amp; 0.33 \\\\-2.00&amp;-4.67 \\\\ 0.00&amp;-0.67 \\\\ 2.00&amp; 1.33 \\\\ 2.00&amp; 1.33 \\\\ 1.00&amp; 3.33 \\\\ 1.00&amp; 3.33 \\\\ 0.00&amp;-0.67 \\\\\\end{bmatrix} = \\begin{bmatrix}24.00&amp;32.00 \\\\32.00&amp;62.00 \\\\\\end{bmatrix}\\] Notice that the order of the resulting matrix is \\(2 \\times 2\\). This is because there were two rows in the first matrix, \\(\\mathbf{C^\\prime}\\), and two columns in the second one, \\(\\mathbf{C}\\). This is the kind of operation we encounter a lot in multivariate statistics. The sum of the squared deviation scores of the sa variable is calculated using the top row of the first matrix, \\(\\mathbf{C^\\prime}\\), and the first column of the second matrix, \\(\\mathbf{C}\\). We’re squaring and summing. When we use the top row of the first matrix and the second column of the second matrix, we are taking the sum of the cross products of sa and csa. 1.8.2 The ordering is important In matrix multiplication, the order is important. Above, we saw the result of \\(\\mathbf{A} \\times \\mathbf{B}\\): \\[\\mathbf{AB} = \\begin{bmatrix}2&amp;4 \\\\3&amp;5 \\\\\\end{bmatrix} \\begin{bmatrix} 6&amp;10 \\\\ 8&amp; 2 \\\\\\end{bmatrix} = \\begin{bmatrix}44&amp;28 \\\\58&amp;40 \\\\\\end{bmatrix}\\] This is a different result from \\(\\mathbf{B} \\times \\mathbf{A}\\): \\[\\mathbf{BA} = \\begin{bmatrix} 6&amp;10 \\\\ 8&amp; 2 \\\\\\end{bmatrix} \\begin{bmatrix}2&amp;4 \\\\3&amp;5 \\\\\\end{bmatrix} = \\begin{bmatrix}42&amp;74 \\\\22&amp;42 \\\\\\end{bmatrix}\\] In the same way, \\(\\mathbf{C^{\\prime}C} \\ne \\mathbf{CC^{\\prime}}\\). 1.9 Inverse of a Matrix It is not possible to divide matrices. Instead, we multiply one matrix by the inverse of another matrix. The inverse is symbolized as a superscript \\(^-1\\) on the matrix. The inverse is analogous to the reciprocal in regular math. For example, the reciprocal of 4 is \\(1 \\div 4 = .25\\), \\[4^{-1} = \\frac{1}{4} = 0.25\\] In matrix algebra, inverting a matrix involves several steps.9 For this procedure, we can rely on software. For example, in R, we can use the solve() function. In SPSS, we can use the INV() function in the syntax editor. Here is the inverse of a matrix, \\(\\mathbf{S}\\), which is symbolized as \\(\\mathbf{S^{-1}}\\): \\[\\mathbf{S} = \\begin{bmatrix}3.00&amp;4.00 \\\\4.00&amp;7.75 \\\\\\end{bmatrix}\\] \\[\\mathbf{S^{-1}} = \\begin{bmatrix} 1.07&amp;-0.55 \\\\-0.55&amp; 0.41 \\\\\\end{bmatrix}\\] If we multiplied \\(\\mathbf{B}\\) by this inverted \\(\\mathbf{S}\\) matrix, we have something analogous to dividing \\(\\mathbf{B}\\) by \\(\\mathbf{S}\\): \\[\\mathbf{BS^{-1}} = \\begin{bmatrix} 6&amp;10 \\\\ 8&amp; 2 \\\\\\end{bmatrix} \\begin{bmatrix} 1.07&amp;-0.55 \\\\-0.55&amp; 0.41 \\\\\\end{bmatrix} = \\begin{bmatrix} 0.90&amp; 0.83 \\\\ 7.45&amp;-3.59 \\\\\\end{bmatrix}\\] We’ll see this when we calculate test-statistic ratios for our hypothesis tests. This is a generalization of univariate test statistics. For example, with ANOVA our F-statistic is with individual numbers, such as \\(F = MS_b / MS_w\\), which is the same as \\(F = MS_b \\times MS_w^{-1}\\). With MANOVA, we cannot divide matrices, but we can multiply by inverses, such as \\(\\mathbf{B}\\mathbf{W^{-1}}\\). 1.10 Matrix determinants, as generalized variance The determinant of a square matrix is a single number that represents the generalized variance of all the data in our multivariate matrix. It is one way to represent the amount of variability in a set of variables after removing their shared variability. This is handy because in ANOVA, we analyze the variance of the Between and Within portions of our single dependent variable. In MANOVA, the variance includes the variances and covariances of multiple dependent variables. For this we use the generalized variance of the Between and Within portions of our data. It is symbolized as \\(\\text{det}(\\mathbf{X})\\) or \\(|\\mathbf{X}|\\). It is best to let the computer calculate this. In SPSS, we can use the DET() function in the syntax editor; in R, it is det(). The determinant of our, \\(\\mathbf{S}\\) matrix, \\(|\\mathbf{S}|\\), is \\(7.25\\). Here’s \\(\\mathbf{S}\\), which above was our covariance matrix of our small data set: \\[\\mathbf{S} = \\begin{bmatrix}3.00&amp;4.00 \\\\4.00&amp;7.75 \\\\\\end{bmatrix}\\] \\[|\\mathbf{S}| = 7.25\\] For this simple matrix, we could calculate this by hand but with higher-order matrices, this gets tedious very quickly.10 We will encounter the determinant when we examine how the test statistic for MANOVA is calculated. Knowledge of its existence also becomes useful when we encounter errors in our software output when we try to conduct some of the advance multivariate methods, such as confirmatory factor analysis.11 1.11 Eigenvectors and Eigenvalues We will address eigenvectors and eigenvalues in this course (but we will not calculate them, as that goes beyond our needs). We can think of eigenvalues as individual numbers that represent the variance of restructured linear combinations of the dependent variables. Eigenvectors are, not surprisingly, vectors of numbers. Each element in the vector serves as a weight on a variable in a manner similar to regression coefficients. The result (analogous to a dependent variable in regression) of this linear combination of weighted variables goes by many names, including composite, component, discrimination function, and so forth. Each observation (each person) in a data set has a score on the component. The variance of each component of scores is the eigenvalue. Many of the test statistics in multivariate methods use eigenvalues. Eigenvectors and eigenvalues come up especially in principal components analysis but are actually used under the hood in almost all of the multivariate statistics in this class.12 We’ll address eigenvalues and eigenvectors using a visual representation. For now, just be aware that these terms exist. 1.12 Summary Here are some of the terms we have encountered. scalars are individual numbers. Usually these are lowercase and not boldfaced. vectors are columns of numbers. Usually these are lowercase and boldfaced, as in \\(\\mathbf{a}\\). matrices are rectangular or square, with data in each cell. Usually these are uppercase and boldfaced, as in \\(\\mathbf{A}\\). order of a matrix is its dimensionality, with rows first and columns second. elements are individual numbers in a matrix or vector. diagonal is all the elements from the top left to the bottom right of a matrix. trace is the sum of all the elements in a diagonal. transposed matrices are pushed over so their rows become columns and columns become rows. The symbol is a prime or a superscript T after the matrix, as in \\(\\mathbf{X^\\prime}\\) or \\(\\mathbf{X}^\\intercal\\). matrix multiplication is a pain in the neck but is the most common operation we see in multivariate statistics. inverse matrices are used when we want to divide one matrix by another. It is symbolized with a superscript \\(-1\\) after the matrix, as in \\(\\mathbf{X^{-1}}\\). determinant of a matrix, symbolized as \\(\\text{det}(\\mathbf{X})\\) or \\(|\\mathbf{X}|\\), is the generalized variance of a matrix. eigenvectors and eigenvalues are are used in the process of creating linear composites of the dependent variables in a multivariate analysis. 1.13 Optional: SPSS and R Code to Manipulate Matrices This part is optional and for the curious. Here is some code to manually perform some matrix operations. This is using our two dependent variables, sa and csa from our data set, which we’re labeling as y1 and y2 in the code. Table 1.2: Our data set id grp sa csa 1 1 1 3 2 1 3 7 3 1 2 2 4 2 4 6 5 2 6 8 6 2 6 8 7 2 5 10 8 2 5 10 9 2 4 6 Here is the R code: # There are many ways to make a matrix. One is to use the `matrix()` function. # In this example, we have the data in two columns. We can use the # `byrow = TRUE` argument to tell R to fill in the matrix by rows. # The `ncol = 2` argument defines the order as an n x 2 matrix. Y &lt;- matrix(c(1, 3, 3, 7, 2, 2, 4, 6, 6, 8, 6, 8, 5, 10, 5, 10, 4, 6), byrow = TRUE, ncol = 2) Y ## Prints to console # Another way to create a matrix is by binding columns of equal length data. # Here, we&#39;re calculating the deviation scores of each variable, then # sticking them together with `cbind()`. y1 &lt;- c(1, 3, 2, 4, 6, 6, 5, 5, 4) y2 &lt;- c(3, 7, 2, 6, 8, 8,10,10, 6) C &lt;- cbind(y1 - mean(y1), y2 - mean(y2) ) C # Use `%*%` for matrix multiplication. Use `t()` for transposing. SSCP &lt;- t(C) %*% C SSCP # To get the covariance matrix from SSCP, we divide each element by n - 1. # In other words, we&#39;re dividing the matrix by a scalar. S &lt;- SSCP / (nrow(Y) - 1) S # If we wanted to calculate the determinant, we use the `det()` function: det(S) # If we wanted the inverse of the matrix, we use the `solve()` function: solve(S) Here is the SPSS code to accomplish the same thing. /* Tell SPSS that the following will use matrix mode. MATRIX. /* Creating two vectors, using semicolons between numbers to make them vertical. COMPUTE y1 = {1; 3; 2; 4; 6; 6; 5; 5; 4}. COMPUTE y2 = {3; 7; 2; 6; 8; 8;10;10; 6}. /* Now subtracting the mean from the vectors to get mean-centered vectors. COMPUTE c1 = y1 - 4. COMPUTE c2 = y2 - 6.666667. /* Sticking the two vertical vectors together into a matrix. COMPUTE C = {c1 , c2}. /* Calculating the SSCP using matrix multiplication of the transpose of C and C. COMPUTE tranC = TRANSPOS(C). COMPUTE SS_CP = tranC * C. /* Dividing the SSCP by n-1 to get the covariance matrix. COMPUTE S = SS_CP / 8. /* Computing the determinant and the inverse of S. COMPUTE DETS = DET(S). COMPUTE INVS = INV(S). /* Print the results. PRINT C. PRINT TranC. PRINT SS_CP /title = &quot;SSCP Matrix&quot;. PRINT S /title = &quot;Covariance Matrix S&quot;. PRINT DETS /title = &quot;Determinant of S&quot;. PRINT INVS /title = &quot;Inverse of S&quot;. /* Close the matrix mode. END MATRIX. Strictly speaking, a matrix has all of the columns in the same type of format, such as all numeric, whereas our data files often have one column in one format, such as character (or text) for persons’ names, and other columns in a different format, such as numeric.↩︎ Terminology in statistics is not always so neat and clean. In other contexts, we reserve upper case N for the number of units in a population, whereas here, it represents the total number of units in the sample regardless of which group they are in. The lowercase n is used to represent the number of units in a group.↩︎ I know, right? Why don’t we just call it dimensions instead of order.↩︎ Unfortunately, some authors such as Tabachnick and Fidell, annoyingly turn around and also label the sum-of-squares-and-cross-products (SSCP) matrix as \\(\\mathbf{S}\\)—which seems like a concerted effort to confuse the bananas out of us when we’re learning this material. However, this is to avoid the confusion of assuming that the symbol \\(\\mathbf{SSCP}\\) represents the result of the matrix multiplication of four matrices, \\(\\mathbf{S} \\times \\mathbf{S} \\times \\mathbf{C} \\times \\mathbf{P}\\). For this reason, we try to use single letters to represent matrices and vectors.↩︎ This is easy to see if we think of the cross-products of a variable with itself as the square. If we use the formula for covariance between two variables (say, \\(x\\) and \\(y\\)), \\(Cov(x,y) = \\frac{\\Sigma(x-\\bar{x})(y-\\bar{y})}{n-1}\\), and replace \\(y\\) with \\(x\\), we get the formula for the variance of x, \\(Cov(x,x) = \\frac{\\Sigma(x-\\bar{x})(x-\\bar{x})}{n-1} = \\frac{\\Sigma(x-\\bar{x})^2}{n-1} = Var(x)\\).↩︎ This is not matrix multiplication, as we’re simply rescaling each element in the matrix by the scalar.↩︎ If you’re comfortable manipulating matrices, you can also try a more general approach to getting \\(\\mathbf{R}\\) from \\(\\mathbf{S}\\): Save these standard deviations as the diagonal of a new \\(p \\times p\\) matrix with zeros on the off diagonal, call it \\(\\mathbf{D}\\) (for convenience), then use matrix algebra with its inverse, \\(\\mathbf{D^{-1}}\\) (which is introduced below), and the covariance matrix, \\(\\mathbf{S}\\) using this formula: \\(\\mathbf{R} = \\mathbf{D^{-1}} \\mathbf{S}\\mathbf{D^{-1}}\\).↩︎ Maybe \\(\\mathbf{C}\\) is not a bad label after all given that subtracting the vector’s mean from each element results in a vector of mean centered scores.↩︎ One of the steps in matrix inversion involves dividing the cells of a matrix of cofactors (which we do not need to address) by the determinant of the matrix. When we have a determinant that is zero, we will get an undefined matrix.↩︎ With a \\(2 \\times 2\\) matrix, like this, the determinant is the product of the diagonal minus the product of the antidiagonal, in this case, it is \\((3 \\times 7.75) - (4 \\times 4) = 23.25 - 16 = 7.25\\). With a simple \\(2 \\times 2\\) covariance matrix, this is the sum of the product of the variances minus the sum of the product of the covariances. With a \\(3 \\times 3\\), it’s more tedious and we should forget trying to do this by hand with a \\(4 \\times 4\\) matrix.↩︎ For example, if we see an error that says our matrix was not positive definite, it means our determinant is negative. When this is negative or zero, and we want to compute generalized variance (of a residual matrix, e.g.), our computer software complains because variance should not be negative.↩︎ The exception will be confirmatory factor analysis.↩︎ "],["hotellings-t2.html", "2 Hotelling’s \\(T^2\\) 2.1 Within-group covariance matrices 2.2 SSCP matrices 2.3 Conducting the Test using Functions", " 2 Hotelling’s \\(T^2\\) Here is our data set, again. Let’s make the grp variable a factor, representing two different types of treatment, behavior modification and cognitive method. The dependent variables are sa (client satisfaction) and csa (client self acceptance). dat &lt;- data.frame( id = 1:9, grp = factor(c(rep(&quot;behavmod&quot;, 3), rep(&quot;cogmethod&quot;,6))), sa = c(1, 3, 2, 4, 6, 6, 5, 5, 4), csa = c(3, 7, 2, 6, 8, 8,10,10, 6)) Table 2.1: Our data set id grp sa csa 1 behavmod 1 3 2 behavmod 3 7 3 behavmod 2 2 4 cogmethod 4 6 5 cogmethod 6 8 6 cogmethod 6 8 7 cogmethod 5 10 8 cogmethod 5 10 9 cogmethod 4 6 2.1 Within-group covariance matrices In ANOVA, we are often interested in partitioning the variance into the between-groups variance and the within-groups variance. The between-groups variance represents how much the groups differ from each other on the variable of interest whereas the within-group variance, or residual variance, represents how much of the variance our statistical model does not explain. In MANOVA, we use covariance matrices for this same approach. In our example, we have two dependent variables, sa and csa, and one independent variable, grp. Let’s obtain the within-group covariance. Basically, we’re splitting the data by group and then obtaining the covariance matrix: # Separately by group: (cov_W1 &lt;- cov(subset(dat, grp == &quot;behavmod&quot;)[,c(&quot;sa&quot;, &quot;csa&quot;)]) ) ## sa csa ## sa 1 2 ## csa 2 7 (cov_W2 &lt;- cov(subset(dat, grp == &quot;cogmethod&quot;)[,c(&quot;sa&quot;, &quot;csa&quot;)]) ) ## sa csa ## sa 0.8 0.8 ## csa 0.8 3.2 2.2 SSCP matrices In ANOVA, the sum of the squared deviations (or sums of squares, SS, for short) is often used. The SS is the numerator in the calculation of the variance, \\(\\frac{\\Sigma(y_i-\\bar{y})^2}{n-1}\\), so if our computer program outputs the variance, we can get the SS by multiplying the variance by the denominator (the degrees of freedom), \\(SS_y = \\frac{\\Sigma(y_i-\\bar{y})^2}{n-1} (n-1)\\) In MANOVA, we use the matrix version of SS, which is the sum-of-squares-and-cross-products (SSCP) matrix. With matrices, we can multiply each element by a single number (also called a scalar) and get a new matrix. Here, we’re multiplying each group’s within-group variance by its degrees of freedom (which is \\(n-1\\) for each group) to get each group’s SSCP matrix.13 n_1 &lt;- nrow(subset(dat, grp == &quot;behavmod&quot;)) n_2 &lt;- nrow(subset(dat, grp == &quot;cogmethod&quot;)) (SSCP_W1 &lt;- cov_W1 * (n_1 - 1) ) ## sa csa ## sa 2 4 ## csa 4 14 (SSCP_W2 &lt;- cov_W2 * (n_2 - 1) ) ## sa csa ## sa 4 4 ## csa 4 16 \\[\\mathbf{W_1} = \\begin{bmatrix} 2.00&amp; 4.00 \\\\ 4.00&amp;14.00 \\\\\\end{bmatrix}\\] \\[\\mathbf{W_2} = \\begin{bmatrix} 4.00&amp; 4.00 \\\\ 4.00&amp;16.00 \\\\\\end{bmatrix}\\] These are the same within-group matrices reported on p. 148 of Pituch and Stevens (2016). The pooled within SSCP is the sum of these two matrices, which is in the SPSS output and labeled as error SSCP: SSCP_W1 + SSCP_W2 ## sa csa ## sa 6 8 ## csa 8 30 In MANOVA, the error term is the pooled within-Covariance matrix, which is the SSCP divided by the degrees of freedom, as displayed on p. 149 of Pituch and Stevens (2016). (S &lt;- (SSCP_W1 + SSCP_W2) / (n_1 + n_2 - 2) ) ## sa csa ## sa 0.8571429 1.142857 ## csa 1.1428571 4.285714 In matrix format, and rounded to two decimals, it is displayed like this: \\[\\mathbf{S} = \\begin{bmatrix}0.86&amp;1.14 \\\\1.14&amp;4.29 \\\\\\end{bmatrix}\\] This error matrix represents the noise in the signal-to-noise ratio that is used in a test statistic, such as in a t-test in univariate tests. In other words, it is the denominator. In our example, we have two groups in the independent variable and two dependent variables. The signal is the difference between the two groups on their mean scores on each of the two dependent variables, sa and csa. However, unlike the critical ratio in univariate tests, we cannot have a matrix in the denominator because we cannot divide by a matrix. Instead, we use the inverse and use matrix multiplication. Pituch and Stevens (p. 145) illustrate the relationship between the univariate t-test and Hotelling’s \\(T^2\\), where the univariate t statistic is calculated as \\[t = \\frac{\\bar{y}_1 - \\bar{y}_2}{\\sqrt{\\frac{(n_1-1)s^2_1 + (n_2-1)s^2_2 }{n_1 + n_2 - 2} }\\left( \\frac{1}{n_1} + \\frac{1}{n_2}\\right)} \\text{,} \\] which can be algebraically manipulated to this, after it is squared: \\[t^2 = \\frac{n_1n_2}{n_1 + n_2}(\\bar{y}_1 - \\bar{y}_2)(s^2)^-1(\\bar{y}_1 - \\bar{y}_2)\\] This allows us to see how the univariate test is similar to its multivariate counterpart, Hotelling’s \\(T^2\\): \\[T^2 = \\frac{n_1n_2}{n_1 + n_2}(\\mathbf{\\bar{y}_1 - \\bar{y}_2)^\\prime{}}\\mathbf{S^{-1}}(\\mathbf{\\bar{y}_1 - \\bar{y}_2)}\\] which is more succinctly presented as \\[T^2 = k\\mathbf{d}^{\\prime}\\mathbf{S^{-1}}\\mathbf{d}\\] library(tidyverse) m_bygrp &lt;- dat %&gt;% group_by(grp) %&gt;% summarize(m_y1 = mean(sa), m_y2 = mean(csa) ) m_bygrp ## # A tibble: 2 × 3 ## grp m_y1 m_y2 ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 behavmod 2 4 ## 2 cogmethod 5 8 # using a fancy function to transpose the table # install.packages(&quot;sjmisc&quot;) library(sjmisc) m_bygrp &lt;- m_bygrp %&gt;% rotate_df(cn = TRUE) %&gt;% mutate(m_diff = behavmod - cogmethod) k &lt;- (n_1*n_2)/(n_1 + n_2) d &lt;- m_bygrp$m_diff d # vector of mean differences ## [1] -3 -4 We can rely on software to calculate the inverse of the within-variance matrix: solve(S) ## sa csa ## sa 1.8103448 -0.4827586 ## csa -0.4827586 0.3620690 \\[\\mathbf{S^{-1}} = \\begin{bmatrix} 1.81&amp;-0.48 \\\\-0.48&amp; 0.36 \\\\\\end{bmatrix}\\] Here is our calculation of Hotelling’s \\(T^2\\) test statistic:14 T_sq &lt;- k * t(d) %*% solve(S) %*% d T_sq ## [,1] ## [1,] 21 We can use Hotelling’s formula for the exact F from a \\(T^2\\) to estimate the F test statistic, which we can subsequently test against our critical F with \\(df_1 = p\\) (i.e., the number of dependent variables), and \\(df_2 = N - p - 1\\). \\[F = \\frac{n_1 + n_2 - p - 1}{(n_1 + n_2 - 2)p}T^2 = 21\\] p &lt;- 2 # number of DVs ( F_exact &lt;- (n_1 + n_2 - p - 1) / ((n_1 + n_2 - 2)*p) * T_sq ) ## [,1] ## [1,] 9 (F_crit &lt;- qf(p = .05, # alpha for critical value of F df1 = p, df2 = (n_1 + n_2 - p - 1), lower.tail = FALSE)) ## [1] 5.143253 F_exact &gt; F_crit # If TRUE, then we can reject null hypothesis at alpha. ## [,1] ## [1,] TRUE 2.3 Conducting the Test using Functions To avoid making coding mistakes with all of those calculations, we can use functions to conduct the tests. Here is one specifically for Hotelling’s \\(T^2\\) from the Hotelling package and another using the manova() function in Base R.15 # install.packages(&quot;Hotelling&quot;) library(Hotelling) # saving the dependent variable data as a matrix dvs &lt;- as.matrix(dat[ , c(&quot;sa&quot;,&quot;csa&quot;)] ) hot_mod &lt;- hotelling.test(x = dvs[1:3, ], y = dvs[4:9, ] ) hot_mod ## Test stat: 21 ## Numerator df: 2 ## Denominator df: 6 ## P-value: 0.01562 dvs &lt;- as.matrix(dat[ , c(&quot;sa&quot;,&quot;csa&quot;)] ) man_mod &lt;- manova(dvs ~ grp, data = dat) summary(man_mod, test = &quot;Hotelling&quot; ) ## Df Hotelling-Lawley approx F num Df den Df Pr(&gt;F) ## grp 1 3 9 2 6 0.01563 * ## Residuals 7 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 There are four test statistics we could report from a MANOVA, Pillais’s trace, Wilks’ lambda, Hotelling’s trace, and Roy’s largest root. Because we’re only examining two groups, all of the tests yield the same statistical significance and statistical conclusion. Typically Wilks’ lambda is reported. Because we’re interested in Hotelling’s \\(T^2\\), we can report the Hotelling’s trace. The test-statistic is \\(3\\), which differs from the Hotelling’s \\(T^2\\) test statistic of 21, but which is directly related to it by a factor of \\(N - k\\), where \\(N\\) is the total sample size and \\(k\\) is the number of groups. With our example, given our test statistic of Hotelling-Lawley \\(= 3\\), Hotelling’s \\(T^2 = 3(9 - 2) = 21\\). In R, the nrow() function can be used to count the number of observations in the data frame.↩︎ In R, we use %*% for matrix multiplication.↩︎ The hotelling.test() function has a var.equal = argument, which is handy if we do not meet the assumption of equal variances and we want to use robust SE for a more cautious test of statistical significance (set var.equal = FALSE). ↩︎ "],["planned-comparisons-and-contrasts.html", "3 Planned Comparisons and Contrasts 3.1 Data from Week 1 3.2 Orthogonal contrasts 3.3 Non-orthogonal contrasts", " 3 Planned Comparisons and Contrasts We use planned comparisons when we have research questions about specific group differences. If we’re simply exploring, we might use post-hoc comparisons, which are useful if we do not have specific research questions about group differences. The term comparisons can encompass both post hoc comparisons and planned contrasts. The term contrasts usually refers to the numerically weighted comparisons. There are two types of planned contrasts we should think about: Orthogonal and non-orthogonal. We’ll look at orthogonal contrasts first and then non-orthogonal. Even though orthogonal are better, we should let our research question guide us in which contrasts we use. 3.1 Data from Week 1 Here is our data set, again. We have three variables, described in the Week 1 slides. dat &lt;- data.frame( person = 1:15, treatment = factor(rep( c(&quot;Placebo&quot;, &quot;Low dose&quot;, &quot;High dose&quot;), each = 5), levels = c(&quot;Placebo&quot;, &quot;Low dose&quot;, &quot;High dose&quot;)), waketime = c(3, 2, 1, 1, 4, 5, 2, 4, 2, 3, 7, 4, 5, 3, 6)) After we look at the descriptive statistics and assumptions, we can set up our analysis. Let’s get right to the contrast coding part: We can assign contrast codes to the categorical variable of interest. Let’s say we had two research questions: (a) whether any form of dosage was better than the placebo, and (b) whether there was a difference in the two dosages. We can look at the ordering of our levels in our variable, which we’ll use when we create corresponding planned contrasts: levels(dat$treatment) ## [1] &quot;Placebo&quot; &quot;Low dose&quot; &quot;High dose&quot; We can create a vector for each contrast and assign them to an object. In the previous code, I used the generic contrast1 as the object name. Here, let’s name the objects so we can remember what two groupings we’re comparing: PvLH &lt;- c(-2, 1, 1) LvH &lt;- c(0, -1, 1) Next, we assign these contrasts to the contrasts attribute of the variable in our data set. We use the cbind() function because this is the format the attribute requires; we’re taking the two vectors and binding them together as columns. contrasts(dat$treatment) &lt;- cbind(PvLH, LvH) We can take a peek at the attributes of our variable, where the contrasts are stored: attributes(dat$treatment) ## $levels ## [1] &quot;Placebo&quot; &quot;Low dose&quot; &quot;High dose&quot; ## ## $class ## [1] &quot;factor&quot; ## ## $contrasts ## PvLH LvH ## Placebo -2 0 ## Low dose 1 -1 ## High dose 1 1 we can also isolate this attribute if we want to perform operations on it (which we will): attributes(dat$treatment)$contrasts ## PvLH LvH ## Placebo -2 0 ## Low dose 1 -1 ## High dose 1 1 3.2 Orthogonal contrasts We use the term orthogonal to mean independent. In this case, we’re partitioning the variance in the dependent variable into independent parts. This means that our Type I error rate stays at our pre-specified alpha level, \\(.05\\), all else being equal.16 3.2.1 Conditions for orthogonal contrasts To have orthogonal contrasts, we need to satisfy these two conditions: The codes within any contrast must sum to zero. The sum of the products of the codes for each group must be zero. Let’s check these two conditions. First, we can look at the sum of the contrasts within each of our two contrasts. This part is easy. For the first one, \\(-2 + 1 + 1 = 0\\) and for the second one, \\(0 + -1 + 1 = 0\\). We can use the apply() function here. This has three arguments: The data (the first argument), the margin, and the function. The FUN = argument specifies which function to use with the data. Here, we’re using sum(). The MARGIN = function specifies which margin to apply this to. I remember this as being in the same order that we use in indexing—the first margin is the rows; the second is the columns. Here, we’re performing the sum() function on the columns (the margin would be a column score): apply(attributes(dat$treatment)$contrasts, MARGIN = 2, # sum() on the columns (across the rows) FUN = sum) ## PvLH LvH ## 0 0 Cool beans. We’ve met that first condition. Let’s look at the sum of the products of the codes. This is the correction to the mistake I made in last week’s code. What we need to do is take the products of the weights within each group, then sum these products. If this is zero, we’re in good shape. # Here are the products of the codes, per group: apply(attributes(dat$treatment)$contrasts, MARGIN = 1, # calculate the product on the rows, across the columns FUN = prod) ## Placebo Low dose High dose ## 0 -1 1 Indeed, if we sum these up, we get zero. We could put this inside a sum() function: sum( apply(attributes(dat$treatment)$contrasts, MARGIN = 1, FUN = prod) ) ## [1] 0 We have an orthogonal contrast. Woohoo! Now, we don’t have to adjust for Type I error rates. 3.2.1.1 An analogy for these conditions Another way to think about this is that we are splitting up the variance so that we’re not reusing a piece of the variance. One analogy could be that we imagine the total variance in the dependent variable as a pizza and our groups as slices of that pizza (maybe with different levels of spice). We have three big slices, so we could cut up that variance in several ways. The rule for orthogonal contrasts is that we cannot isolate a group more than one time (which in turn would result in the second condition above not being met). With our first contrast, we taking the placebo as a single slice of pizza and we’re comparing it the other two slices combined. We have isolated the placebo group in the first contrast, so after we’ve done that, we can’t put it back on the pizza pan—we’ve accounted for its variance. With the second contrast, we’re taking individual pieces and comparing them, so each is isolated but that’s okay because we don’t have any more contrasts. Now we can fit our ANOVA model: a1 &lt;- aov(waketime ~ treatment, data = dat) summary(a1) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## treatment 2 20.13 10.067 5.119 0.0247 * ## Residuals 12 23.60 1.967 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Because our omnibus null hypothesis was rejected, we can look at the contrasts. Here’s one way in R to do this: summary.lm(a1) ## ## Call: ## aov(formula = waketime ~ treatment, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.0 -1.2 -0.2 0.9 2.0 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.4667 0.3621 9.574 5.72e-07 *** ## treatmentPvLH 0.6333 0.2560 2.474 0.0293 * ## treatmentLvH 0.9000 0.4435 2.029 0.0652 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.402 on 12 degrees of freedom ## Multiple R-squared: 0.4604, Adjusted R-squared: 0.3704 ## F-statistic: 5.119 on 2 and 12 DF, p-value: 0.02469 Here’s another way, which includes the Omnibus result and seems to be a more informative output. summary.aov(a1, split = list(treatment = list(&quot;Placebo vs Low &amp; High dose&quot; = 1, &quot;Low vs High dose&quot; = 2) )) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## treatment 2 20.13 10.067 5.119 0.0247 * ## treatment: Placebo vs Low &amp; High dose 1 12.03 12.033 6.119 0.0293 * ## treatment: Low vs High dose 1 8.10 8.100 4.119 0.0652 . ## Residuals 12 23.60 1.967 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #The `= 1` and `= 2` refer to their respective contrast order in the attributes. 3.3 Non-orthogonal contrasts If our research questions directed us to use a different contrast, such as whether the placebo group differed from the low-dose group and whether the placebo differed from the high-dose group, we can use this contrast coding: PvL &lt;- c(-1, 1, 0) PvH &lt;- c(-1, 0, 1) contrasts(dat$treatment) &lt;- cbind(PvL, PvH) attributes(dat$treatment)$contrasts ## PvL PvH ## Placebo -1 -1 ## Low dose 1 0 ## High dose 0 1 Notice that we have isolated the placebo group twice. We’ve taken out that slice of pizza and then put it back. Let’s check the conditions for orthogonality. As for summing the codes within each contrast, we’re okay—we meet the condition. apply(attributes(dat$treatment)$contrasts, MARGIN = 2, # sum() across the columns FUN = sum) ## PvL PvH ## 0 0 But, as for meeting the second condition, no dice!17 The sum of the products of the codes within group is not equal to zero. sum( apply(attributes(dat$treatment)$contrasts, MARGIN = 1, FUN = prod) ) ## [1] 1 Still, if this is our research question, we go with it and fit our model: a1 &lt;- aov(waketime ~ treatment, data = dat) The overall omnibus test is the same as before, of course. Let’s look at the contrasts: aov.summ.out &lt;- summary.aov(a1, split = list(treatment = list(&quot;Placebo vs Low dose&quot; = 1, &quot;Placebo vs High dose&quot; = 2) )) aov.summ.out ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## treatment 2 20.13 10.067 5.119 0.0247 * ## treatment: Placebo vs Low dose 1 2.50 2.500 1.271 0.2816 ## treatment: Placebo vs High dose 1 17.63 17.633 8.966 0.0112 * ## Residuals 12 23.60 1.967 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 3.3.1 Accounting for Type I error in non-orthogonal contrasts. Because the contrast is not orthogonal, I would use a Benjamini-Hochberg false-discovery-rate adjustment to the alpha. A false-discovery rate is the number of tests that are “discovered” but are false. In other words, of all the discoveries (i.e., of all the tests that we observe to have a p-value lower than our critical value of \\(\\alpha = .05\\)), what is the proportion that we would expect to be false. observed.pvalues &lt;- c(.2816, .0112) # Or, for reproducibility: observed.pvalues &lt;- aov.summ.out[[1]][2:3,5] # Benjamini-Hochberg false discovery rate adjustment: p.adjust(p = observed.pvalues, method = &quot;BH&quot;) ## Placebo vs Low dose Placebo vs High dose ## 0.28158387 0.02236722 What I don’t like about this output (not the procedure) is that it adjusts the p-value instead of the alpha. The statistical conclusion is the same, however—a p-value less than .05 can be used to reject the null hypothesis that the two groups being compared are the same. In this case, the statistical conclusion is the same as it would be if we had not made the adjustment—the second contrast between the placebo and high-dosage group is statistically significant. 3.3.1.1 Bonferroni—the more conservative pizza sauce A more conservative adjustment is the Bonferroni adjustment to the family-wise error rate. This adjusts for the probability that we will have at least one Type I error (beyond our alpha of .05 Type I error rate), in our set of tests in the family. With a few contrasts, this is useful; I would not use it if we have many contrasts because it can reduce our alpha levels to levels that are unrealistically low. With only two contrasts, the adjusted alpha is \\(\\frac{.05}{2} = .025\\). This is the new critical alpha, where those comparisons with observed p-values exceeding this would then fail to be rejected. With 20 contrasts, it would be much lower, \\(\\frac{.05}{20} = 0.0025\\). The p.adjust() function reports the so-called adjusted p-values instead of the statistical conclusion-based approach of observed \\(p &lt; .05\\). The statistical conclusions are correct, however, if we simply apply the adjusted \\(p &lt; .05\\) decision criterion. observed.pvalues &lt;- aov.summ.out[[1]][2:3,5] p.adjust(p = observed.pvalues, method = &quot;bonferroni&quot;) ## Placebo vs Low dose Placebo vs High dose ## 0.56316774 0.02236722 We arrive at the same statistical conclusion as we had with the Benjamini-Hochberg approach. 3.3.2 Post-hoc tests If we had no research questions about group comparisons (which would be bizarre) and wanted to explore all contrasts, we could use Tukey’s HSD: contrasts(dat$treatment) &lt;- NULL # clearing previous contrast codes a1 &lt;- aov(waketime ~ treatment, data = dat) TukeyHSD(a1) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = waketime ~ treatment, data = dat) ## ## $treatment ## diff lwr upr p adj ## Low dose-Placebo 1.0 -1.3662412 3.366241 0.5162761 ## High dose-Placebo 2.8 0.4337588 5.166241 0.0209244 ## High dose-Low dose 1.8 -0.5662412 4.166241 0.1474576 We now see all three possible comparisons. The Tukey HSD post-hoc comparisons automatically adjust the p-values reported. Note that this does not compare combined groups with individual groups. Type I error can still be a problem if assumptions are not met, especially statistical independence.↩︎ This idiomatic phrase means that we don’t get the result we want.↩︎ "],["week-03-statistical-assumptions-of-manova.html", "4 Week 03 Statistical Assumptions of MANOVA 4.1 Data 4.2 Descriptive statistics 4.3 Normality Assumption 4.4 Fitting the MANOVA Model 4.5 Robust One-way MANOVA 4.6 Summary", " 4 Week 03 Statistical Assumptions of MANOVA After assuming that the dependent variables are continuous, there are three statistical assumptions, which Pituch &amp; Stevens presented. Here, we address the assumptions of multivariate normality and homogeneity of covariance matrices among groups. 4.1 Data This is the data set discussed in Pituch and Stevens’ chapter for this week’s reading. According to them, “Our example comes from a study on the cost of transporting milk from farms to dairy plants” (Pituch &amp; Stevens, 2016, p. 230). Their groups (the gp variable) are two types of trucks: Gasoline and diesel fueled. The dependent variables are y1 for fuel costs; y2 for repairs costs, and y3 for capital costs. library(tidyverse) library(haven) # To read SPSS data files dat &lt;- read_sav(&quot;MILK.sav&quot;) # Changing the names of the variables to lowercase: names(dat) &lt;- tolower(names(dat)) dat$gp &lt;- haven::as_factor(dat$gp) # Let&#39;s add an id variable so we can identify cases. dat &lt;- data.frame(id = 1:nrow(dat), dat) glimpse(dat) # or use str(dat) ## Rows: 59 ## Columns: 5 ## $ id &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25… ## $ gp &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ y1 &lt;dbl&gt; 16.44, 7.19, 9.92, 4.24, 11.20, 14.25, 13.50, 13.32, 29.11, 12.68, 7.51, 9.90, 10.25, 11.… ## $ y2 &lt;dbl&gt; 12.43, 2.70, 1.35, 5.78, 5.05, 5.78, 10.98, 14.27, 15.09, 7.61, 5.80, 3.63, 5.07, 6.15, 1… ## $ y3 &lt;dbl&gt; 11.23, 3.92, 9.75, 7.78, 10.67, 9.88, 10.60, 9.45, 3.28, 10.23, 8.13, 9.13, 10.17, 7.61, … Here are the general steps in a MANOVA: Examine descriptive statistics and plots. Address the statistical assumptions. We will focus on multivariate normality and homogeneity of the covariance matrices among groups. Specify any planned comparisons and set the contrasts. Our data set has only two groups, so we won’t need this in this exampe. Fit the MANOVA model and test the omnibus null hypothesis. If the omnibus test is rejected, examine either (a) univariate ANOVAs or (b) discriminant function analysis. We’ll use univariate ANOVAs, but many scholars suggest doing a discriminant function analysis, which will be a topic in a future session for us. 4.2 Descriptive statistics We should examine if there is a relationship among the dependent variables. If there is zero, or very low, correlation among them, we should forget MANOVA and simply use ANOVA on each. If the correlations are too high, we risk multicollinearity and, in turn, problems in inverting the matrix when we conduct the multivariate test; Tabachnick and Fidell (2013) suggest correlations over .90 should be flagged. ycors &lt;- cor(dat[ , c(&quot;y1&quot;, &quot;y2&quot;, &quot;y3&quot;)]) round(ycors, 2) ## y1 y2 y3 ## y1 1.00 0.33 -0.06 ## y2 0.33 1.00 0.36 ## y3 -0.06 0.36 1.00 We see moderate correlations among two pairs but effectively no relationship between y1 and y3 (fuel costs and capital costs seem to be unrelated). These data are okay for a MANOVA. Similarly, we can examine plots among the three variables in case there is a non-linear relationship or there are outliers affecting the relationships. Here, we’re combining them into a single plot, but plot could be viewed independently if we don’t want to use the gridExtra package. py1 &lt;- dat %&gt;% ggplot(aes(x = y1, y = y2)) + geom_point() py2 &lt;- dat %&gt;% ggplot(aes(x = y1, y = y3)) + geom_point() py3 &lt;- dat %&gt;% ggplot(aes(x = y2, y = y3)) + geom_point() library(gridExtra) grid.arrange(py1, py2, py3, ncol=2, top = &quot;Dependent variables&quot;) It seems like the correlation between y1 and y2 might be affected by two outliers. Still, there are no obvious non-linear relationships. In future analyses, such as in factor analysis, we’ll also look at the assumption of linearity among the dependent variables. These bivariate plots provide some evidence of this assumption. We could also look at three-dimensional plots (which I don’t know how to do in R) to see the relationships among all three variables—in that case, we might see an American football shape representing some relationship among three variables. With four variables, my brain is unable to imagine such a plot but that’s what the assumption of linearity with multivariate data would entail. 4.2.1 By-group descriptive statistics We can examine descriptive statistics by group. We did this in our previous class with the psych package’s describeBy() function. library(psych) describeBy(dat$y1, group = dat$gp, data = dat) # Fuel costs ## ## Descriptive statistics by group ## group: 1 ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 36 12.22 4.8 11.15 11.61 3.22 4.24 29.11 24.87 1.71 3.68 0.8 ## ---------------------------------------------------------------------------- ## group: 2 ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 23 10.11 2.09 9.77 10.01 1.88 6.47 15.9 9.43 0.61 0.55 0.44 describeBy(dat$y2, group = dat$gp, data = dat) # Repair costs ## ## Descriptive statistics by group ## group: 1 ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 36 8.11 4.19 7.67 7.97 4.66 1.35 17.44 16.09 0.33 -0.95 0.7 ## ---------------------------------------------------------------------------- ## group: 2 ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 23 10.76 5.08 11.75 10.63 4.4 2.94 21.52 18.58 0.16 -0.94 1.06 describeBy(dat$y3, group = dat$gp, data = dat) # Capital costs ## ## Descriptive statistics by group ## group: 1 ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 36 9.59 3.74 9.62 9.41 3.68 3.28 17.59 14.31 0.33 -0.61 0.62 ## ---------------------------------------------------------------------------- ## group: 2 ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 23 18.17 6.83 17.45 17.81 5.56 5.99 35.18 29.19 0.52 -0.02 1.42 We see that the gasoline and diesel trucks (the two groups) differ in each of these three costs, especially in the third dependent variable. Our MANOVA will test whether the two types of trucks statistically-significantly differ in total costs while accounting for the relationships among these costs. We might ask why go to the trouble when we could simply calculate a composite total cost for each truck as the sum of the three variables. Taking the sum is one kind of linear combination of variables. MANOVA finds the optimal linear combination by re-combining the variance, which is the topic of discriminant analysis and principal components analysis. For now, let’s get back to the topic of statistical assumptions. 4.3 Normality Assumption 4.3.1 Univariate Normality Univariate normality is a necessary but insufficient for multivariate normality. We can explore normality of each dependent variable in each group.18 First, we could look for univariate outliers, per the example in the reading. After that, let’s examine the kurtosis and skew estimates and plots. At this point, we’re following Pituch and Stevens’ (and Tabachnick &amp; Fidell’s) approach to analyze assumptions of the observed dependent-variable data. The code and interpretations can also be used with the residual scores corresponding with each dependent variable, which is addressed later on in this code. I’d actually recommend simply skipping to doing these steps with the residual data, but I’ll follow Pituch and Stevens’ approach first. The primary reason scholars suggest using the observed data is to avoid temptation of looking at the model results, such as with summary(my.fitted.model), which in R we can avoid by simply not asking for the results even though we have fit the model. 4.3.1.1 Identifying univariate outliers within groups Pituch and Stevens suggested examining z-scores of each variable within each group to identify any plausible outliers. They suggested a z-score greater than 2.5 in magnitude would merit scrutiny, which is approximately equal to a two-tailed test at alpha = .01. This advice is different from that of Tabachnick and Fidell (2013), who suggest a criterion of 3.29, corresponding with an alpha = .001, but also qualify this by saying that we might change this threshold depending on the sample size, as a large sample will more likely yield outliers.19 To get standardized scores, we can use the scale() function for each variable separately.20 In the code below, we use the dplyr package to calculate the z-scores and create a dummy variable that indicates which are outliers based on the criterion Pituch and Stevens suggested. library(tidyverse) datz &lt;- dat %&gt;% group_by(gp) %&gt;% mutate(y1.z = scale(y1), y2.z = scale(y2), y3.z = scale(y3), outlier_uni = if_else(abs(y1.z) &gt; 2.5 | abs(y2.z) &gt; 2.5 | abs(y3.z) &gt; 2.5, 1, 0)) datz %&gt;% filter(outlier_uni == 1) ## # A tibble: 3 × 9 ## # Groups: gp [2] ## id gp y1 y2 y3 y1.z[,1] y2.z[,1] y3.z[,1] outlier_uni ## &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 9 1 29.1 15.1 3.28 3.52 1.67 -1.69 1 ## 2 21 1 26.2 17.4 16.9 2.91 2.23 1.95 1 ## 3 52 2 15.9 12.9 19.1 2.77 0.421 0.135 1 We see three cases we could flag as outliers, all on y1. Two of these cases (Cases 9 &amp; 21) are in Group 1; one (Case 52) was in Group 2. These z-scores are not terribly excessive—only Case 9 exceeds the criterion of 3.29. We could examine these data to see if there was a data-entry error. Let’s assume it was not and carry on by seeing if these same cases show up in our examination of multivariate normality. 4.3.1.2 Kurtosis and skew We used the psych package above when we used the describeBy() function. We could simply interpret those and use the absolute value of 2 as the criterion. Looking at that, we see that on y1, Group 1 has a kurtosis exceeding the magnitude of 2.00. It is positive, indicating leptokurtosis (too much pointiness), which becomes less problematic with samples greater than 100 or so; a platykurtic skew, which has thick tails, underestimates the sample’s variance until about a sample size of 200 (Tabachnick &amp; Fidell, 2013, p. 80, cite Waternaux, 1976). Based on the univariate kurtosis and skew estimates of y2 and y3, we do not see evidence that these two variables differ from normality within each group. We could also use confidence intervals to determine if the kurtosis and skew are outside of an expected range that might constitute normality. The DescTools package can provide these confidence intervals; they suggest using the bootstrap estimator, bca for the confidence intervals. Here’s an example of skew with the first dependent variable only: library(DescTools) tapply(dat$y1, dat$gp, DescTools::Kurt, method = 3 , # Use method 2 if you want the same result as SPSS conf.level = .95, ci.type = &quot;bca&quot;) ## $`1` ## kurt lwr.ci upr.ci ## 3.6784860 0.5597267 11.2928002 ## ## $`2` ## kurt lwr.ci upr.ci ## 0.5480051 -0.7509224 4.0266324 The 3.68 kurtosis for Group 1 on this variable is within their confidence interval, suggesting it might not be enough to flag non-normality. 4.3.1.3 Examining univariate normality using density plots We can examine plots of each variable, by group. First, let’s examine density plots of each variable, within each group, and compare that to the theoretical normal distribution. For this, the easiest way seems to be with the ggpubr package, which has the ggdensity() function instead of the ggplot() function we usually use.21 library(ggpubr) dat %&gt;% filter(gp == 1) %&gt;% # subset to Group 1, using dplyr. ggdensity(x = &quot;y1&quot;, # specify which variable. fill = &quot;gray50&quot;, # observed density is colored gray. color = &quot;white&quot;, # remove line from observed density plot title = &quot;Gasoline trucks&quot;) + # I know `gp == 1` matches gasoline trucks, so I use `title = `. # And, here&#39;s the theoretical normal density line, layered on top: stat_overlay_normal_density(color = &quot;darkred&quot;) We can see that compared to a theoretical normal distribution, the distribution of y1 with the Gasoline-truck group is leptokurtic (too peaked) and positively skewed. This is consistent with the skew and kurtosis reported in the describeBy() function earlier, 1.71 and 3.68 respectively. The two outliers seem to be responsible for this bump on the right side. We can do this for every group and every variable. Here’s the diesel group with y1. dat %&gt;% filter(gp == 2) %&gt;% ggdensity(x = &quot;y1&quot;, fill = &quot;gray50&quot;, color = &quot;white&quot;, title = &quot;Diesel trucks&quot;, xlab = &quot;Fuel costs&quot;) + stat_overlay_normal_density(color = &quot;darkred&quot;) The second group’s scores on this variable seem to come from a population with a normal distribution.22 We see a little bump on the right hand side, probably caused by that outlier we found with the z-scores. We can also display both groups in a plot as separate panels (or facets). Here, we’re adding the facet.by = argument from this function (which is specific to this function, in looking at ?ggdensity), with the group variable in our data, gp, plot the two groups next to each other in their respective panel of the plot. Then, we’re making the label of the panel more informative with panel.labs = list(gp = c(\"Gasoline\", \"Diesel\")), where the gp here is, again, the name of the group variable in our data set. dat %&gt;% ggdensity(x = &quot;y1&quot;, fill = &quot;gray50&quot;, color = &quot;white&quot;, xlab = &quot;Fuel costs&quot;, facet.by = &quot;gp&quot;, panel.labs = list(gp = c(&quot;Gasoline&quot;, &quot;Diesel&quot;)) ) + stat_overlay_normal_density(color = &quot;darkred&quot;) The one problem with this layout is that the y-axis is on the same scale for both variables, which seems to stretch the y-axis scale so it appears as though the second group is excessively leptokurtic. However, by comparing this to the superimposed normal curve, we see it is not too peaked. Let’s look at the other two variables: dat %&gt;% ggdensity(x = &quot;y2&quot;, fill = &quot;gray50&quot;, color = &quot;white&quot;, xlab = &quot;Repair costs&quot;, facet.by = &quot;gp&quot;, panel.labs = list(gp = c(&quot;Gasoline&quot;, &quot;Diesel&quot;)) ) + stat_overlay_normal_density(color = &quot;darkred&quot;) Although the observed density plots look truncated, with some fat tails, we do see support for the conclusion that the sample data are drawn from a population of normally distributed data. dat %&gt;% ggdensity(x = &quot;y3&quot;, fill = &quot;gray50&quot;, color = &quot;white&quot;, xlab = &quot;Capital costs&quot;, facet.by = &quot;gp&quot;, panel.labs = list(gp = c(&quot;Gasoline&quot;, &quot;Diesel&quot;)) ) + stat_overlay_normal_density(color = &quot;darkred&quot;) These plots also suggest the sample data seem to come from a population of normally distributed data. 4.3.2 Examining univariate normality using quantile-quantile plots Quantile-quantile plots are probably the most useful for examining normality. They’re a little less work than the density plots, too. If you want a refresher on what quantiles are and how Q-Q plots are generated, Josh Starmer has a couple of videos: one on quantiles, linked to here, and one on Q-Q plots, linked to here. Here’s a plot of the first group with the first variable: dat %&gt;% filter(gp == &quot;1&quot;) %&gt;% ggplot(aes(sample = y1)) + stat_qq() + stat_qq_line() + scale_x_continuous(&quot;Norm quantiles&quot;) + scale_y_continuous(&quot;Sample quantiles&quot;) + labs(title = &quot;Gasoline trucks&quot;) We see the effect of those two outliers on the plot in Group 1 (gasoline trucks). Their scores are much higher than we would expect if the data were from a normal distribution. Without them, it would look normal. Here’s the second group: dat %&gt;% filter(gp == &quot;2&quot;) %&gt;% ggplot(aes(sample = y1)) + stat_qq() + stat_qq_line() + scale_x_continuous(&quot;Norm quantiles&quot;) + scale_y_continuous(&quot;Sample quantiles&quot;) + labs(title = &quot;Diesel trucks&quot;) In Group 2, y1 looks normal enough, with most of the scores very close to the line, though there is one observation with a score that is unexpectedly high. We could use ggplot for the other four plots. Let’s look at a function from the car package that also provides some error bands: 4.3.2.1 Univariate Q-Q plots from other packages The car package’s qqPlot() is convenient in that it includes an error band for interpreting points that are off the line, and it labels outliers. This approach requires the input be a matrix or vector (not a data frame, as ggplot requires). For this, we need to change our group variable to be numeric and create a matrix with group and our dependent variables: ym &lt;- cbind(gp = as.numeric(dat$gp), dat[, 3:5]) library(car) car::qqPlot(ym[,2], ylab = &quot;Y1&quot;, groups = ym[,1]) # For the other two variables: # car::qqPlot(ym[,3], ylab = &quot;Y2&quot;, groups = ym[,1]) # car::qqPlot(ym[,4], ylab = &quot;Y3&quot;, groups = ym[,1]) This also labels the row number of the observations that might be flagged as outliers. We see results that are consistent with the above plots. This was only with the first dependent variable; we’d use the same code with the other two. Another package we could use is ggpubr, with the ggqqplot() function. We used this package with our density plots. This is convenient because we don’t have to change the data into matrix format: Here’s are all the plots with this package: library(ggpubr) dat %&gt;% filter(gp == &quot;1&quot;) %&gt;% ggqqplot(x = &quot;y1&quot;, title = &quot;Gasoline trucks&#39; fuel costs&quot;) dat %&gt;% filter(gp == &quot;2&quot;) %&gt;% ggqqplot(x = &quot;y1&quot;, title = &quot;Diesel trucks&#39; fuel costs&quot;) dat %&gt;% filter(gp == &quot;1&quot;) %&gt;% ggqqplot(x = &quot;y2&quot;, title = &quot;Gasoline trucks&#39; repair costs&quot;) dat %&gt;% filter(gp == &quot;2&quot;) %&gt;% ggqqplot(x = &quot;y2&quot;, title = &quot;Diesel trucks&#39; repair costs&quot;) dat %&gt;% filter(gp == &quot;1&quot;) %&gt;% ggqqplot(x = &quot;y3&quot;, title = &quot;Gasoline trucks&#39; capital costs&quot;) dat %&gt;% filter(gp == &quot;2&quot;) %&gt;% ggqqplot(x = &quot;y3&quot;, title = &quot;Diesel trucks&#39; capital costs&quot;) This one does not label the outliers, which is why I like the car package’s version better. If I were in a hurry and I needed to assess univariate normality, I’d simply look at the Q-Q plots from the car package. If I wanted to publish something pretty, I’d use ggplot or ggpubr’s ggqqplot, and I’d display the density plots with the normal distribution layered over them for comparison. 4.3.2.2 Statistical tests of univariate normality There are also statistical tests of normality. The Shapiro–Wilk test seems to be well regarded. To demonstrate, let’s examine y1 with Group 1: g1 &lt;- subset(dat, gp == 1, select = c(&quot;y1&quot;, &quot;y2&quot;, &quot;y3&quot;)) shapiro.test(g1$y1) ## ## Shapiro-Wilk normality test ## ## data: g1$y1 ## W = 0.83672, p-value = 9.555e-05 The p-value below our critical alpha of .05 indicates that for Group 1, we can reject the null hypothesis that y1 is from a normal distribution. We don’t like this result. Let’s look at Group 2 on this same variable: shapiro.test(g1$y2) ## ## Shapiro-Wilk normality test ## ## data: g1$y2 ## W = 0.96282, p-value = 0.2623 The same test with Group 2 did not reject the null hypothesis, which indicates we do not have evidence to suggest y1 differs from normality. We would do this test by-group for the other two variables and hope we got non-significance.23 4.3.3 A caution about statistical tests of statistical assumptions As with most (or all) statistical tests of statistical assumptions, we are arguing for the null hypothesis, which is kind of antithetical to null-hypothesis statistical testing but that’s how it seems to be done. With small sample sizes, statistical tests like Shapiro-Wilk show non-significance even when the data are not normal. With large sample sizes, they’re too sensitive to deviations from normality. With this, statistical tests of normality should not be the primary source of evidence for evaluating this assumption. 4.3.4 Mahalanobis Distance and Multivariate Outliers We can calculate each observation’s Mahalanobis distance. I think of this as the observation’s degree of weirdness compared to the rest of the data. This is a function of leverage, which was addressed in multiple regression. Mahalanobis distances are in squared units, so we use a chi-square distribution instead of a normal distribution with quantiles in our Q-Q plots. The mahalanobis() function from Base R has three arguments: the matrix of multivariate variables, the means of those those variables (in center=), and the covariance matrix of those variables.24 We’ll create a matrix of our dependent variables for the first argument and use the colMeans() and cov() functions to get the latter two. dvs &lt;- cbind(dat[,c(&quot;y1&quot;,&quot;y2&quot;,&quot;y3&quot;)]) distances &lt;- mahalanobis(dvs, center = colMeans(dvs), cov = cov(dvs)) Each observation has a multivariate distance, so let’s save it to the data frame: dat$distancesraw &lt;- distances We might notice that for some observations, they have really high Mahalanobis distance values. We could sort them from high to low to see if there is a quick descent before it then levels out: sort(dat$distancesraw, decreasing = TRUE)[1:20] # first 20 observations ## [1] 20.369430 13.881471 12.473991 11.967188 9.750828 5.731465 5.132105 4.292636 3.937175 ## [10] 3.885500 3.418971 3.413827 2.976672 2.950409 2.940999 2.823275 2.647751 2.496250 ## [19] 2.491660 2.472095 4.3.4.1 Outliers There is a standard way to see if they are outliers. We calculate the p-value of that Mahalanobis distance, from a chi-square distribution with \\(p-1\\) degrees of freedom (where p = number of dependent variables), and use \\(p &lt; .001\\) as a criterion for identifying an outlier. dat$p &lt;- pchisq(distances, df = (ncol(dvs)-1), # df is number of variables - 1 lower.tail=FALSE) Let’s flag any cases with a p-value lower than our alpha = .001 criterion. In this code, we’re using ifelse() to dummy code any cases with criterion as 1 and 0 otherwise. Then, we’re requesting rows of the data that have a value of 1 on this new variable. dat$outlier &lt;- ifelse(dat$p &lt; .001, 1, # the case gets a 1 if p &lt; .001 0) # otherwise, the value on this new variable is 0 # We can see two outliers here: dat[dat$outlier == 1, ] ## id gp y1 y2 y3 distancesraw p outlier ## 9 9 1 29.11 15.09 3.28 20.36943 3.774282e-05 1 ## 21 21 1 26.16 17.44 16.89 13.88147 9.675577e-04 1 Not surprisingly, these are the two pains in the neck in our univariate analysis. Out liars, out! The Mahalanobis distances are in squared units. The car package has a function we can use for this to generate a kind of Q-Q plot. Instead of a normal distribution of quantiles on the X axis, we can specify a chi-squared distribution. Everitt and Hothorn (2011) use similar procedures to obtain the same type of plot. library(car) car::qqPlot(distances, distribution = &quot;chisq&quot;, df = mean(distances), lwd = 1, grid = FALSE, main = &quot;Multi-normal Q-Q Plot on Raw Data&quot;, xlab = expression(chi^2 * &quot; quantiles&quot;), ylab = expression(&quot;Mahalanobis distances &quot;^2)) ## [1] 9 21 4.3.4.2 Mahalanobis Distance using Residuals Many scholars point out that the normality assumption is really about the residuals of the model. Let’s fit our MANOVA model without examining the results and save the residuals to the data frame, then conduct this same approach we did above with Mahalanobis distance: dvs &lt;- cbind(dat$y1, dat$y2, dat$y3) man.mod &lt;- manova(dvs ~ gp, data = dat) dat[, c(&quot;res1&quot;, &quot;res2&quot;, &quot;res3&quot;)] &lt;- man.mod$residuals resids &lt;- cbind(dat[,c(&quot;res1&quot;, &quot;res2&quot;, &quot;res3&quot;)]) distances &lt;- mahalanobis(resids, center = colMeans(resids), cov = cov(resids)) dat$distances &lt;- distances dat$p &lt;- pchisq(distances, df = (ncol(resids)-1), # df is number of variables - 1 lower.tail=FALSE) dat$outlier &lt;- ifelse(dat$p &lt; .001, 1, 0) dat[dat$outlier == 1, ] ## id gp y1 y2 y3 distancesraw p outlier res1 res2 res3 distances ## 9 9 1 29.11 15.09 3.28 20.36943 1.989997e-05 1 16.89139 6.9775 -6.310278 21.64958 Now, we see that Case 9 is the only outlier. Let’s look at the multivariate analogue to the univariate Q-Q plot, now with residuals: library(car) car::qqPlot(distances, distribution = &quot;chisq&quot;, df = mean(distances), lwd = 1, grid = FALSE, main = &quot;Multi-normal Q-Q Plot on Residuals&quot;, xlab = expression(chi^2 * &quot; quantiles&quot;), ylab = expression(&quot;Mahalanobis distances &quot;^2)) ## [1] 9 21 We see a very similar shape to what we saw with the raw data. It looks like we will not be able to claim we have met the assumption of normality. However, we should really be examining multidimensional normality within each group, not the entire data set. Let’s do that: 4.3.4.3 Multivariate residual Q-Q plot by group With the residuals, we can use the groups = argument to specify the group. This requires that the matrix have a column that is the group variable. library(car) distances_gp &lt;- cbind(gp = as.numeric(dat$gp), distances) car::qqPlot(distances_gp[, -1], distribution = &quot;chisq&quot;, df = mean(distances_gp[, -1]), groups = distances_gp[,&quot;gp&quot;], lwd = 1, grid = FALSE, main = &quot;Multi-normal Q-Q Plot on Residuals&quot;, xlab = expression(chi^2 * &quot; quantiles&quot;), ylab = expression(&quot;Mahalanobis distances &quot;^2)) Clearly, Case 9 is a candidate for removal and seeing if the results will lead to the same statistical conclusion if the case were retained. 4.3.5 Statistical Tests of Multivariate Normality Just as there are statistical tests of univariate normality, there are of multivariate normality. And, just as was the case with their univariate counterparts, the statistical test is sensitive to sample size, just as any test is. With this, if we have a large sample size, we will likely have a statistical significance when the divergence from normality is only negligible. In other words, this source of evidence should be used in conjunction with plots. Here is the mshapiro.test() function from the mvnormtest package. This function requires the data be arranged as rows instead of columns, so we’ll use the transpose function, t() within the test. First, let’s separate the dependent variables by group. Here, we’re using their respective residuals, but many authors simply use the dependent variables on the grounds that researchers would be tempted to analyze the results of their fitted model (which is needed to get the residuals) before analyzing the assumptions. dvs.gp1 &lt;- subset(dat, gp == 1, select = c(&quot;res1&quot;, &quot;res2&quot;, &quot;res3&quot;)) dvs.gp2 &lt;- subset(dat, gp == 2, select = c(&quot;res1&quot;, &quot;res2&quot;, &quot;res3&quot;)) library(mvnormtest) mshapiro.test( t(dvs.gp1) ) ## ## Shapiro-Wilk normality test ## ## data: Z ## W = 0.81141, p-value = 2.767e-05 mshapiro.test( t(dvs.gp2) ) ## ## Shapiro-Wilk normality test ## ## data: Z ## W = 0.94576, p-value = 0.2384 We see results that are consistent with our outliers and plots. The multivariate normality assumption was not met because Group 1 had a multivariate distribution that was significantly different from normal. 4.3.6 Homogeneity of Covariance Box’s M test can be conducted using the box_m() function from the rstatix package, the boxM() function from the biotools package, or the boxM() function from the heplots package. Let’s arbitrarily use the latter: library(heplots) boxM(dat[ , c(&quot;y1&quot;, &quot;y2&quot;, &quot;y3&quot;)], dat[ , &quot;gp&quot;]) ## ## Box&#39;s M-test for Homogeneity of Covariance Matrices ## ## data: dat[, c(&quot;y1&quot;, &quot;y2&quot;, &quot;y3&quot;)] ## Chi-Sq (approx.) = 30.543, df = 6, p-value = 3.098e-05 This assumption was not met. If we had a balanced design, this might not be an issue. Tabachick and Fidell state that Box’s M test tends to be too sensitive, but in this case, the violation seems to be undeniable. One thing we could do is examine the covariance matrices for each group, and see whether the smaller group has the larger variance-covariance, which in turn would lead to overly lenient estimations of statistical significance and inflated Type I errors. Here are the n-sizes: table(dat$gp) ## ## 1 2 ## 36 23 max(table(dat$gp)) / min(table(dat$gp)) ## [1] 1.565217 Group 1 (n = 36) is over 1.5 times larger than Group 2 (or, Group 2 is about two-thirds as big as Group 1). If we see that Group 2 has larger values in the covariance matrix, our MANOVA test statistics will err on the side of leniency, risking Type I error rate inflation. On the other hand, if Group 1 has larger covariance, we err on the side of conservation, risking loss of statistical power. For credibility in our statistical conclusions, the latter scenario is preferable. Note that if we had equal group sizes, MANOVA is pretty “robust” in its statistical tests; here, we don’t. dat1 &lt;- subset(dat, gp == 1) dat2 &lt;- subset(dat, gp == 2) covm1 &lt;- cov(dat1[, c(&quot;res1&quot;, &quot;res2&quot;, &quot;res3&quot;)]) covm2 &lt;- cov(dat2[, c(&quot;res1&quot;, &quot;res2&quot;, &quot;res3&quot;)]) round(covm1, 1) ## res1 res2 res3 ## res1 23.0 12.4 2.9 ## res2 12.4 17.5 4.8 ## res3 2.9 4.8 14.0 round(covm2, 1) ## res1 res2 res3 ## res1 4.4 0.8 2.4 ## res2 0.8 25.9 7.7 ## res3 2.4 7.7 46.7 The pattern is not consistent in terms of which group has larger variances or covariances. With y1, Group 1 has larger variance (23.0 vs 4.4); with y2, there might be similar variances in the two groups (17.5 and 25.9); with y3, there is lower variance in Group 1 (14.0 vs 46.7). And, we haven’t even compared the covariances, which also seem to differ. What do we do? Boiling each of these covariance matrices down to a single number will be helpful in identifying which group has higher variance, which in turn is important for determining if our violation of this assumption will result in a more conservative or more liberal statistical conclusion. Pituch and Stevens used SPSS, which included the determinant of the covariance matrix for each group in its output. Our Box test output did not provide this, but we can use the determinants of the by-group residual covariance matrices to accomplish the same thing: det(covm1) ## [1] 3172.914 det(covm2) ## [1] 4860.31 det(covm2) / det(covm1) ## [1] 1.531813 The generalized variance of the smaller group is 1.5 times that of the larger group. This suggests that the test will be biased to show an effect when there might not be one—that is, increasing Type I error. This is bad news because if we find statistical significance, we won’t know if it is because of population differences or because of this bias. 4.3.7 Conclusions about our statistical assumptions The statistical assumptions of multivariate normality and homogeneity of covariance were not met with this data set. Pituch and Stevens recommend transforming the data, as would Tabachnick and Fidell. Other scholars (such Curran and Hancock in their podcast, linked to here) do not recommend data transformations but instead recommend robust standard errors be used. We’ll take the robust approach in this example. 4.4 Fitting the MANOVA Model If we had met the assumptions, we’d use the following code. This is for demonstration only. The multivariate omnibus null hypothesis is \\[H_0 = \\begin{bmatrix} \\mu_{11} \\\\ \\mu_{21} \\\\ \\mu_{31} \\ \\end{bmatrix} = \\begin{bmatrix} \\mu_{12} \\\\ \\mu_{22} \\\\ \\mu_{32} \\ \\end{bmatrix} \\] In other words, with \\(\\mu_{11}\\) and \\(\\mu_{12}\\), the population mean of Group 1 on y1 is equal to the population mean of Group 2 on y1; with with \\(\\mu_{21}\\) and \\(\\mu_{22}\\), the population mean of Group 1 on y2 is equal to the population mean of Group 2 on y2; and so forth for y3. We use the manova() function to fit the model. This requires the dependent variables be in a matrix, which below we accomplish with the cbind() function. We then examine the results with the summary() function (use ?summary.manova to look at the help file for these options). We’ll use test = \"Pillai\", which is the default because among the four main possible tests to choose from, Pillai’s trace is more suitable for data with unequal covariances. dvs &lt;- cbind(dat$y1, dat$y2, dat$y3) man.mod &lt;- manova(dvs ~ gp, data = dat) summary.manova(man.mod, test = &quot;Pillai&quot;) ## Df Pillai approx F num Df den Df Pr(&gt;F) ## gp 1 0.47179 16.375 3 55 1.001e-07 *** ## Residuals 57 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The other tests are Wilks’ lambda, Hotelling’s trace, and Roy’s largest root. Wilks’ lambda is by far the most frequently used test. Wilks’ lambda is calculated as the ratio of the generalized variance of the within to the total generalized variance, \\[ \\text{Wilks&#39; } \\Lambda = \\frac{|\\mathbf{W}|}{|\\mathbf{B} + \\mathbf{W}|}\\] summary.manova(man.mod, test = &quot;Wilks&quot;) ## Df Wilks approx F num Df den Df Pr(&gt;F) ## gp 1 0.52821 16.375 3 55 1.001e-07 *** ## Residuals 57 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 In this data set, we have only two groups, so the p-values and approximated F-tests are the same across the four types of tests. summary.manova(man.mod, test = &quot;Hotelling-Lawley&quot;) ## Df Hotelling-Lawley approx F num Df den Df Pr(&gt;F) ## gp 1 0.89319 16.375 3 55 1.001e-07 *** ## Residuals 57 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary.manova(man.mod, test = &quot;Roy&quot;) ## Df Roy approx F num Df den Df Pr(&gt;F) ## gp 1 0.89319 16.375 3 55 1.001e-07 *** ## Residuals 57 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We can pull out information from the output. For example, here are the SSCP matrices: man.sum &lt;- summary.manova(man.mod, test = &quot;Pillai&quot;) man.sum$SS ## $gp ## [,1] [,2] [,3] ## [1,] 62.65568 -78.55802 -254.3505 ## [2,] -78.55802 98.49647 318.9060 ## [3,] -254.35048 318.90598 1032.5347 ## ## $Residuals ## [,1] [,2] [,3] ## [1,] 901.4386 449.5777 153.6975 ## [2,] 449.5777 1182.8086 336.1357 ## [3,] 153.6975 336.1357 1515.1135 The residuals and eigenvalues are in there, too. We’ll address eigenvalues later. 4.4.0.1 Interpretation If our research question were indeed about the composite costs of transporting milk given either a diesel or gasoline truck, the best we could say at this point is that there is a difference in costs between the two trucks. Where that difference lies is yet unknown. Should we use gasoline or diesel?25 We don’t have an answer (which is why I find MANOVA on its own not very useful). To find an answer to this, we turn to either univariate F-tests or discriminant function analysis—which we’ll address in an upcoming lesson. Though, we should present our results with caution, as we did not meet the statistical assumptions. 4.4.1 Follow-up analysis with univariate F-tests Traditionally, though not an agreed-upon practice, researchers follow a statistically significant MANOVA with ANOVAs. For this, we could use aov() for each dependent variable. Alternatively, we can use the summary.aov() function on the MANOVA model fitted object: summary.aov(man.mod) ## Response 1 : ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## gp 1 62.66 62.656 3.9619 0.05135 . ## Residuals 57 901.44 15.815 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Response 2 : ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## gp 1 98.5 98.496 4.7466 0.03351 * ## Residuals 57 1182.8 20.751 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Response 3 : ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## gp 1 1032.5 1032.53 38.845 5.966e-08 *** ## Residuals 57 1515.1 26.58 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We would then carry out the usual ANOVA process of examining planned comparisons or post-hoc tests on the variables that had statistically significant omnibus F-tests. In this example, we only have two groups, so there’s no need. 4.4.2 Effect size The effect size for each independent variable is calculated from Wilks’ Lambda as \\[ \\text{Partial } \\eta^2 = 1-\\Lambda^{1/s}\\] where \\(s\\) is either the number of dependent variables, \\(p\\), or the degrees of freedom of the effect, whichever is smaller: \\(s = \\text{min}(p, df_{\\text{effect}})\\). We can do this by hand or use the min() function from R: dvs &lt;- cbind(dat$y1, dat$y2, dat$y3) # Here&#39;s `p`, our number of dependent variables: p &lt;- nrow(dvs) # Here&#39;s the degrees of freedom of the effect, which with a main effect in # MANOVA is the number of groups minus 1 (or k - 1). We could use this code: k &lt;- length(levels(dat$gp)) # `levels()` gives the levels of the factor. `length()` returns how many. df.eff &lt;- (k - 1) df.eff ## [1] 1 # Or, we can look at the summary statistics to find the df of the effect: man.mod &lt;- manova(dvs ~ gp, data = dat) (man.sum &lt;- summary(man.mod, test = &quot;Wilks&quot;) ) ## Df Wilks approx F num Df den Df Pr(&gt;F) ## gp 1 0.52821 16.375 3 55 1.001e-07 *** ## Residuals 57 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # We can see it is 1 in this case. # For reproducibility, we could also do it this way: df.eff &lt;- man.sum$stats[&quot;gp&quot; , &quot;Df&quot;] # Now, let&#39;s find what `s` is, for our calculation of eta squared: s &lt;- min(p, df.eff) # 1 is less than 3, so the result is 1, here. # Get Wilks&#39; lambda: WLambda &lt;- man.sum$stats[&quot;gp&quot;, &quot;Wilks&quot;] # Calculate it: my.eta.sq &lt;- 1 - WLambda^(1/s) my.eta.sq ## [1] 0.4717907 Because we have only one independent variable, we don’t really have a partial \\(\\eta^2\\), as there is no variance from other variables to partial out, so we have an \\(\\eta^2\\). Also, when we have only two groups, the degrees of freedom of the effect is 1, making \\(s = 1\\), which results in: \\[\\begin{aligned} \\eta^2 &amp;= 1-\\Lambda^{1/1} \\\\ &amp;= 1-\\Lambda^1 \\\\ &amp;= 1-\\Lambda \\end{aligned}\\] Here, our effect size is \\(\\eta^2 = 1 - 0.52821 = 0.47\\), which means that our independent variable explains 47 percent of the variability in the linear composite of the three dependent variables. With two groups, the effect size is the same as the Pillai’s trace statistic. Instead of doing this calculation ourselves (or maybe to check our calculation), we can use the eta_squared() function on the MANOVA model output. This function is from the effectsize package. library(effectsize) dvs &lt;- cbind(dat$y1, dat$y2, dat$y3) man.mod &lt;- manova(dvs ~ gp, data = dat) eta_squared(man.mod) ## # Effect Size for ANOVA (Type I) ## ## Parameter | Eta2 (partial) | 95% CI ## ----------------------------------------- ## gp | 0.47 | [0.30, 1.00] ## ## - One-sided CIs: upper bound fixed at [1.00]. 4.4.3 Comparing the Model Without Outlier We can conduct the MANOVA analysis without the multivariate outlier to determine if this has an effect on the statitical conclusion. Below, we’re creating a second data set, without the outlier, by using the ! operator in front of the row index. dat2 &lt;- dat dat2 &lt;- dat2[!dat2$outlier == 1, ] dvs2 &lt;- cbind(dat2$y1, dat2$y2, dat2$y3) man.mod2 &lt;- manova(dvs2 ~ gp, data = dat2) summary.manova(man.mod2, test = &quot;Wilks&quot;) ## Df Wilks approx F num Df den Df Pr(&gt;F) ## gp 1 0.50628 17.553 3 54 4.438e-08 *** ## Residuals 56 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We see that the statistical conclusion is unchanged and that the effect is slightly stronger, with \\(\\eta^2 = 0.49\\). Given this, we should retain the outlier and report the test with all of the observations. That one outlier, ID = 9, was from the Mahalanobis distance with the residuals. If we wanted to see the results with the two outliers detected from the raw data, our outlier variable in our data set would have included both Case 9 and 11. Another option is to remember who these two cases were (id = 9 &amp; 11) and to create a data set without these two people for the manova() procedure. Here, I’m using (a) the filter() function from the dplyr package (in the tidyverse suite), (b) the code ! to exclude these cases that meet the criterion, and (c) the %in% in the criterion. Here, my criterion is if the row contains either a 9 or 21 in the id variable (which is a factor in our dataframe). dat2 &lt;- dat dat2 &lt;- dat2 %&gt;% filter(!id %in% c(&quot;9&quot;, &quot;21&quot;) ) dvs2 &lt;- cbind(dat2$y1, dat2$y2, dat2$y3) man.mod2 &lt;- manova(dvs2 ~ gp, data = dat2) summary.manova(man.mod2, test = &quot;Wilks&quot;) ## Df Wilks approx F num Df den Df Pr(&gt;F) ## gp 1 0.51219 16.826 3 53 8.359e-08 *** ## Residuals 55 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Similar to the model results when we excluded only Case 9, we see a similar statistical conclusion, which in turn is the same as if we had included these two cases. With this, we can include these outliers in ouar data set and report that we conducted this sensitivity analysis. 4.5 Robust One-way MANOVA Because the assumptions were not met, we could use a robust version of Wilks’ lambda. This is for one-way MANOVAs only—that is, having a single independent variable. To my knowledge, there are no robust methods for factorial MANOVA or MANCOVA designs—we’d be out of luck. We will not pursue this further, but the documentation is in the package and it is worth knowing that robust approaches exist, at least for one-way MANOVA. The output with the method I’ve specified below (which is the default) provides a Wilks’ lambda estimate, which is identical to our test, a chi-square test statistic and the p-value leading to the same statistical conclusion. It also reports a matrix of by-group (rows) sample means on the variables (columns), which is not interesting because we already examined that above with describeBy(). library(rrcov) dvs &lt;- cbind(dat$y1, dat$y2, dat$y3) grp &lt;- dat$gp Wtest &lt;- Wilks.test(dvs, grouping = grp, method = &quot;c&quot;) Wtest ## ## One-way MANOVA (Bartlett Chi2) ## ## data: x ## Wilks&#39; Lambda = 0.52821, Chi2-Value = 35.424, DF = 3.000, p-value = 9.914e-08 ## sample estimates: ## [,1] [,2] [,3] ## 1 12.21861 8.11250 9.590278 ## 2 10.10565 10.76174 18.167826 4.6 Summary The primary focus of this session was to practice examining the assumptions used in MANOVA. MANOVA is often described as reasonably robust to violations of these two assumptions (probably not to violations of statistical independence) if the sample sizes in each group are equal. To some scholars, data transformation is not a recommended solution because the statistical tests reflect the transformed variables, which are also hard to interpret. If we embrace our non-normality (as Curran and Hancock word it) in MANOVA, the furthest it seems we can get is with one-way MANOVAs. After that, we either throw in the towel or acquiesce to data transformation. The second example in the reading is with SENIORWISE.sav data and provides a much more satisfying data set for practicing one-way MANOVA. Using the tools in this tutorial, try examining the assumptions with that data set. A shortcut is to fit an ANOVA for each dependent variable and then examine the plots. That approach would also be examining the normality of the residuals rather than the raw data, which many scholars believe is more appropriate.↩︎ We can use the qnorm() function to get the critical value of z at a probability level, such as at p = .01: abs(qnorm(p = .01/2, lower.tail=TRUE)) = 2.58.↩︎ This standardization can also be in a single line of code, as in apply(dat[, c(\"y1\", \"y2\", \"y3\")], 2, scale).↩︎ Alternatively, we could use dat %&gt;% filter(gp == 1) %&gt;% ggplot(aes(x = scale(y1) )) + geom_density(fill = \"wheat\", color = \"white\") + stat_function(fun = dnorm) if we did not want to use the package.↩︎ Note that we’re using the term sample and population here because we’re assuming our dependent variables are random (rather than fixed) variables because the cases are randomly sampled from the population.↩︎ We could also fit the models and conduct this test on the residuals.↩︎ Can also use the mahalanobis_distance() function in the rstatix package.↩︎ This makes me want to blame carbon-based pollution on milk, and that’s not even including the massive amount of methane released from cow belching and farting.↩︎ "],["week-04-factorial-manova.html", "5 Week 04 Factorial MANOVA 5.1 Following Tabachnick and Fidell’s notes 5.2 Calculating Wilks’ Lambda 5.3 MANCOVA", " 5 Week 04 Factorial MANOVA Drawing from Ch. 7 of Tabachnick &amp; Fidell First, we break down the sums of squares and cross products described in the chapter, to understand the factorial MANOVA. We then fit a factorial MANOVA model and then a MANCOVA model. 5.1 Following Tabachnick and Fidell’s notes Here are the data: dat &lt;- data.frame( id = 1:18, treat = rep(c(&quot;T&quot;, &quot;C&quot;), each = 9), disab = rep(c(&quot;Mild&quot;, &quot;Mild&quot;, &quot;Mild&quot;, &quot;Moderate&quot;, &quot;Moderate&quot;, &quot;Moderate&quot;, &quot;Severe&quot;, &quot;Severe&quot;, &quot;Severe&quot;), times = 2), wratR = c(115, 98, 107, 100, 105, 95, 89, 100, 90, 90, 85, 80, 70, 85, 78, 65, 80, 72), wratA = c(108, 105, 98, 105, 95, 98, 78, 85, 95, 92, 95, 81, 80, 68, 82, 62, 70, 73), IQ = c(110, 102, 100, 115, 98, 100, 99, 102, 100, 108, 115, 95, 100, 99, 105, 101, 95, 102) ) str(dat) ## &#39;data.frame&#39;: 18 obs. of 6 variables: ## $ id : int 1 2 3 4 5 6 7 8 9 10 ... ## $ treat: chr &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; ... ## $ disab: chr &quot;Mild&quot; &quot;Mild&quot; &quot;Mild&quot; &quot;Moderate&quot; ... ## $ wratR: num 115 98 107 100 105 95 89 100 90 90 ... ## $ wratA: num 108 105 98 105 95 98 78 85 95 92 ... ## $ IQ : num 110 102 100 115 98 100 99 102 100 108 ... dat$treat &lt;- as.factor(dat$treat) dat$disab &lt;- as.factor(dat$disab) str(dat) ## &#39;data.frame&#39;: 18 obs. of 6 variables: ## $ id : int 1 2 3 4 5 6 7 8 9 10 ... ## $ treat: Factor w/ 2 levels &quot;C&quot;,&quot;T&quot;: 2 2 2 2 2 2 2 2 2 1 ... ## $ disab: Factor w/ 3 levels &quot;Mild&quot;,&quot;Moderate&quot;,..: 1 1 1 2 2 2 3 3 3 1 ... ## $ wratR: num 115 98 107 100 105 95 89 100 90 90 ... ## $ wratA: num 108 105 98 105 95 98 78 85 95 92 ... ## $ IQ : num 110 102 100 115 98 100 99 102 100 108 ... We can see the calculations presented on p. 258. This is only with the first observation in the data set. mat_GM &lt;- cbind(c(mean(dat$wratR), mean(dat$wratA)) ) Y_111 &lt;- cbind( unlist(dat[1, c(&quot;wratR&quot;, &quot;wratA&quot;)]) ) SS_111 &lt;- Y_111 - mat_GM SS_111 %*% t(SS_111) ## wratR wratA ## wratR 670.2346 537.9136 ## wratA 537.9136 431.7160 We could do this for every person and add up all the matrices, which would give us a \\(\\mathbf{SSCP}_{total}\\) matrix. But, let’s not. Instead, we can get the total SSCP matrix by getting the covariance with the cov() function and scalar-multiplying that by the df: DVs &lt;- cbind(dat[, c(&quot;wratR&quot;, &quot;wratA&quot;)]) cov(DVs) ## wratR wratA ## wratR 185.7516 150.9150 ## wratA 150.9150 189.0065 N &lt;- nrow(dat) SSCP_T &lt;- cov(DVs) * (N-1) SSCP_T ## wratR wratA ## wratR 3157.778 2565.556 ## wratA 2565.556 3213.111 This is the \\(\\mathbf{SSCP_{total}}\\), which will be the same as the sum of all of the SSCP matrices of the two main effects, the interaction effect, and the residual (within). If we’re using “by-hand” calculations, the Within and Between SSCP matrices are the following: # SSCP_W = sum of all of the Ws in each cell library(tidyverse) sqrs_cps &lt;- dat %&gt;% # `group_by()` conducts the rest by treatment and disability; i.e., within each cell. group_by(treat, disab) %&gt;% mutate(m_1 = mean(wratR), m_2 = mean(wratA), sq_1tb= (wratR - m_1)^2, cp = (wratR - m_1)*(wratA - m_2), sq_2tb= (wratA - m_2)^2 ) # Summing the squares and cross products: SSCP_Ws &lt;- sqrs_cps %&gt;% summarize(SS_1tb = sum(sq_1tb), cp_12 = sum(cp), SS_2tb = sum(sq_2tb) ) SSCP_Ws ## # A tibble: 6 × 5 ## # Groups: treat [2] ## treat disab SS_1tb cp_12 SS_2tb ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 C Mild 50 55 109. ## 2 C Moderate 113. -87.3 115. ## 3 C Severe 113. 57.7 64.7 ## 4 T Mild 145. 22.7 52.7 ## 5 T Moderate 50 -15 52.7 ## 6 T Severe 74 -2 146 W_1 &lt;- matrix( unlist(SSCP_Ws[1, c(&quot;SS_1tb&quot;, &quot;cp_12&quot;, &quot;cp_12&quot;, &quot;SS_2tb&quot;)]), nrow = 2) W_2 &lt;- matrix( unlist(SSCP_Ws[2, c(&quot;SS_1tb&quot;, &quot;cp_12&quot;, &quot;cp_12&quot;, &quot;SS_2tb&quot;)]), nrow = 2) W_3 &lt;- matrix( unlist(SSCP_Ws[3, c(&quot;SS_1tb&quot;, &quot;cp_12&quot;, &quot;cp_12&quot;, &quot;SS_2tb&quot;)]), nrow = 2) W_4 &lt;- matrix( unlist(SSCP_Ws[4, c(&quot;SS_1tb&quot;, &quot;cp_12&quot;, &quot;cp_12&quot;, &quot;SS_2tb&quot;)]), nrow = 2) W_5 &lt;- matrix( unlist(SSCP_Ws[5, c(&quot;SS_1tb&quot;, &quot;cp_12&quot;, &quot;cp_12&quot;, &quot;SS_2tb&quot;)]), nrow = 2) W_6 &lt;- matrix( unlist(SSCP_Ws[6, c(&quot;SS_1tb&quot;, &quot;cp_12&quot;, &quot;cp_12&quot;, &quot;SS_2tb&quot;)]), nrow = 2) Now, can use matrix addition to add them up. This is the \\(\\mathbf{SSCP_{within}}\\) matrix: SSCP_W &lt;- W_1 + W_2 + W_3 + W_4 + W_5 + W_6 colnames(SSCP_W) &lt;- rownames(SSCP_W) &lt;- c(&quot;wratR&quot;, &quot;wratA&quot;) SSCP_W ## wratR wratA ## wratR 544 31.0000 ## wratA 31 539.3333 5.1.1 Another way to get the \\(SSCP_W\\): The following bit of code is shorter than doing it by hand, but is way harder to read. Basically, we’re getting the covariance within each group-by-group cell in the full factorial model. Then, we’re obtaining the SSCP by multiplying by the n-size within the cell (assuming a balanced design, with all cells are same size). Then, we save this list of 6 within-matrices and then add them up. This gives the total within-SSCP. This is the same kind of operation we did in Assignment 1. library(tidyverse) temp &lt;- dat %&gt;% group_by(treat, disab) %&gt;% summarize(S_b = cov(cbind(wratR, wratA)), degf = n()-1, sscp= S_b * degf ) %&gt;% dplyr::select(sscp) %&gt;% data.frame() temp ## treat disab sscp.1 sscp.2 ## 1 C Mild 50.00000 55.00000 ## 2 C Mild 55.00000 108.66667 ## 3 C Moderate 112.66667 -87.33333 ## 4 C Moderate -87.33333 114.66667 ## 5 C Severe 112.66667 57.66667 ## 6 C Severe 57.66667 64.66667 ## 7 T Mild 144.66667 22.66667 ## 8 T Mild 22.66667 52.66667 ## 9 T Moderate 50.00000 -15.00000 ## 10 T Moderate -15.00000 52.66667 ## 11 T Severe 74.00000 -2.00000 ## 12 T Severe -2.00000 146.00000 sscp.mat &lt;- temp$sscp # The result of the summarize function was a matrix. k_grps &lt;- length( unique(dat[,c(&quot;treat&quot;, &quot;disab&quot;)]) ) p &lt;- 2 # number of DVs is 2 # Add a column to tell R how to split this up into 6 smaller matrices: sscp.mat &lt;- cbind(sscp.mat, c( rep(1:k_grps, each = p) ) ) # Split this up into 6 matrices, each stored in this list SSCP_W2 &lt;- lapply( split(sscp.mat[,1:2], f = sscp.mat[,3] ), FUN = matrix, ncol=p) SSCP_W2 # Our list of matrices ## $`1` ## [,1] [,2] ## [1,] 50.00000 55.00000 ## [2,] 55.00000 108.66667 ## [3,] 112.66667 57.66667 ## [4,] 57.66667 64.66667 ## [5,] 50.00000 -15.00000 ## [6,] -15.00000 52.66667 ## ## $`2` ## [,1] [,2] ## [1,] 112.66667 -87.33333 ## [2,] -87.33333 114.66667 ## [3,] 144.66667 22.66667 ## [4,] 22.66667 52.66667 ## [5,] 74.00000 -2.00000 ## [6,] -2.00000 146.00000 # This fancy function adds the matrices across the lists: SSCP_W2 &lt;- Reduce(f = &quot;+&quot;, SSCP_W2) SSCP_W2 # The within-SSCP matrix ## [,1] [,2] ## [1,] 162.66667 -32.33333 ## [2,] -32.33333 223.33333 ## [3,] 257.33333 80.33333 ## [4,] 80.33333 117.33333 ## [5,] 124.00000 -17.00000 ## [6,] -17.00000 198.66667 SSCP_W # compare with the previous hand-calculation ## wratR wratA ## wratR 544 31.0000 ## wratA 31 539.3333 5.1.2 \\(\\mathbf{SSCP_{Between}}\\) for each factor Here is something similar for the Between SSCP. There are three B matrices, one for each main effect (of treat and of disab) and one for the interaction. Let’s look at the main effect of treat: meanSSCP_treat &lt;- dat %&gt;% group_by(treat) %&gt;% summarize(m_1 = mean(wratR), m_2 = mean(wratA)) meanSSCP_treat$GM1 &lt;- mean(dat$wratR) meanSSCP_treat$GM2 &lt;- mean(dat$wratA) meanSSCP_treat$n &lt;- table(dat$treat)[1][[1]] # n per group is 9 meanSSCP_treat ## # A tibble: 2 × 6 ## treat m_1 m_2 GM1 GM2 n ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 C 78.3 78.1 89.1 87.2 9 ## 2 T 99.9 96.3 89.1 87.2 9 sqrs_cps_B &lt;- meanSSCP_treat %&gt;% mutate(sq_1tb= (m_1 - GM1)^2, cp = (m_1 - GM1)*(m_2 - GM2), sq_2tb= (m_2 - GM2)^2 ) SSCP_Bs &lt;- sqrs_cps_B %&gt;% summarize(SS_1tb = n * sq_1tb, cp_12 = n * cp, SS_2tb = n * sq_2tb ) B_1 &lt;- matrix( unlist(SSCP_Bs[1, c(&quot;SS_1tb&quot;, &quot;cp_12&quot;, &quot;cp_12&quot;, &quot;SS_2tb&quot;)]), nrow = 2) B_2 &lt;- matrix( unlist(SSCP_Bs[2, c(&quot;SS_1tb&quot;, &quot;cp_12&quot;, &quot;cp_12&quot;, &quot;SS_2tb&quot;)]), nrow = 2) SSCP_treat &lt;- B_1 + B_2 colnames(SSCP_treat) &lt;- rownames(SSCP_treat) &lt;- c(&quot;wratR&quot;, &quot;wratA&quot;) SSCP_treat # Could use S to stand for the SSCP matrix to follow T and F ## wratR wratA ## wratR 2090.889 1767.556 ## wratA 1767.556 1494.222 Let’s look at the main effect of disab: meanSSCP_disab &lt;- dat %&gt;% group_by(disab) %&gt;% # We&#39;re conducting the analysis by level of disab. summarize(m_1 = mean(wratR), m_2 = mean(wratA)) meanSSCP_disab$GM1 &lt;- mean(dat$wratR) meanSSCP_disab$GM2 &lt;- mean(dat$wratA) meanSSCP_disab$n &lt;- table(dat$disab)[1][[1]] # n per group is 6 meanSSCP_disab ## # A tibble: 3 × 6 ## disab m_1 m_2 GM1 GM2 n ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Mild 95.8 96.5 89.1 87.2 6 ## 2 Moderate 88.8 88 89.1 87.2 6 ## 3 Severe 82.7 77.2 89.1 87.2 6 sqrs_cps_B &lt;- meanSSCP_disab %&gt;% mutate(sq_1tb= (m_1 - GM1)^2, cp = (m_1 - GM1)*(m_2 - GM2), sq_2tb= (m_2 - GM2)^2 ) SSCP_Bs &lt;- sqrs_cps_B %&gt;% summarize(SS_1tb = n * sq_1tb, cp_12 = n * cp, SS_2tb = n * sq_2tb ) B_1 &lt;- matrix( unlist(SSCP_Bs[1, c(&quot;SS_1tb&quot;, &quot;cp_12&quot;, &quot;cp_12&quot;, &quot;SS_2tb&quot;)]), nrow = 2) B_2 &lt;- matrix( unlist(SSCP_Bs[2, c(&quot;SS_1tb&quot;, &quot;cp_12&quot;, &quot;cp_12&quot;, &quot;SS_2tb&quot;)]), nrow = 2) B_3 &lt;- matrix( unlist(SSCP_Bs[3, c(&quot;SS_1tb&quot;, &quot;cp_12&quot;, &quot;cp_12&quot;, &quot;SS_2tb&quot;)]), nrow = 2) SSCP_disab &lt;- B_1 + B_2 + B_3 SSCP_disab ## [,1] [,2] ## [1,] 520.7778 761.7222 ## [2,] 761.7222 1126.7778 5.1.3 \\(\\mathbf{SSCP_{Between}}\\) for the interaction Let’s look at the interaction: means_intxn &lt;- dat %&gt;% group_by(treat, disab) %&gt;% # by cell summarize(m_1 = mean(wratR), m_2 = mean(wratA)) means_intxn$GM1 &lt;- mean(dat$wratR) means_intxn$GM2 &lt;- mean(dat$wratA) means_intxn$n &lt;- 3 # n per group sqrs_cps_B &lt;- means_intxn %&gt;% mutate(sq_1tb= (m_1 - GM1)^2, cp = (m_1 - GM1)*(m_2 - GM2), sq_2tb= (m_2 - GM2)^2 ) SSCP_Bs &lt;- sqrs_cps_B %&gt;% summarize(SS_1tb = n * sq_1tb, cp_12 = n * cp, SS_2tb = n * sq_2tb ) B_1 &lt;- matrix( unlist(SSCP_Bs[1, c(&quot;SS_1tb&quot;, &quot;cp_12&quot;, &quot;cp_12&quot;, &quot;SS_2tb&quot;)]), nrow = 2) B_2 &lt;- matrix( unlist(SSCP_Bs[2, c(&quot;SS_1tb&quot;, &quot;cp_12&quot;, &quot;cp_12&quot;, &quot;SS_2tb&quot;)]), nrow = 2) B_3 &lt;- matrix( unlist(SSCP_Bs[3, c(&quot;SS_1tb&quot;, &quot;cp_12&quot;, &quot;cp_12&quot;, &quot;SS_2tb&quot;)]), nrow = 2) B_4 &lt;- matrix( unlist(SSCP_Bs[4, c(&quot;SS_1tb&quot;, &quot;cp_12&quot;, &quot;cp_12&quot;, &quot;SS_2tb&quot;)]), nrow = 2) B_5 &lt;- matrix( unlist(SSCP_Bs[5, c(&quot;SS_1tb&quot;, &quot;cp_12&quot;, &quot;cp_12&quot;, &quot;SS_2tb&quot;)]), nrow = 2) B_6 &lt;- matrix( unlist(SSCP_Bs[6, c(&quot;SS_1tb&quot;, &quot;cp_12&quot;, &quot;cp_12&quot;, &quot;SS_2tb&quot;)]), nrow = 2) B_int &lt;- B_1 + B_2 + B_3 + B_4 + B_5 + B_6 B_int ## [,1] [,2] ## [1,] 2613.778 2534.556 ## [2,] 2534.556 2673.778 From this, we then subtract the SSCP for each of the two main effects: SSCP_intrxn &lt;- (B_int - SSCP_treat - SSCP_disab) # We can see these same four matrices reported on p. 259: SSCP_disab # T and F have a typo in their first cell of 570.78. ## [,1] [,2] ## [1,] 520.7778 761.7222 ## [2,] 761.7222 1126.7778 SSCP_treat ## wratR wratA ## wratR 2090.889 1767.556 ## wratA 1767.556 1494.222 SSCP_intrxn ## wratR wratA ## wratR 2.111111 5.277778 ## wratA 5.277778 52.777778 SSCP_W ## wratR wratA ## wratR 544 31.0000 ## wratA 31 539.3333 5.1.4 These by-hand calculations will be available in manova(). If we fit the data with manova(), as we will below, we can look at these: (B_int - fit_summary$SS$treat - fit_summary$SS$disab) fit_summary$SS$treat:disab If we wanted the total SSCP, we could use this code. We don’t really need \\(\\mathbf{SSCP_{total}}\\) except maybe for reporting in a MANOVA table. SSCP_tot = SSCP_treat + SSCP_disab + SSCP_intrxn + SSCP_W SSCP_tot ## wratR wratA ## wratR 3157.778 2565.556 ## wratA 2565.556 3213.111 5.1.5 Calculating the generalized variance We can use the determinant function, det(), to get the generalized variance of each matrix used to test the effects. These are slightly different from those reported in Tabachnick and Fidell because they rounded and because of their typo. det(SSCP_W) # generalized within variance (i.e., residuals, or error) ## [1] 292436.3 det(SSCP_disab + SSCP_W) # T and F have a typo in their first cell of 570.78. ## [1] 1145630 det(SSCP_treat + SSCP_W) ## [1] 2123391 det(SSCP_intrxn + SSCP_W) ## [1] 322042.4 We can calculate Wilks’ Lambda, as reported on p. 261: numerator &lt;- det(SSCP_W) numerator denomintr &lt;- det(SSCP_intrxn + SSCP_W) denomintr numerator / denomintr ## [1] 292436.3 ## [1] 322042.4 ## [1] 0.9080679 5.2 Calculating Wilks’ Lambda Here’s the Wilks’ Lambda for the main effect of disability, reported in Table 7.2: det(SSCP_W) / det(SSCP_disab + SSCP_W) ## [1] 0.2552626 And for the main effect of treatment: det(SSCP_W) / det(SSCP_treat + SSCP_W) ## [1] 0.1377214 5.2.1 Fit the factorial MANOVA Let’s try using the manova() function to fit the model and peak under the hood to see if indeed Tabachnick and Fidell are telling the truth about these matrices and how to calculate Wilks’ Lambda: fit &lt;- manova(cbind(wratR, wratA) ~ treat*disab, data = dat) summary(fit, test = &quot;Wilks&quot;) ## Df Wilks approx F num Df den Df Pr(&gt;F) ## treat 1 0.13772 34.436 2 11 1.839e-05 *** ## disab 2 0.25526 5.386 4 22 0.003528 ** ## treat:disab 2 0.90807 0.272 4 22 0.893037 ## Residuals 12 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We can look at the matrices. These are the same as those on p. 259 (save their typo): fit_summary &lt;- summary.manova(fit) fit_summary$SS ## $treat ## wratR wratA ## wratR 2090.889 1767.556 ## wratA 1767.556 1494.222 ## ## $disab ## wratR wratA ## wratR 520.7778 761.7222 ## wratA 761.7222 1126.7778 ## ## $`treat:disab` ## wratR wratA ## wratR 2.111111 5.277778 ## wratA 5.277778 52.777778 ## ## $Residuals ## wratR wratA ## wratR 544 31.0000 ## wratA 31 539.3333 Notice that the sum of these SSCP matrices will be the total: DVs &lt;- cbind(dat[, c(&quot;wratR&quot;, &quot;wratA&quot;)]) N &lt;- nrow(dat) cov(DVs) * (N-1) # Compare this matrix to the sum from the MANOVA object: ## wratR wratA ## wratR 3157.778 2565.556 ## wratA 2565.556 3213.111 fit_summary$SS$treat + fit_summary$SS$disab + fit_summary$SS$`treat:disab` + fit_summary$SS$Residuals ## wratR wratA ## wratR 3157.778 2565.556 ## wratA 2565.556 3213.111 Again, like we did above, we can use the det() function to get the determinant of each matrix and calculate Wilks’ Lambda: numerator &lt;- det(fit_summary$SS$Residuals) numerator denomintr &lt;- det(fit_summary$SS$`treat:disab` + fit_summary$SS$Residuals) denomintr ## [1] 292436.3 ## [1] 322042.4 Here’s Wilks’ lambda, as reported on p. 261: numerator / denomintr ## [1] 0.9080679 5.3 MANCOVA Whereas in MANOVA, we compare group levels on the means on the dependent variables, in MANCOVA, we compare the group levels on the adjusted means of the dependent variables. The means are adjusted after accounting for the variability in the dependent variables that is explained by the covariate(s). The omnibus null hypothesis in a one-way MANOVA with p dependent variables and j levels in the factor can be displayed like this: \\[ H_0: \\text{ } \\begin{bmatrix} \\mu_{11}\\\\ \\mu_{12}\\\\ \\vdots \\\\ \\mu_{1p}\\\\ \\end{bmatrix} = \\begin{bmatrix} \\mu_{21}\\\\ \\mu_{22}\\\\ \\vdots \\\\ \\mu_{2p}\\\\ \\end{bmatrix} = \\cdots = \\begin{bmatrix} \\mu_{j1}\\\\ \\mu_{j2}\\\\ \\vdots \\\\ \\mu_{jp}\\\\ \\end{bmatrix} \\] Its counterpart in MANCOVA is this, where each \\(\\mu^*\\) represents the population mean after having adjusted for the covariate. \\[ H_0: \\text{ } \\begin{bmatrix} \\mu^*_{11}\\\\ \\mu^*_{12}\\\\ \\vdots \\\\ \\mu^*_{1p}\\\\ \\end{bmatrix} = \\begin{bmatrix} \\mu^*_{21}\\\\ \\mu^*_{22}\\\\ \\vdots \\\\ \\mu^*_{2p}\\\\ \\end{bmatrix} = \\cdots = \\begin{bmatrix} \\mu^*_{j1}\\\\ \\mu^*_{j2}\\\\ \\vdots \\\\ \\mu^*_{jp}\\\\ \\end{bmatrix} \\] 5.3.1 MANCOVA example We can include IQ as a covariate because presumably it was measured and recorded before the treatment intervention was carried out. The following code is one way—not the best way perhaps—of including the covariate: fit &lt;- manova(cbind(wratR, wratA) ~ treat + disab + IQ + treat:disab, data = dat) summary.manova(fit, test = &quot;Pillai&quot;) ## Df Pillai approx F num Df den Df Pr(&gt;F) ## treat 1 0.90138 45.701 2 10 9.328e-06 *** ## disab 2 0.83269 3.923 4 22 0.01494 * ## IQ 1 0.44867 4.069 2 10 0.05094 . ## treat:disab 2 0.06126 0.174 4 22 0.94950 ## Residuals 11 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary.manova(fit, test = &quot;Wilks&quot;) ## Df Wilks approx F num Df den Df Pr(&gt;F) ## treat 1 0.09862 45.701 2 10 9.328e-06 *** ## disab 2 0.17401 6.986 4 20 0.001089 ** ## IQ 1 0.55133 4.069 2 10 0.050941 . ## treat:disab 2 0.93896 0.160 4 20 0.956106 ## Residuals 11 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The results differ from what was reported in the chapter with SPSS because R manova() uses Type I sums of squares, where the order of entry of the independent variable is important. In contrast, SPSS uses Type III sums of squares, where every variable is adjusted for every other variable. There is a package written for conducting MANCOVA (and MANOVA) which has some nice output. This mancova() procedure enters the covariate after the main effects whereas if we include our covariate in manova(), we have more control over its order into the model, which comes into play when we make our case about how to specify the model. library(jmv) manc.fit &lt;- jmv::mancova(data = dat, deps = vars(wratR, wratA), covs = IQ, factors = c(treat, disab), multivar = list(&quot;wilks&quot;)) This code does not print to PDF but you can run it in R to see the results. This is from the Jamovi package, which also has a point-and-click version of R. The output looks nice. manc.fit$multivar We can also see the output as a data frame. There may be a way to use the broom package to get a pretty output but for now, we can use the round() function on the relevent columns of the output to make it slightly more readable. mancova.jamovi &lt;-manc.fit$multivar$asDF row.names(mancova.jamovi) &lt;- NULL mancova.jamovi[, c(3,4,7)] &lt;- round( mancova.jamovi[, c(3,4,7)], 3) mancova.jamovi ## term[wilks] test[wilks] stat[wilks] f[wilks] df1[wilks] df2[wilks] p[wilks] ## 1 treat Wilks&#39; Lambda 0.099 45.701 2 10 0.000 ## 2 disab Wilks&#39; Lambda 0.174 6.986 4 20 0.001 ## 3 treat:disab Wilks&#39; Lambda 0.939 0.160 4 20 0.956 ## 4 IQ Wilks&#39; Lambda 0.551 4.069 2 10 0.051 Let’s compare those results to that of the MANOVA model without the covariate: man.fit &lt;- manova(cbind(wratR, wratA) ~ treat + disab + treat:disab, data = dat) (man.sum &lt;- summary.manova(man.fit, test = &quot;Wilks&quot;)) ## Df Wilks approx F num Df den Df Pr(&gt;F) ## treat 1 0.13772 34.436 2 11 1.839e-05 *** ## disab 2 0.25526 5.386 4 22 0.003528 ** ## treat:disab 2 0.90807 0.272 4 22 0.893037 ## Residuals 12 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 manc.fit &lt;- manova(cbind(wratR, wratA) ~ treat + disab + IQ + treat:disab, data = dat) (manc.sum &lt;- summary.manova(manc.fit, test = &quot;Wilks&quot;)) ## Df Wilks approx F num Df den Df Pr(&gt;F) ## treat 1 0.09862 45.701 2 10 9.328e-06 *** ## disab 2 0.17401 6.986 4 20 0.001089 ** ## IQ 1 0.55133 4.069 2 10 0.050941 . ## treat:disab 2 0.93896 0.160 4 20 0.956106 ## Residuals 11 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 What do you notice about the F-statistic and p-values for the treatment variable? We can also compare the error SSCP matrices. We see that the model with the covariate has lower covariance matrix elements and a lower generalized variance. det(man.sum$SS$Residuals) ## [1] 292436.3 det(manc.sum$SS$Residuals) ## [1] 171032.8 Compare that to the following model, which has the covariate with a different specification. What do you notice about the ordering of the variables in the right side of the equation? And, what do you notice about the effects of (a) the covariate and (b) the treatment variable compared to the previous MANCOVA model? manc.fit.IQ1 &lt;- manova(cbind(wratR, wratA) ~ IQ + treat + disab + treat:disab, data = dat) (manc.sum.IQ1 &lt;- summary.manova(manc.fit.IQ1, test = &quot;Wilks&quot;)) ## Df Wilks approx F num Df den Df Pr(&gt;F) ## IQ 1 0.23880 15.938 2 10 0.0007765 *** ## treat 1 0.10427 42.952 2 10 1.233e-05 *** ## disab 2 0.25608 4.881 4 20 0.0065404 ** ## treat:disab 2 0.93896 0.160 4 20 0.9561057 ## Residuals 11 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 This next model below is not a MANCOVA because we are including interactions with the covariate. However, we can use this model to examine whether the effect of treatment (or disability) depends on the level of IQ. If it does, we cannot conduct a MANCOVA because this also suggests that the assumption of homogeneity of regression slopes has been violated. As an alternative, we can a multivariate regression with the interaction in the model—if that is appropriate for our research question. Fortunately, in this case, the interaction is not significant, so we can conduct a MANOVA (we probably should have done that first.) manc.fit.IQints &lt;- manova(cbind(wratR, wratA) ~ IQ + treat + disab + IQ:treat + IQ:disab + treat:disab, data = dat) (manc.sum.IQints &lt;- summary.manova(manc.fit.IQints, test = &quot;Wilks&quot;)) ## Df Wilks approx F num Df den Df Pr(&gt;F) ## IQ 1 0.21243 12.976 2 7 0.0044185 ** ## treat 1 0.08324 38.547 2 7 0.0001664 *** ## disab 2 0.22024 3.958 4 14 0.0236389 * ## IQ:treat 1 0.97273 0.098 2 7 0.9077480 ## IQ:disab 2 0.70411 0.671 4 14 0.6227926 ## treat:disab 2 0.93601 0.118 4 14 0.9740075 ## Residuals 8 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We see from the above model that the interactions of the factors with the covariate are not statistically significant; therefore, we can argue support for the assumption of homogeneity of regression slopes—or more accurately for the multivariate model, homogeneity of hyperplanes, as Pituch and Stevens call it. 5.3.2 Let’s think about ordering and Type I sums of squares Here are some things to consider in using R vs SPSS with MANOVA and MANCOVA. SPSS uses Type III sums of squares (which is the go-to sums of squares in regression in both R and SPSS). However, R’s manova() function (which is what the Jamovi package uses) uses only Type I sums of squares, which means that the order of the variables in the model is important. Variables first in the model are estimated without controlling for the other variables. The second variable controls for the first variable; the third for the first two variables, and so forth. The Jamovi package’s macova() function uses Type I errors and includes the covariate after the factors, no matter how many there are. In other words, if we believe that we are presenting our effects after having accounted for the covariates, we would be incorrect. However, if the covariate is completely unrelated to the other independent variables (as is the case in randomized control trials), this would be acceptable. If we use Type I sums of squares, it is important to report that we used Type I sums of squares and that we report the order we included them in the model. It makes sense to include earlier in the equation the variables that are not likely affected by the other variables. For example, if the study takes into account pre-test scores before the intervention was conducted, they can be included first, as we are taking into account all of the variation in the outcome variables that are explained by those pre-intervention test scores. This can also be done for demographic variables that that are not determined by the treatment or other factors in the model. Another way to approach it is to consider if our primary research question was based on an experiment (which is the purpose for which MANOVA was developed), such that cases had been randomly assigned to treatment or control conditions. If that were the case, we could place our treatment factor variable as the first variable in the equation. The rationale is that in expectation the levels of the treatment variable will be identical. Only by random luck (or lack thereof) will the levels of the treatment differ in the covariate means. Let’s examine the model with the covariate as the first entry in the equation. Notice how the parameter for treat has changed. fit &lt;- manova(cbind(wratR, wratA) ~ IQ + treat + disab + treat:disab, data = dat) summary.manova(fit, test = &quot;Wilks&quot;) ## Df Wilks approx F num Df den Df Pr(&gt;F) ## IQ 1 0.23880 15.938 2 10 0.0007765 *** ## treat 1 0.10427 42.952 2 10 1.233e-05 *** ## disab 2 0.25608 4.881 4 20 0.0065404 ** ## treat:disab 2 0.93896 0.160 4 20 0.9561057 ## Residuals 11 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 5.3.2.1 Adjusted means Using the effect() function from the effects package on the output of the aov() model, we can obtain the adjusted means per group; that is, the mean on a DV after partialing out the variance due to the IQ covariate. We can specify the sums-of-squares type with the type = argument, which came from the car package. library(effects) library(car) a1 &lt;- aov(wratR ~ IQ + treat + disab + treat:disab, data = dat) summary(a1, type = &quot;III&quot;) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## IQ 1 215.7 215.7 4.490 0.0577 . ## treat 1 2020.3 2020.3 42.049 4.52e-05 *** ## disab 2 391.2 195.6 4.071 0.0475 * ## treat:disab 2 2.1 1.0 0.021 0.9789 ## Residuals 11 528.5 48.0 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 adj.mean.int &lt;- effect(&quot;treat:disab&quot;, a1, se = TRUE) # but this interaction was NS, so we don&#39;t need this. adj.mean.treat &lt;- effect(&quot;treat&quot;, a1, se = TRUE) summary(adj.mean.treat) ## ## treat effect ## treat ## C T ## 78.39268 99.82954 ## ## Lower 95 Percent Confidence Limits ## treat ## C T ## 73.30206 94.73893 ## ## Upper 95 Percent Confidence Limits ## treat ## C T ## 83.4833 104.9202 adj.mean.disab &lt;- effect(&quot;disab&quot;, a1, se = TRUE) summary(adj.mean.disab) ## ## disab effect ## disab ## Mild Moderate Severe ## 95.39814 88.78388 83.15132 ## ## Lower 95 Percent Confidence Limits ## disab ## Mild Moderate Severe ## 88.94537 82.55259 76.64580 ## ## Upper 95 Percent Confidence Limits ## disab ## Mild Moderate Severe ## 101.85091 95.01517 89.65684 "],["week-05-discriminant-function-analysis.html", "6 Week 05, Discriminant Function Analysis 6.1 Write-up", " 6 Week 05, Discriminant Function Analysis The following code follows the first example in Chapter 10 of Pituch and Stevens. The results are not exactly the same, with some discriminant-function scores being opposite in sign or slightly different from those in SPSS. This is because the algorithms in SPSS and R differ; however, the interpretations are the same. The data are the same as those from previous chapters, in the SENIORWISE.sav data set. library(tidyverse) library(haven) dat &lt;- read_sav(&quot;SENIORWISE.sav&quot;) names(dat) &lt;- tolower(names(dat)) dat$group &lt;- haven::as_factor(dat$group) glimpse(dat) ## Rows: 300 ## Columns: 5 ## $ self_efficacy &lt;dbl&gt; 71.12113, 52.79091, 48.47966, 44.68057, 63.27477, 57.46376, 63.44811, 55.29445… ## $ verbal &lt;dbl&gt; 68.77818, 65.92806, 47.47397, 53.70983, 62.73965, 61.65866, 61.41480, 44.31990… ## $ dafs &lt;dbl&gt; 84.16686, 61.80243, 38.93529, 77.71841, 60.50377, 58.31163, 47.58763, 52.05460… ## $ group &lt;fct&gt; Memory Training, Memory Training, Memory Training, Memory Training, Memory Tra… ## $ case &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22,… 6.0.1 Fitting the discriminant function analysis model As is usual in R, there are many different ways to accomplish the same thing. The MASS package, and its lda() function seems to be one frequently used function for conducting linear discriminant function analysis in R. More conveniently, the DFA.CANCOR package provides output that is similar to that of SPSS and SAS and provides an output that is easy to use. In the DFA.CANCOR::DFA() function, we specify the data, the group variable, the discriminant variables (which were the dependent variables in MANOVA), and we set predictive = FALSE because we’re doing descriptive discriminant analysis (rather than classification, which is discussed later in the chapter). I also set verbos = FALSE but you can set it to TRUE (the default) if you want to see the lengthy output. We can save the output to an object and request specific information, stored as lists (which we can read about in the help file), from that output. You can examine the plot they provide if you set that the TRUE. # install.packages(&quot;DFA.CANCOR&quot;) library(DFA.CANCOR) fit.dfa &lt;- DFA(data = dat, groups = &quot;group&quot;, variables = c(&quot;self_efficacy&quot;, &quot;verbal&quot;, &quot;dafs&quot;), predictive = FALSE, plot = FALSE, verbos = FALSE) # verbose = TRUE is the default and outputs everything. 6.0.2 Dimension reduction analysis Here are the results reported in Table 10.4 (p. 401) of our reading. Here are the eigenvalues: fit.dfa$evals ## Function eigenvalue proportion of variance canonical r ## 1 0.29047037 0.91909386 0.4744352 ## 2 0.02556957 0.08090614 0.1578989 We learned in MANOVA that Wilks’ lambda can be calculated as \\[\\Lambda = \\frac{|\\mathbf{W}|}{|\\mathbf{W} + \\mathbf{B}|}\\] It can also be calculated from the eigenvalues as \\[\\Lambda = \\frac{1}{1 + \\phi_1} \\times \\frac{1}{1 + \\phi_2} \\times \\cdots \\times \\frac{1}{1 + \\phi_r}\\] In R, we can use the following code to compute this equation: Wilks.Lambda &lt;- prod( 1 / (1 + fit.dfa$evals[ ,2]) ) Wilks.Lambda ## [1] 0.7555911 The statistical test is distributed as a \\(\\chi^2\\) with \\(p(k-1)\\) degrees of freedom. N &lt;- nrow(dat) p &lt;- 3 # number of discriminant variables (DVs if we were doing MANOVA) k &lt;- length(levels(dat$group)) df &lt;- p*(k-1) eigenvalues &lt;- fit.dfa$evals[ ,2] Chi.sqr &lt;- (N - 1 - (p + k) / 2) * sum(log(1 + eigenvalues)) Chi.sqr ## [1] 82.95546 And here is the p-value: pchisq(Chi.sqr, df = df, lower.tail = FALSE) ## [1] 8.74953e-16 With the second discriminant function, we use the rest of the eigenvalues in the equation. In this case, there is only one: Wilks.Lambda2 &lt;- prod( 1 / (1 + eigenvalues[2] ) ) Wilks.Lambda2 ## [1] 0.9750679 The statistical test is now of the remaining function(s): df2 &lt;- (p-1)*(k-2) Chi.sqr &lt;- (N - 1 - (p + k) / 2) * sum(log(1 + fit.dfa$evals[2,2])) p.value &lt;- pchisq(Chi.sqr, df = df2, lower.tail = FALSE) Chi.sqr ## [1] 7.473449 p.value ## [1] 0.02383204 Alternatively, we can pull up the results from our output of the DFA() function. Here are the statistical tests: fit.dfa$mv_Wilk ## Wilk&#39;s Lambda F-approx. df1 df2 p ## 1 through 2 0.7555911 14.791342 6 590 8.764499e-16 ## 2 through 2 0.9750679 3.784297 2 296 2.383204e-02 Because Wilk’s Lambda with the second discriminant function resulted in NaN, let’s look at the Pillai’s trace statistics: fit.dfa$mv_Pillai ## Pillai-Bartlett Trace F-approx. df1 df2 p ## 1 through 2 0.25002083 14.096580 6 592 4.933440e-15 ## 2 through 2 0.02493207 3.809866 2 596 2.269257e-02 The Wilks’ lambda for the second discriminant function in R showed up as NaN (not a number), but the Pillai’s trace results were successful, so we used those and conluded that we can use all two discriminant functions. Note that we have three levels to the group variable, so \\(k - 1 = 2\\). We have three discriminant variables (self_efficacy, verbal, and dafs), so \\(p = 3\\). The maximum number of discriminant functions we can get with these data is \\(min(p, k-1)\\), which is two. The model with the second discriminant function is statistically significant, so we can include both the first and second discriminant function in our analysis. If only the first model were significant, we would interpret only that first discriminant function; in other words, we could reduce the number of dimensions to one. 6.0.2.1 Effect sizes We can look at the effect size in two different ways: 1) as the proportion of between-group variance that is accounted for by each discriminant function, and 2) the proportion of variance in the discriminant variable that is between groups. Here’s the first one, which uses the sums-of-squares.26 (N - k)*eigenvalues / sum((N - k)*eigenvalues ) ## ## 0.91909386 0.08090614 Which are the same values in the proportion of variance column in the output: fit.dfa$evals ## Function eigenvalue proportion of variance canonical r ## 1 0.29047037 0.91909386 0.4744352 ## 2 0.02556957 0.08090614 0.1578989 The first discriminant function explains 92% of the between-group variance, which is a strong effect. The second one was 8.10% of the variance, which is much lower. The other effect size is the proportion of variance in the discriminating variable that is explained by the group, which is analogous to eta-squared. canon.cor &lt;- fit.dfa$evals[, 4] canon.cor ## ## 0.4744352 0.1578989 canon.cor^2 ## ## 0.22508876 0.02493207 So, about 23% of the variance in the first discriminant function is explained by the group variable; about 2.50% of that of the second discriminant function is explained by the group variable. 6.0.3 Structure and standardized discriminant function coefficients Here are the results that are displayed in Table 10.5 (p. 403) of the text. fit.dfa$coefs_structure ## Function 1 Function 2 ## self_efficacy -0.8199226 0.3637932 ## verbal -0.7656266 -0.6366741 ## dafs -0.6760304 0.2335248 fit.dfa$coefs_standardized ## Function 1 Function 2 ## self_efficacy -0.5714374 0.5478547 ## verbal -0.4399244 -1.0593537 ## dafs -0.2829758 0.5262326 This is where we interpret what the discriminant functions mean. We use the standardized discriminant-function coefficients for this purpose unless only the first one was found statistically significant. Pituch and Stevens observed that the three standardized coefficients on the first composite variable are somewhat similar. We see that self-efficacy and verbal constituted good proportions of this discriminant function—their standardized DF coefficients indicated that this relationship was, respectively, about about 0.57 and 0.44 of a standard deviation unit of the discriminant function. This result makes sense because self-efficacy in memory skills and verbal memory skills will be related if the self-efficacy belief held by the seniors was accurate. The third one, DAFS (daily functioning skills) was somewhat related to the discriminant function, at 0.28. Note that all three are the same sign, which means the three discriminant variables relate to the discriminant function in the same direction. Our interpretation of this discriminant function is a matter of judgement. Any ideas on how to label this discriminant function? The second discriminant function had one relationship, with verbal memory of -1.06, that stood out stronger than the other two. It was also in the opposite direction to the other two observed variables’ relationships to this composite variable. This second function seems to represent verbal memory beyond what the first function had represented. 6.0.4 Discriminant function scores Each case in our data set has a discriminant function score. This is their score on the composite (AKA discriminant function), which is calculated based on the unstandardized (raw) coefficients. fit.dfa$coefs_raw ## Function 1 Function 2 ## self_efficacy -0.06128037 0.05875138 ## verbal -0.04621916 -0.11129737 ## dafs -0.02972191 0.05527201 For example, Case 1 has a discriminant function score of \\[d_{(i=1),1} = -2.67\\] which comes from the raw coefficients and an intercept. Here are the raw scores on the discriminant variables (the DVs if we do MANOVA) for Case 1 (using some excessive tidyverse coding to get around the troubles with tibbles): (dat[1, 1:3]) %&gt;% data.frame() %&gt;% round(., 2) ## self_efficacy verbal dafs ## 1 71.12 68.78 84.17 Their discriminant function score is calculated as \\[d_{(i=1),1} = 7.37 + -0.0613 (71.12) + -0.0462 (68.78) + -0.0297 (84.17) \\] Here are the results that are displayed in Table 10.9 (p. 407) of the text. We’ll use these to calculate the centroids that are reported in Table 10.6. Each person’s discriminant-function score is recorded in the output, which we can attach to the data. Here, I’m labeling discriminant function 1 and 2 d1 and d2. dat[, c(&quot;d1&quot;, &quot;d2&quot;)] &lt;- fit.dfa$dfa_scores[, -1] head(dat, 10) ## # A tibble: 10 × 7 ## self_efficacy verbal dafs group case d1 d2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 71.1 68.8 84.2 Memory Training 1 -2.67 1.16 ## 2 52.8 65.9 61.8 Memory Training 2 -0.750 -0.834 ## 3 48.5 47.5 38.9 Memory Training 3 1.05 -0.297 ## 4 44.7 53.7 77.7 Memory Training 4 -0.161 0.929 ## 5 63.3 62.7 60.5 Memory Training 5 -1.21 0.0649 ## 6 57.5 61.7 58.3 Memory Training 6 -0.735 -0.277 ## 7 63.4 61.4 47.6 Memory Training 7 -0.772 -0.491 ## 8 55.3 44.3 52.1 Memory Training 8 0.385 1.18 ## 9 52.8 67.7 61.1 Memory Training 9 -0.810 -1.07 ## 10 46.0 52.5 36.8 Memory Training 10 1.03 -1.12 6.0.5 Group means (centroids) of the discriminant-function scores Here are the results that are displayed in Table 10.6 (p. 404) of the text. We can use dplyr’s group_by() and summarize() functions, then save the means as a small data frame, which we’ll use in the plot reported in our reading. We can also add the standard deviations.27 If we want, we can also include the raw scores of the discriminant variables, though these would be earlier in the report when we reported the descriptive statistics. grpcentroids &lt;- dat %&gt;% group_by(group) %&gt;% summarize(M1 = mean(d1), M2 = mean(d2), SD1 = sd(d1), SD2 = sd(d2), m_sef= mean(self_efficacy), m_ver= mean(verbal), m_daf= mean(dafs)) grpcentroids %&gt;% mutate(across(where(is.numeric), round, 2)) ## # A tibble: 3 × 8 ## group M1 M2 SD1 SD2 m_sef m_ver m_daf ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Memory Training -0.76 -0.01 1.01 1.07 58.5 60.2 59.2 ## 2 Health Training 0.36 0.2 0.99 0.95 50.6 50.8 52.4 ## 3 Control 0.4 -0.19 1.01 0.98 49.0 52.9 51.2 Alternatively, we can use the output from the function to get the same report of the centroid means. fit.dfa$centroids ## Function 1 Function 2 ## Memory Training -0.7579728 -0.01040184 ## Health Training 0.3575702 0.19985504 ## Control 0.4004027 -0.18945320 6.0.6 Bivariate scatterplot The plot like that in Figure 10.2 is not automatically generated by the DFA() function, probably because this is only meaningful when we have two discriminant functions, but we can use ggplot with the discriminant-function scores to do something similar. Here, we’re using the discriminant-function scores we had attached to the data, plotting them on a scatterplot, and then using the group centroids we just calculated and saved to the grpcentroids object to overlay the group means. Note that the sign of the discriminant functions is reversed compared to that of SPSS—in other words, those who scored positive in SPSS scored negative in R. What we’re interested in is the difference between the groups, not in their signs, so the interpretation is the same. dat %&gt;% ggplot(aes(x = d1, y = d2)) + geom_point(alpha = .2) + geom_point(data = grpcentroids, aes(x = M1, y = M2, shape = group), color = &quot;dark red&quot;, size = 4) 6.1 Write-up We fit the linear discriminant function analysis to the data using the CFA function from the DFA.CANCOR package (O’Connor, 2022) in R. There were two possible discriminant functions from the data; both were statistically significant and retained for the analysis. The first discriminant function of the multivariate solution accounted for 92% of the between-group variance on these three variables. We examined the associated standardized discriminant-function coefficients (SDFC) and identified that self-efficacy \\((SDFC = -0.575)\\) and verbal memory \\((SDFC = -0.44)\\) were most important in forming the function that discriminated among the three treatment groups. Daily functioning skills (DAFS) contributed somewhat to the function \\((SDFC = -0.28)\\) as well. The structure coefficients indicated that the first two observed measures had strong correlations with the multivariate composite: For self-efficacy, \\(r = -.82\\) and for verbal memory, \\(r = -.77\\); with dafs, there was a moderate correlation \\((r = -.68)\\). For this composite variable, the mean standardized discriminant-function score for the memory-training group was \\(-0.76 (SD = 1.00)\\), for the health-training group was \\(M = 0.36 (SD = 0.99)\\), and for the control group was \\(M = 0.40 (SD = 1.00)\\). Note that these discriminant-function scores are reverse in sign, at least in our model here, of those on the raw scores; in other words, the memory training group had higher scores on this linear combination of self-efficacy and verbal memory (and DAFS to some extent) than the other two groups. The difference was strong—more than one standard-deviation unit. The second discriminant function only explained about 8% of the between-group variance on these discriminant variables. On this composite variable, verbal memory was the strongest contributor, \\((SDFC = -1.06)\\). The SDFCs of the other two discriminant variables (0.55 and 0.53) were about half the strength. The structure coefficients were moderate to weak: With the verbal-memory variable, \\(r = -.64\\); with the other two, the correlations were \\(r = .36\\) and \\(r = .23\\) for self-efficacy and dafs respectively. On this discriminant function, the health-training group \\(M = 0.20 (SD = 0.95)\\) differed from the other two groups, \\(M = -0.01 (SD = 1.01)\\) for the memory training group and \\(M = -0.19 (SD = 1.01)\\) for the dafs group. Though this dimension explains very little of the systematic variance in the model, there is some degree to which the health training had an effect on seniors’ verbal-memory skills beyond what was accounted for in the first composite score. Recall that we can calculate the sums of squares as the variance times the degrees of freedom. The eigenvalues are variances of the discriminant functions.↩︎ Note that because these are standardized scores, the standard deviation of the entire sample across the groups is \\(1.00\\) but within each group, it not necessarily be \\(1.00\\).↩︎ "],["week-06-principal-components-analysis.html", "7 Week 06: Principal Components Analysis 7.1 Example 1, to see some properties of PCA 7.2 Example Two 7.3 Heptathlon example 7.4 Other example from later in the chapter (not assigned) 7.5 Another resource 7.6 References", " 7 Week 06: Principal Components Analysis Principal components analysis is a method developed by Pearson and later, independently, by Hottelling, in the early 20th century. It is a method for reducing the number of dimensions—or components—in a (usually large) set of variables. Originally, this was to identify the first, or principal, component, but there are often more dimensions that may be of interest in our research. The maximum number of components is p (or as Everitt and Hothorn (2011) label it, q), which is the number of variables. In this approach, we can think of the composites as dependent variables and the observed variables as the independent variables. This is similar to discriminant function analysis (DFA) in the sense that we’re creating optimal composites of variables based on the correlations in the data, but different in that—unlike DFA—in PCA we’re not optimizing these composites so that they explain group differences. We’re simply reducing the data to fewer dimensions. We can use the resulting composites scores in follow-up analyses, such as including them as the dependent or independent variable in a regression. Let’s use the examples in Chapter 3 of Everitt and Hothorn (2011). Along the way, we’ll explore several features of principal components analysis, including the scaled and unscaled eigenvectors, the eigenvalues and their relationship to the variance of the components, reproducing the original covariance matrix (or correlation matrix if that is what our PCA is fit to), and different methods for calculating cases’ component scores. We’ll also meddle with different functions for PCA, including princomp(), prcomp(), eigen() of Base R, and the psych package’s pca() function (Revelle 2025).28 If you’re using SPSS, your PCA results will resemble those of the psych package’s output, which under the hood uses these other Base R functions. 7.1 Example 1, to see some properties of PCA Here is the correlation matrix from the chapter. (I simply copied and pasted this and reformatted it, so there might be rounding error discrepancies with what is reported in the chapter.) The N-size is 72 but we do not have individual cases’ data. varbnames &lt;- c(&quot;rblood&quot;, &quot;plate&quot;, &quot;wblood&quot;, &quot;neut&quot;, &quot;lymph&quot;, &quot;bilir&quot;, &quot;sodium&quot;, &quot;potass&quot;) blood_corr &lt;- matrix( c( 1, 0.29, 0.202, -0.055, -0.105, -0.252, -0.229, 0.058, 0.290, 1, 0.415, 0.285, -0.376, -0.349, -0.164, -0.129, 0.202, 0.415, 1, 0.419, -0.521, -0.441, -0.145, -0.076, -0.055, 0.285, 0.419, 1, -0.877, -0.076, 0.023, -0.131, -0.105, -0.376, -0.521, -0.877, 1, 0.206, 0.034, 0.151, -0.252, -0.349, -0.441, -0.076, 0.206, 1, 0.192, 0.077, -0.229, -0.164, -0.145, 0.023, 0.034, 0.192, 1, 0.423, 0.058, -0.129, -0.076, -0.131, 0.151, 0.077, 0.423, 1), ncol = 8, byrow = TRUE, dimnames = list(varbnames, varbnames) ) blood_sd &lt;- c(0.371, 41.253, 1.935, 0.077, 0.071, 4.037, 2.732, 0.297) names(blood_sd) &lt;- varbnames Let’s print the correlation matrix to be sure it is what we expect: blood_corr ## rblood plate wblood neut lymph bilir sodium potass ## rblood 1.000 0.290 0.202 -0.055 -0.105 -0.252 -0.229 0.058 ## plate 0.290 1.000 0.415 0.285 -0.376 -0.349 -0.164 -0.129 ## wblood 0.202 0.415 1.000 0.419 -0.521 -0.441 -0.145 -0.076 ## neut -0.055 0.285 0.419 1.000 -0.877 -0.076 0.023 -0.131 ## lymph -0.105 -0.376 -0.521 -0.877 1.000 0.206 0.034 0.151 ## bilir -0.252 -0.349 -0.441 -0.076 0.206 1.000 0.192 0.077 ## sodium -0.229 -0.164 -0.145 0.023 0.034 0.192 1.000 0.423 ## potass 0.058 -0.129 -0.076 -0.131 0.151 0.077 0.423 1.000 Here is the vector of standard deviations, which are needed to calculate the covariance matrix, which we only look at for comparison purposes: blood_sd ## rblood plate wblood neut lymph bilir sodium potass ## 0.371 41.253 1.935 0.077 0.071 4.037 2.732 0.297 Given the standard deviations and the correlation matrix, we can get the covariance matrix using some matrix algebra. Recall that to get from covariance to correlation, we divide each cell by the product of the two standard-deviations corresponding to the row and column. Here, we’re going in the opposite direction, so we’re multiplying each cell by the product of the row and column standard deviations. 29 Everitt and Hothorn are using the covariance matrix for demonstration of the difference between covariance and correlation—and of what not to do—so we normally wouldn’t care about this if we have the correlation matrix. blood_cov &lt;- diag(blood_sd) %*% blood_corr %*% diag(blood_sd) round(blood_cov, 2 ) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] ## [1,] 0.14 4.44 0.15 0.00 0.00 -0.38 -0.23 0.01 ## [2,] 4.44 1701.81 33.13 0.91 -1.10 -58.12 -18.48 -1.58 ## [3,] 0.15 33.13 3.74 0.06 -0.07 -3.44 -0.77 -0.04 ## [4,] 0.00 0.91 0.06 0.01 0.00 -0.02 0.00 0.00 ## [5,] 0.00 -1.10 -0.07 0.00 0.01 0.06 0.01 0.00 ## [6,] -0.38 -58.12 -3.44 -0.02 0.06 16.30 2.12 0.09 ## [7,] -0.23 -18.48 -0.77 0.00 0.01 2.12 7.46 0.34 ## [8,] 0.01 -1.58 -0.04 0.00 0.00 0.09 0.34 0.09 7.1.1 PCA with Covariance vs. Correlation We can use a PCA on a covariance matrix, correlation matrix, or the raw data. The correlation matrix is better than the covariance matrix if the observed variables are not on the same scale (which is most of the time). Conducting a PCA on the raw data is useful if we want to save each case’s composite scores and use them later on, say, in a regression or a plot. Here is the PCA on the covariance matrix, which is on the original scale of the variables. The results of this are reported on Page 67 of the chapter. blood_pcacov &lt;- princomp(covmat = blood_cov) summary(blood_pcacov, loadings = TRUE) ## Importance of components: ## Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 ## Standard deviation 41.2877486 3.880212624 2.64197339 1.624583979 0.353951757 2.561722e-01 ## Proportion of Variance 0.9856182 0.008705172 0.00403574 0.001525986 0.000072436 3.794288e-05 ## Cumulative Proportion 0.9856182 0.994323381 0.99835912 0.999885108 0.999957544 9.999955e-01 ## Comp.7 Comp.8 ## Standard deviation 8.510631e-02 2.372715e-02 ## Proportion of Variance 4.187837e-06 3.255049e-07 ## Cumulative Proportion 9.999997e-01 1.000000e+00 ## ## Loadings: ## Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8 ## [1,] 0.943 0.329 ## [2,] 0.999 ## [3,] 0.192 0.981 ## [4,] 0.758 0.650 ## [5,] -0.649 0.760 ## [6,] -0.961 0.195 0.191 ## [7,] -0.193 -0.979 ## [8,] 0.329 -0.942 In this output under the label Loadings:, the eigenvector coefficients with very low absolute values (less than 0.10, it seems) are left blank in the printout. These coefficients actually do exist, and we can see them if we ask for them using blood_pcacov$loadings[1:64], where 64 is the number of cells in the eigenvector matrix (8 eigenvectors across 8 items). What do you notice about the first component? How much of the total variance does it explain? Which observed variables have the largest coefficients? (Refer back to the standard deviations of the raw data and look at which variable seems to have a much larger scale, as reflected by the standard deviations.) And, for comparison, here is the PCA on the correlation matrix, which is the preferred method. blood_pcacor &lt;- princomp(covmat = blood_corr) summary(blood_pcacor, loadings = TRUE) ## Importance of components: ## Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 ## Standard deviation 1.6710100 1.2375848 1.1177138 0.88227419 0.78839505 0.69917350 0.66002394 ## Proportion of Variance 0.3490343 0.1914520 0.1561605 0.09730097 0.07769584 0.06110545 0.05445395 ## Cumulative Proportion 0.3490343 0.5404863 0.6966468 0.79394778 0.87164363 0.93274908 0.98720303 ## Comp.8 ## Standard deviation 0.31996216 ## Proportion of Variance 0.01279697 ## Cumulative Proportion 1.00000000 ## ## Loadings: ## Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8 ## rblood 0.194 0.417 0.400 0.652 0.175 0.363 0.176 0.102 ## plate 0.400 0.154 0.168 -0.848 -0.230 -0.110 ## wblood 0.459 0.168 -0.274 0.251 -0.403 0.677 ## neut 0.430 -0.472 -0.171 0.169 0.118 -0.237 0.678 ## lymph -0.494 0.360 -0.180 -0.139 -0.136 0.157 0.724 ## bilir -0.319 -0.320 -0.277 0.633 -0.162 -0.384 0.377 ## sodium -0.177 -0.535 0.410 -0.163 -0.299 0.513 0.367 ## potass -0.171 -0.245 0.709 0.198 -0.469 -0.376 Notice how the correlation matrix has a very different proportion of variance being explained by the first component and that several of the observed variables are contributing to that component. This is very different from what we observed with the covariance matrix. In a correlation matrix, all of the variables are treated equally, being placed on the same scale that is bound between -1 and +1, which in turn provides a more accurate estimate of the components. 7.1.2 Unscaled eigenvectors Note that what the princomp() function labels as “loadings” are actually the unscaled eigenvectors. To get the rescaled eigenvectors, which are more appropriate for interpreting the components, and which are what analysts from a factor-analysis tradition call loadings, we use the equation at the bottom of Page 70 in the chapter: \\[\\mathbf{a}^*_i = \\mathbf{a}_i\\sqrt{\\lambda}_i \\] where \\(i\\) refers to the component number—in this example we have eight possible components. Following Everitt and Hothorn’s labeling system, \\(\\mathbf{a}_i\\) represents the eigenvector for component \\(i\\), and the rescaled eigenvector has an asterisk. Also, the lowercase lambda, \\(\\lambda_i\\), represents the eigenvalue of component \\(i\\). If we stuck these eight columns of vectors side-by-side in a matrix, we have a matrix of eigenvectors, \\(\\mathbf{A}\\) and \\(\\mathbf{A^*}\\), for the unscaled and scaled eigenvectors respectively (described on pp. 70–71 in the chapter). Here are the eight eigenvectors, as they’re reported in the princomp() function under each column, labeled by the component. (Again, these are not loadings—they’re unscaled eigenvectors.) blood_pcacor$loadings ## ## Loadings: ## Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8 ## rblood 0.194 0.417 0.400 0.652 0.175 0.363 0.176 0.102 ## plate 0.400 0.154 0.168 -0.848 -0.230 -0.110 ## wblood 0.459 0.168 -0.274 0.251 -0.403 0.677 ## neut 0.430 -0.472 -0.171 0.169 0.118 -0.237 0.678 ## lymph -0.494 0.360 -0.180 -0.139 -0.136 0.157 0.724 ## bilir -0.319 -0.320 -0.277 0.633 -0.162 -0.384 0.377 ## sodium -0.177 -0.535 0.410 -0.163 -0.299 0.513 0.367 ## potass -0.171 -0.245 0.709 0.198 -0.469 -0.376 ## ## Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8 ## SS loadings 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 ## Proportion Var 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125 ## Cumulative Var 0.125 0.250 0.375 0.500 0.625 0.750 0.875 1.000 The eigenvalues are available from the output, as well. The $sdev includes the standard deviations of each component (based on the unscaled eigenvector coefficients). If we square those, we have the component’s variance, or eigenvalue. There are eight eigenvalues—one for each component and these represent the repackaged variances when we create these components from the variables. blood_pcacor$sdev^2 ## Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8 ## 2.7922743 1.5316161 1.2492841 0.7784077 0.6215668 0.4888436 0.4356316 0.1023758 The components with higher eigenvalues explain more variability in the data. The first component, (which is the “principal” component that Pearson originally looked at), explains most of the variability in our data set. 7.1.3 Looking at the properties of eigenvectors and eigenvalues Our handy R calculator has done the work in obtaining the eigenvectors and eigenvalues for us. The eigenvectors are the weights, analogous to regression coefficients; the eigenvalues represent the variance of the component, with higher values representing a stronger component. To obtain the eigenvectors, two constraints had to be in place: The sum of the squared coefficients is one, \\(\\mathbf{a}_i\\prime\\mathbf{a}_i = 1\\). The cross-products of each eigenvector is zero, \\(\\mathbf{a}_i\\prime\\mathbf{a}_j = 0 \\text{, where } j &gt; i\\). Let’s check this out: Here’s our first eigenvector: a1 &lt;- blood_pcacor$loadings[, 1] a1 ## rblood plate wblood neut lymph bilir sodium potass ## 0.1942203 0.4003625 0.4588793 0.4303359 -0.4937748 -0.3194549 -0.1768857 -0.1705160 Does \\(\\mathbf{a}_1\\prime\\mathbf{a}_1 = 1\\)? In other words, does \\[\\begin{bmatrix} 0.19 &amp; 0.40 &amp; 0.46 &amp; 0.43 &amp; -0.49 &amp; -0.32 &amp; -0.18 &amp; -0.17 \\end{bmatrix} \\begin{bmatrix} 0.19 \\\\ 0.40 \\\\ 0.46 \\\\ 0.43 \\\\ -0.49 \\\\ -0.32 \\\\ -0.18 \\\\ -0.17 \\end{bmatrix} = 1 \\text{ ?}\\] t(a1) %*% a1 ## [,1] ## [1,] 1 It does. Here’s our second eigenvector: a2 &lt;- blood_pcacor$loadings[, 2] a2 ## rblood plate wblood neut lymph bilir sodium ## 0.4171230843 0.1539289974 -0.0002984974 -0.4724424229 0.3604497505 -0.3201664742 -0.5352734751 ## potass ## -0.2452834631 Does \\(\\mathbf{a}_2\\prime\\mathbf{a}_2 = 1\\)? t(a2) %*% a2 ## [,1] ## [1,] 1 It does. This also sums to 1. Okay, how about their cross-products: Does \\[\\begin{bmatrix} 0.19 &amp; 0.40 &amp; 0.46 &amp; 0.43 &amp; -0.49 &amp; -0.32 &amp; -0.18 &amp; -0.17 \\end{bmatrix} \\begin{bmatrix} 0.42 \\\\ 0.15 \\\\ 0.00 \\\\ -0.47 \\\\ 0.36 \\\\ -0.32 \\\\ -0.54 \\\\ -0.25 \\end{bmatrix} = 0 \\text{ ?}\\] round( t(a1) %*% a2 , 3) ## [,1] ## [1,] 0 Indeed it does. This latter condition ensures that the components are orthogonal—that is, unrelated. We only checked the first two components, but we would find the same results with the rest. 7.1.3.1 Eigenvalues add up to total observed variance We can also see that the sum of the eigenvalues (\\(\\lambda_1\\) through \\(\\lambda_8\\)) equals the sum of the variances of the observed variables. In other words, does \\[\\sum{\\lambda_i = s^2_1 + s^2_2 + \\cdots + s^2_p} \\text{ ?}\\] In our case, we fit the PCA to the data’s correlation matrix, which includes 1 on the diagonal, meaning that the variables are scaled to have a variance (and standard deviation) of 1. With that, the sum of our eigenvalues (before we reduce the number of components) should be the number of variables. So we can ask whether the sum of the eigenvalues is 8, given that we have eight variables. sum(blood_pcacor$sdev^2) ## [1] 8 This demonstrates that the amount of variance is the same as it is in our original data, just repackaged. 7.1.3.2 Each component’s proportion of variance What is the proportion of variance that each component explains? Given that we know the total variance, and each eigenvalue, we can use this code: props &lt;- blood_pcacor$sdev^2 / sum(blood_pcacor$sdev^2) round(props, 3) ## Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8 ## 0.349 0.191 0.156 0.097 0.078 0.061 0.054 0.013 If we look up above to the results of summary(blood_pcacor, loadings = TRUE), we see these same values reported in the Proportion of Variance row. 7.1.3.3 Rescaling the eigenvectors Let’s calculate the rescaled eigenvalues so they can be more easily interpreted. These coefficients represent the correlation between the observed variable and the component. These are what are outputted in SPSS and in the psych package’s pca() function, which we’ll use for our actual analysis. First, let’s calculate \\[\\mathbf{a}^*_i = \\mathbf{a}_i\\sqrt{\\lambda}_i \\] We could get the unscaled eigenvalues from the princomp() object using blood_pcacor$loadings[1:64] but that is less convenient (and not as easily reproducible in R code). Instead, we can use the eigen() function on the correlation matrix to get each component’s eigenvector and eigenvalue. vals.and.vects &lt;- eigen(blood_corr) vals.and.vects ## eigen() decomposition ## $values ## [1] 2.7922743 1.5316161 1.2492841 0.7784077 0.6215668 0.4888436 0.4356316 0.1023758 ## ## $vectors ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] ## [1,] -0.1942203 0.4171230843 0.39976114 0.65159275 0.1752060 0.36281561 0.1763116 -0.10240413 ## [2,] -0.4003625 0.1539289974 0.16772869 0.06371996 -0.8476003 -0.23041340 -0.1104646 -0.01017235 ## [3,] -0.4588793 -0.0002984974 0.16777536 -0.27379988 0.2512311 -0.40295337 0.6769694 -0.05038622 ## [4,] -0.4303359 -0.4724424229 -0.17128192 0.16914858 0.1177228 0.06459323 -0.2367450 -0.67792429 ## [5,] 0.4937748 0.3604497505 0.08716408 -0.18037205 -0.1389990 -0.13572092 0.1572459 -0.72364606 ## [6,] 0.3194549 -0.3201664742 -0.27661854 0.63331424 -0.1615438 -0.38374488 0.3765128 0.05214312 ## [7,] 0.1768857 -0.5352734751 0.41027697 -0.16314269 -0.2988956 0.51279925 0.3670544 -0.01484605 ## [8,] 0.1705160 -0.2452834631 0.70861094 0.08690517 0.1978511 -0.46913476 -0.3757113 0.02620828 Let’s plug these into our equation, \\(\\mathbf{a}^*_i = \\mathbf{a}_i\\sqrt{\\lambda}_i\\). We could do each rescaled eigenvector at a time, or we could use this fancy matrix operation. loads &lt;- vals.and.vects$vectors %*% diag(sqrt(vals.and.vects$values)) round(loads, 2) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] ## [1,] -0.32 0.52 0.45 0.57 0.14 0.25 0.12 -0.03 ## [2,] -0.67 0.19 0.19 0.06 -0.67 -0.16 -0.07 0.00 ## [3,] -0.77 0.00 0.19 -0.24 0.20 -0.28 0.45 -0.02 ## [4,] -0.72 -0.58 -0.19 0.15 0.09 0.05 -0.16 -0.22 ## [5,] 0.83 0.45 0.10 -0.16 -0.11 -0.09 0.10 -0.23 ## [6,] 0.53 -0.40 -0.31 0.56 -0.13 -0.27 0.25 0.02 ## [7,] 0.30 -0.66 0.46 -0.14 -0.24 0.36 0.24 0.00 ## [8,] 0.28 -0.30 0.79 0.08 0.16 -0.33 -0.25 0.01 In this code, the diag(sqrt(vals.and.vects$values)) is the square root of each component’s eigenvalue placed on the diagonal in a square matrix, which has zeros on the off-diagonal. With our data, the eigenvalue diagonal matrix (before square-rooting) looks like this, with capital lambda, \\(\\mathbf{\\Lambda}\\), being used to symbolize it given that each element on the diagonal is a little lambda \\(\\lambda_i\\): \\[ \\mathbf{\\Lambda} = \\begin{bmatrix} 2.79 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 1.53 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 1.25 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0.78 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0.62 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0.49 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0.44 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0.10 \\end{bmatrix} \\] This is also represented as \\(\\mathbf{\\lambda} \\mathbf{I}\\) because multiplying the vector of eigenvalues by a \\(p \\times p\\) identity matrix will give the same result as diag(eigenvalues). Again, we’re using the square roots of the diagonal, diag(sqrt(vals.and.vects$values)): \\[ \\begin{bmatrix} \\sqrt{2.79} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; \\sqrt{1.53} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; \\sqrt{1.25} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; \\sqrt{0.78} &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\sqrt{0.62} &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\sqrt{0.49} &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\sqrt{0.44} &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\sqrt{0.10} \\end{bmatrix} \\] When we matrix-multiply the matrix of unscaled eigenvectors (vals.and.vects$vectors) by this diagonal matrix, each eigenvector cell is multiplied by its respective component’s \\(\\sqrt{\\text{eigenvalue}}\\) and placed into its corresponding cell in the resulting matrix, which contains our rescaled eigenvectors (\\(\\mathbf{a}^*_i\\)), AKA “loadings”. This is the set of coefficients that represents the correlation between the observed variable and the component. 7.1.3.4 Reproducing the covariance (or correlation) matrix We can reproduce the matrix from which we performed the principal components analysis on (in this case, the correlation matrix) using the equation at the top of Page 71 in the chapter: \\[\\mathbf{S} = \\mathbf{A^*}\\mathbf{A^*}^\\prime \\] A.star &lt;- loads R.reprod &lt;- A.star %*% t(A.star) round( R.reprod, 3) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] ## [1,] 1.000 0.290 0.202 -0.055 -0.105 -0.252 -0.229 0.058 ## [2,] 0.290 1.000 0.415 0.285 -0.376 -0.349 -0.164 -0.129 ## [3,] 0.202 0.415 1.000 0.419 -0.521 -0.441 -0.145 -0.076 ## [4,] -0.055 0.285 0.419 1.000 -0.877 -0.076 0.023 -0.131 ## [5,] -0.105 -0.376 -0.521 -0.877 1.000 0.206 0.034 0.151 ## [6,] -0.252 -0.349 -0.441 -0.076 0.206 1.000 0.192 0.077 ## [7,] -0.229 -0.164 -0.145 0.023 0.034 0.192 1.000 0.423 ## [8,] 0.058 -0.129 -0.076 -0.131 0.151 0.077 0.423 1.000 We can get the residual covariance matrix from this by subtracting the reproduced covariance from the original covariance (or correlations in this case): round( blood_corr - R.reprod, 3) ## rblood plate wblood neut lymph bilir sodium potass ## rblood 0 0 0 0 0 0 0 0 ## plate 0 0 0 0 0 0 0 0 ## wblood 0 0 0 0 0 0 0 0 ## neut 0 0 0 0 0 0 0 0 ## lymph 0 0 0 0 0 0 0 0 ## bilir 0 0 0 0 0 0 0 0 ## sodium 0 0 0 0 0 0 0 0 ## potass 0 0 0 0 0 0 0 0 This reproduced correlation is indeed the same as our raw correlation matrix. (If we had conducted the PCA on the covariance matrix, it would return the covariance matrix). This demonstrates that the linear recombination of the variables stil represents the covariance of the raw data. But, what if we used only seven of those components instead of all eight? Our reproduced covariance or correlation matrix will be very similar, but not identical to, the original matrix because that last component was explaining some proportion of the variance. Now, we see some residuals. The seven components do not perfectly explain the data. R.reprod3 &lt;- A.star[,1:7] %*% t( (A.star)[,1:7] ) round( blood_corr - R.reprod3, 3) ## rblood plate wblood neut lymph bilir sodium potass ## rblood 0.001 0.000 0.001 0.007 0.008 -0.001 0.000 0.000 ## plate 0.000 0.000 0.000 0.001 0.001 0.000 0.000 0.000 ## wblood 0.001 0.000 0.000 0.003 0.004 0.000 0.000 0.000 ## neut 0.007 0.001 0.003 0.047 0.050 -0.004 0.001 -0.002 ## lymph 0.008 0.001 0.004 0.050 0.054 -0.004 0.001 -0.002 ## bilir -0.001 0.000 0.000 -0.004 -0.004 0.000 0.000 0.000 ## sodium 0.000 0.000 0.000 0.001 0.001 0.000 0.000 0.000 ## potass 0.000 0.000 0.000 -0.002 -0.002 0.000 0.000 0.000 When we reduce the data to fewer components, we’re seeking parsimony while capitalizing on the optimal linear combinations of the variables, but we are not perfectly explaining the data. 7.1.3.5 Summary of the vectors and matrices We have the following vectors and matrices: \\(\\mathbf{a}_i\\) An eigenvector. \\(\\mathbf{a}_1\\) is eigenvector 1, \\(\\mathbf{a}_2\\) is eigenvector 2, and so forth \\(\\mathbf{A}\\) A matrix of eigenvectors. Column 1 is eigenvector 1. Column 2 is eigenvector 2, and so forth \\(\\mathbf{a}^*_i\\) A rescaled eigenvector, so we can interpret the coefficients as correlations (or so-called loadings) \\(\\mathbf{A}^*\\) A matrix of rescaled eigenvectors \\(\\mathbf{\\lambda_i}\\) Lowercase lambda, the eigenvalue of Component \\(i\\) \\(\\mathbf{\\Lambda}\\) Uppercase lambda, a diagonal matrix of eigenvalues, also represented as \\(\\mathbf{\\lambda} \\mathbf{I}\\) \\(\\mathbf{S}\\) The covariance matrix of the observed data \\(\\mathbf{R}\\) The correlation matrix of the observed data It is worth noting that correlation, eigenvectors, and eigenvalues are all related: An eigenvalue matrix is related to the eigevectors and correlation matrix, \\(\\mathbf{\\Lambda} = \\mathbf{A}^\\prime \\mathbf{R} \\mathbf{A}\\) Lambda &lt;- t(vals.and.vects$vectors) %*% blood_corr %*% vals.and.vects$vectors round(Lambda, 2) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] ## [1,] 2.79 0.00 0.00 0.00 0.00 0.00 0.00 0.0 ## [2,] 0.00 1.53 0.00 0.00 0.00 0.00 0.00 0.0 ## [3,] 0.00 0.00 1.25 0.00 0.00 0.00 0.00 0.0 ## [4,] 0.00 0.00 0.00 0.78 0.00 0.00 0.00 0.0 ## [5,] 0.00 0.00 0.00 0.00 0.62 0.00 0.00 0.0 ## [6,] 0.00 0.00 0.00 0.00 0.00 0.49 0.00 0.0 ## [7,] 0.00 0.00 0.00 0.00 0.00 0.00 0.44 0.0 ## [8,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.1 A correlation is reproduced from the eigenvectors and eigenvalues, \\(\\mathbf{R} = \\mathbf{A} \\mathbf{\\Lambda} \\mathbf{A}^\\prime\\), or \\(\\mathbf{R} = (\\mathbf{A} \\mathbf{\\Lambda^{1/2}})( \\mathbf{\\Lambda^{1/2}} \\mathbf{A}^\\prime)\\) R &lt;- vals.and.vects$vectors %*% Lambda %*% t(vals.and.vects$vectors) round(R, 2) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] ## [1,] 1.00 0.29 0.20 -0.06 -0.11 -0.25 -0.23 0.06 ## [2,] 0.29 1.00 0.42 0.29 -0.38 -0.35 -0.16 -0.13 ## [3,] 0.20 0.42 1.00 0.42 -0.52 -0.44 -0.15 -0.08 ## [4,] -0.06 0.28 0.42 1.00 -0.88 -0.08 0.02 -0.13 ## [5,] -0.11 -0.38 -0.52 -0.88 1.00 0.21 0.03 0.15 ## [6,] -0.25 -0.35 -0.44 -0.08 0.21 1.00 0.19 0.08 ## [7,] -0.23 -0.16 -0.15 0.02 0.03 0.19 1.00 0.42 ## [8,] 0.06 -0.13 -0.08 -0.13 0.15 0.08 0.42 1.00 7.1.4 Using the psych package The psych package’s pca() function reports results that are ready for interpretation. It automatically uses the correlation matrix rather than the covariance matrix. We specify the correlation matrix or the raw data, the number of factors we wish to extract, which in a preliminary analysis is the number of observed variables (8 in our example). In PCA, we can avoid rotating the solution for now. library(psych) pc_psy &lt;- psych::pca(blood_corr, # can also use the raw data as input. nfactors = 8, rotate = &#39;none&#39;) print(pc_psy,digits=2) ## Principal Components Analysis ## Call: principal(r = r, nfactors = nfactors, residuals = residuals, ## rotate = rotate, n.obs = n.obs, covar = covar, scores = scores, ## missing = missing, impute = impute, oblique.scores = oblique.scores, ## method = method, use = use, cor = cor, correct = 0.5, weight = NULL) ## Standardized loadings (pattern matrix) based upon correlation matrix ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 h2 u2 com ## rblood 0.32 -0.52 0.45 0.57 -0.14 -0.25 0.12 0.03 1 -4.4e-16 4.2 ## plate 0.67 -0.19 0.19 0.06 0.67 0.16 -0.07 0.00 1 1.1e-15 2.5 ## wblood 0.77 0.00 0.19 -0.24 -0.20 0.28 0.45 0.02 1 1.4e-15 2.5 ## neut 0.72 0.58 -0.19 0.15 -0.09 -0.05 -0.16 0.22 1 2.0e-15 2.6 ## lymph -0.83 -0.45 0.10 -0.16 0.11 0.09 0.10 0.23 1 6.7e-16 2.0 ## bilir -0.53 0.40 -0.31 0.56 0.13 0.27 0.25 -0.02 1 1.8e-15 4.5 ## sodium -0.30 0.66 0.46 -0.14 0.24 -0.36 0.24 0.00 1 1.8e-15 3.7 ## potass -0.28 0.30 0.79 0.08 -0.16 0.33 -0.25 -0.01 1 2.8e-15 2.4 ## ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 ## SS loadings 2.79 1.53 1.25 0.78 0.62 0.49 0.44 0.10 ## Proportion Var 0.35 0.19 0.16 0.10 0.08 0.06 0.05 0.01 ## Cumulative Var 0.35 0.54 0.70 0.79 0.87 0.93 0.99 1.00 ## Proportion Explained 0.35 0.19 0.16 0.10 0.08 0.06 0.05 0.01 ## Cumulative Proportion 0.35 0.54 0.70 0.79 0.87 0.93 0.99 1.00 ## ## Mean item complexity = 3 ## Test of the hypothesis that 8 components are sufficient. ## ## The root mean square of the residuals (RMSR) is 0 ## ## Fit based upon off diagonal values = 1 The output under the label Standardized loadings is the same reweighted eigenvector matrix, \\(\\mathbf{A^*}\\), we calculated above. Again, these represent the correlations between each observed variable and each component. 7.1.5 Determining how many components to retain One of the most valuable reasons for using PCA is for dimension reduction. There are several different methods for determining how many components to retain. Some analysts set the threshold to be sure that they have enough components to explain some pre-specified proportion of variance, such as 80%. Some use Kaiser’s rule of retaining components of values of at least \\(1.00\\), though Joliffe (1972) suggested a threshold of \\(0.70\\). Others (Cattell 1966) suggested using a scree plot to see where the elbow in the curve is, though this may be more appropriate for factor analysis rather than PCA. Another is the parallel test. And still another is the Hull test, which can be used if the PCA is on the data set rather than a correlation matrix. 7.1.5.1 1. Kaiser’s rule The eigenvalue-greater-than-one approach, also called the K1 criterion named after the Kaiser’s (1974) work, can be used in PCA to determine the number of factors to retain. pc_psy$loadings ## ## Loadings: ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 ## rblood 0.325 -0.516 0.447 0.575 -0.138 -0.254 0.116 ## plate 0.669 -0.191 0.187 0.668 0.161 ## wblood 0.767 0.188 -0.242 -0.198 0.282 0.447 ## neut 0.719 0.585 -0.191 0.149 -0.156 0.217 ## lymph -0.825 -0.446 -0.159 0.110 0.104 0.232 ## bilir -0.534 0.396 -0.309 0.559 0.127 0.268 0.249 ## sodium -0.296 0.662 0.459 -0.144 0.236 -0.359 0.242 ## potass -0.285 0.304 0.792 -0.156 0.328 -0.248 ## ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 ## SS loadings 2.792 1.532 1.249 0.778 0.622 0.489 0.436 0.102 ## Proportion Var 0.349 0.191 0.156 0.097 0.078 0.061 0.054 0.013 ## Cumulative Var 0.349 0.540 0.697 0.794 0.872 0.933 0.987 1.000 Based on this criterion, we would retain three components, as the eigenvalues, listed in the SS loadings row are above 1.00 in the first three components. If we used 80% of the variance explained as a criterion instead of Kaiser’s rule, we would look at the pca() output at the Cumulative Proportion line and see that we’d need more at least five components, as Component 4 explains 79% of the variance. It’s borderline, but them’s the rules we would have set for ourselves. 7.1.5.2 2. Scree plot Let’s generate a scree plot. We can use this to not only see those components with eigenvalues that exceed 1.00 but we can see where the plot levels off. Though there are packages that can do this for us, we can do this with ggplot.30 First, we create a data frame that includes the component number and the eigenvalue. For this, we can get the eigenvalues from the psych::pca() outputted object with the $values. vals.and.vects &lt;- data.frame(Component.number = 1:length(pc_psy$values), Eigenvalue = pc_psy$values) # Commented out here is the corresponding code if we used the object outputted # from the `princomp()` function. # vals.and.vects &lt;- data.frame(Component.number = 1:length(blood_pcacor$sdev^2), # Eigenvalue = blood_pcacor$sdev^2) ggplot(vals.and.vects, aes(x = Component.number, y = Eigenvalue ) ) + geom_line() + geom_point() With this plot, we seek to find where the slope changes. The first component is clearly present, as the slope is much steeper from the first to second component than it is from the second to third component, whose slope resembles the rest of the scree. In other words, if we drew a line of best for all of the eigenvalues from Component 2 through 8, such a line would have the points close to it. However, we might decide to retain three components because the eigenvalues after Component 3 seem to flatten out. This is a good example of when this eyeballing technique is not so straightforward. 7.1.5.3 3. Parallel analysis The parallel analysis (Horn 1965) provides a method for comparing our data to randomly generated data sets having the same number of cases and observed variables. After ranking these randomly generated data from low to high, the procedure can estimate the eigenvalue at each \\(n\\)th component at the 95 percentile level. We compare the eigenvalues calculated from our observed data with those of the randomly generated data. If with a given model, such as one with \\(n\\) components, 95% of the randomly drawn samples have eigenvalues that are lower than what we observe in our data, we can assume that the component structure in our data lies outside that 0-to-95 percent range (in other words, our extreme eigenvalues do not likely to occur by random luck). The last component to meet this criterion is the number of components to retain. Up to that point, the components explain more of the variance than would occur by chance at the 95% cutoff. We can use the parallel() function from the nFactors package (Raiche and Magis 2025) to perform a parallel analysis. The parallel analysis takes random draws of data and estimates the eigenvalues that would occur randomly in each random draw. The mean of the random draws is reported, along with the 95 percentile. We want to use the 95 percentile. The first step is to compute the eigenvalues, which we did above with the scree plot. library(nFactors) # help(package=&quot;nFactors&quot;) n_i &lt;- 72 # The number of cases in our data n_p &lt;- ncol(blood_corr) # The number of variables in our data set.seed(123) # To reproduce our randomly generated results. Eigs &lt;- pc_psy$values # The eigenvalues n_components &lt;- length(Eigs) # number of components paral &lt;- parallel(subject = n_i, # The number of cases in our data var = n_p, # The number of variables in our data rep = 100, quantile = .95, model = &quot;components&quot;) ParallelAna &lt;- data.frame(Ncomponent = 1:n_components, Eigs, RandEigM = paral$eigen$mevpea, RandEig95= paral$eigen$qevpea) ParallelAna &lt;- round(ParallelAna, 3) ParallelAna ## Ncomponent Eigs RandEigM RandEig95 ## 1 1 2.792 1.527 1.707 ## 2 2 1.532 1.315 1.454 ## 3 3 1.249 1.159 1.265 ## 4 4 0.778 1.033 1.122 ## 5 5 0.622 0.919 0.980 ## 6 6 0.489 0.803 0.882 ## 7 7 0.436 0.688 0.778 ## 8 8 0.102 0.556 0.660 Based on these results, it looks like we should retain two components, as explained below. First, it is worth mentioning that because this operation is based on random draws of the data, the point estimate and 95th percentile estimate will differ if we performed this operation again. In our code, we used the set.seed() function to make this particular result reproducible. We can remove that line of code or change the seed values and observe different results.31 In our output, we can use the rightmost column and identify at what point 95% of the randomly drawn data’s eigenvalues exceed our reduced-eigenvalue estimates. We see that it is at component number \\(3\\) that our observed reduced eigenvalue (\\(1.249\\)) is exceeded by the randomly generated eigenvalue (\\(1.265\\)). Based on this, we step down to one component below this and decide we should retain \\(2\\) components. How many should we retain? The parallel analysis might make the most sense with these data, as it is compares random draws. With this, we might retain two components. 7.1.6 Reduced PCA model library(psych) pc_psy &lt;- psych::pca(blood_corr, nfactors = 2, rotate = &#39;none&#39;) print(pc_psy, digits = 3) ## Principal Components Analysis ## Call: principal(r = r, nfactors = nfactors, residuals = residuals, ## rotate = rotate, n.obs = n.obs, covar = covar, scores = scores, ## missing = missing, impute = impute, oblique.scores = oblique.scores, ## method = method, use = use, cor = cor, correct = 0.5, weight = NULL) ## Standardized loadings (pattern matrix) based upon correlation matrix ## PC1 PC2 h2 u2 com ## rblood 0.325 -0.516 0.372 0.628 1.68 ## plate 0.669 -0.191 0.484 0.516 1.16 ## wblood 0.767 0.000 0.588 0.412 1.00 ## neut 0.719 0.585 0.859 0.141 1.92 ## lymph -0.825 -0.446 0.880 0.120 1.54 ## bilir -0.534 0.396 0.442 0.558 1.85 ## sodium -0.296 0.662 0.526 0.474 1.38 ## potass -0.285 0.304 0.173 0.827 1.99 ## ## PC1 PC2 ## SS loadings 2.792 1.532 ## Proportion Var 0.349 0.191 ## Cumulative Var 0.349 0.540 ## Proportion Explained 0.646 0.354 ## Cumulative Proportion 0.646 1.000 ## ## Mean item complexity = 1.6 ## Test of the hypothesis that 2 components are sufficient. ## ## The root mean square of the residuals (RMSR) is 0.127 ## ## Fit based upon off diagonal values = 0.823 7.1.7 Interpretation The pysch output is formatted for factor-analysis, which is closely related to PCA but used for a a different purpose (EFA is for estimating latent variables). Here, we have the first two components, PC1 and PC2, the communalities h2, the unique variances, u2, and the complexity, com. These were not explained in our reading but will be in the explanation about exploratory factor analysis. Each observed variable’s communality is the proportion of its variance that is shared with the two factors. If we sum the squared coefficients, we get the communality; for example, with rblood, \\(.325^2 + (-.516)^2 = .372\\). The unique variance is what is not explained by the shared variance; with this example, it is \\(1 - .3718 = .628\\). The complexity refers to how strongly the item loads on other components. The last observed variable, potassium, seems to be evenly shared across two components; it’s complexity is 1.99 and we can see that indeed its loadings on PC1 and PC2 are nearly equal in strength. It is easier to interpret models that have low complexity. These coefficients (or “loadings” to some researchers) are unrotated. In this data set, it seems that the first component is due to a large part from the variability in the lymphocites variable, (-825) as well as to the white blood cell counts, neutrophil levels, platelet counts, bilirubin levels, and to a small extent red blood cell counts. The component seems to be explained in one direction by lymphocyte levels and bilirubin levels and in the opposite direction from the others.32 The second component seems to be an indicator of levels of sodium, neutrophil, red blood cells, lymphocytes, bilirubin, and to a small extent, potassium, with different signs among these. Interpreting these is not easy because the components do not seem to parsimoniously explain the observed variables. When this occurs, many analysts will rotate the solution so that the components more easily align with the variables; others will disagree and state that this changes the structure. 7.1.7.1 Plot Let’s create a plot of the component scores. To prepare for this, let’s save the rescaled eigenvector matrix (the loadings). We’ll use the cbind() function on the $loadings part of the outputted object from our PCA model. A &lt;- cbind(pc_psy$loadings) A ## PC1 PC2 ## rblood 0.3245441 -0.5162251871 ## plate 0.6690097 -0.1905001868 ## wblood 0.7667919 0.0003694159 ## neut 0.7190955 0.5846875596 ## lymph -0.8251026 -0.4460871309 ## bilir -0.5338122 0.3962331606 ## sodium -0.2955777 0.6624463145 ## potass -0.2849339 0.3035590847 A %&gt;% data.frame(.) %&gt;% ggplot(aes(x = PC1, y = PC2)) + geom_vline( aes(xintercept = 0) ) + geom_hline( aes(yintercept = 0) ) + geom_point() + scale_x_continuous(limits=c(-1,1)) + scale_y_continuous(limits=c(-1,1)) + geom_text(hjust=-.2, vjust=0, label = rownames(A)) + labs( title = &quot;Unrotated PCA Solution&quot;) This kind of plot helps us to see which variables are more strongly explaining which component. Those closest to the horizontal axis and farther away from the origin are more strongly contributing to Component 1. Those closer to the vertical axis and farther away from the origin more strongly explain Component 2, which in this case does not seem to include many; sodium is the strongest one with Component 2. 7.1.8 Interpretation of the rotated solution Some researchers caution against rotating the solution, as it changes structure. Others argue for it, as it helps us to explain the components. We specify rotate = varimax for varimax rotation so that the PCA solution remains orthogonal; that is, so that the components do not correlate with each other but rather explain unrelated features of the combination of variables.33 This type of orthogonal rotation is probably fine with data that are assumed to exist in the world rather. With data that are based on social or psychological constructions, such as academic proficiency, orthogonal rotation makes less sense. library(psych) pc_psy_rot &lt;- psych::pca(blood_corr, nfactors = 2, rotate = &#39;varimax&#39;) print(pc_psy_rot, digits = 3) ## Principal Components Analysis ## Call: principal(r = r, nfactors = nfactors, residuals = residuals, ## rotate = rotate, n.obs = n.obs, covar = covar, scores = scores, ## missing = missing, impute = impute, oblique.scores = oblique.scores, ## method = method, use = use, cor = cor, correct = 0.5, weight = NULL) ## Standardized loadings (pattern matrix) based upon correlation matrix ## RC1 RC2 h2 u2 com ## rblood 0.003 -0.610 0.372 0.628 1.00 ## plate 0.467 -0.515 0.484 0.516 1.98 ## wblood 0.651 -0.405 0.588 0.412 1.67 ## neut 0.919 0.116 0.859 0.141 1.03 ## lymph -0.936 0.058 0.880 0.120 1.01 ## bilir -0.244 0.619 0.442 0.558 1.30 ## sodium 0.099 0.719 0.526 0.474 1.04 ## potass -0.081 0.408 0.173 0.827 1.08 ## ## RC1 RC2 ## SS loadings 2.440 1.884 ## Proportion Var 0.305 0.235 ## Cumulative Var 0.305 0.540 ## Proportion Explained 0.564 0.436 ## Cumulative Proportion 0.564 1.000 ## ## Mean item complexity = 1.3 ## Test of the hypothesis that 2 components are sufficient. ## ## The root mean square of the residuals (RMSR) is 0.127 ## ## Fit based upon off diagonal values = 0.823 In this rotated structure, we see it is easier to give a label to the first component as being explained by lymphocyte and neutrophil levels (in opposite directions), and white blood cell and platelet counts. A_rot &lt;- cbind(pc_psy_rot$loadings) A_rot %&gt;% data.frame(.) %&gt;% ggplot(aes(x = RC1, y = RC2)) + geom_vline( aes(xintercept = 0) ) + geom_hline( aes(yintercept = 0) ) + geom_point() + scale_x_continuous(limits=c(-1,1)) + scale_y_continuous(limits=c(-1,1)) + geom_text(hjust=-.2, vjust=0, label = rownames(A)) + labs( title = &quot;Rotated PCA Solution&quot;) In this plot, the variables all remain in same space relative to each other. It is the component axes that have moved. In comparing this to the unrotated solution, it looks like we rotated the axes about 45 degrees. We see that with the rotated solution, the axes of the components have been moved closer to the observed variables, thereby reducing their complexity and making it easier to associate some variables with individual components. This comes at a cost, however, as we also see that now, compared to our unrotated model, the platelet and white-blood cell variables are now somewhere in between the two components. Their complexities are now high whereas before in the unrotated model, they were low because they were both close to the first component axis (the horizontal axis). Whether to rotate or not is a decision made by the researcher. In the social sciences, we tend to rotate the components, though in the social sciences, we should not usually use PCA because we tend to think more about latent constructs, which are best estimated using EFA. 7.2 Example Two Everitt and Hothorn use a data set called headsize which I copied and pasted and saved as a csv file. Each case in this data set is a family and the two observed variables being analyzed are head1 and head2 for the head size of the first and second born male child in the family. This provides an example of how we can perform PCA on a data set instead of the correlation matrix. It also allows for us to see how we can obtain a component score for each observation. There are several ways to calculate component scores (unfortunately). headsize &lt;- read.csv(&quot;headsize.csv&quot;) head_dat &lt;- headsize %&gt;% dplyr::select(head1, head2) Let’s try the prcomp() function for the PCA. This is similar to the princomp() function, which they used in the chapter with these data, but it’s easier to use its output. 7.2.1 PCA on Unstandardized Scores Here’s the PCA Everitt and Hothorn conducted, which was on the raw data and in the original scale. Normally, we do not perform PCA on the unstandardized scale of the data because, as we saw above, variables with larger scales are given more weight.34 head_pca &lt;- prcomp(~ head1 + head2, data = head_dat, retx = FALSE, # Not rotating scale = FALSE) # Using raw data scores instead of standardized scores. head_pca ## Standard deviations (1, .., p=2): ## [1] 12.952459 5.322951 ## ## Rotation (n x k) = (2 x 2): ## PC1 PC2 ## head1 -0.6929858 0.7209512 ## head2 -0.7209512 -0.6929858 summary(head_pca) ## Importance of components: ## PC1 PC2 ## Standard deviation 12.9525 5.3230 ## Proportion of Variance 0.8555 0.1445 ## Cumulative Proportion 0.8555 1.0000 The eigenvalues of the PCA on the non-standardized scores are \\(12.9525^2\\) and \\(5.3230^2\\), or \\(167.767\\) and \\(28.334\\) respectively. 7.2.1.1 Hand calculating component scores from this PCA Following the explanation in Everitt and Horthorn, we can calculate each family’s score on each of the two components after centering their observed scores on the variables. For example, each family’s observed center score on head1 is their deviation score, \\(x_{i1} - \\bar{x}_1\\); on head1, it is \\(x_{i2} - \\bar{x}_2\\). \\[ C_{i1} = 0.693(x_{i1} - \\bar{x}_1) + .721(x_{i2} - \\bar{x}_2) \\] \\[ C_{i2} = .721(x_{i1} - \\bar{x}_1) - .693(x_{i2} - \\bar{x}_2) \\] We can do this in tidyverse: head_dat &lt;- head_dat %&gt;% mutate(C1.cent = 0.693 *(head1 - mean(head1)) + .721*(head2 - mean(head2)), C2.cent = 0.721 *(head1 - mean(head1)) - .693*(head2 - mean(head2)) ) head(head_dat) # PCA can be very ... heady `groan() = TRUE`. ## head1 head2 C1.cent C2.cent ## 1 191 179 0.1694 7.161 ## 2 195 201 18.8034 -5.201 ## 3 181 185 -2.4346 -4.207 ## 4 183 188 1.1144 -4.844 ## 5 176 171 -15.9936 1.890 ## 6 208 192 21.3234 10.409 A more reproducible way to do this same calculation is with the matrix of eigenvectors and the raw scores. Also, whereas above, we calculated the centered scores by hand, we can use the scale() function in Base R to get centered scores, with the arguments center = TRUE and scale = FALSE. (If we set scale = TRUE, we get standardized scores.) We also need to save the eigenvectors as a matrix using A &lt;- cbind(head_pca$rotation). Then we can matrix multiply the matrix of centered scores by the matrix of eigenvectors, \\(\\mathbf{A}\\) head_centrd &lt;- cbind( scale(head_dat[ , c(&quot;head1&quot;, &quot;head2&quot;)], center = TRUE, scale = FALSE) ) A &lt;- cbind(head_pca$rotation) head_dat[, c(&quot;C1.cent&quot;, &quot;C2.cent&quot;)] &lt;- head_centrd %*% A head(head_dat) ## head1 head2 C1.cent C2.cent ## 1 191 179 -0.1695614 7.160674 ## 2 195 201 -18.8024312 -5.201210 ## 3 181 185 2.4345897 -4.206753 ## 4 183 188 -1.1142355 -4.843808 ## 5 176 171 15.9928357 1.890292 ## 6 208 192 -21.3226862 10.408028 We learned that the eigenvalues are the variances of the composite variables. Let’s look into this. head_dat %&gt;% summarize(varC1 = var(C1.cent), varC2 = var(C2.cent)) ## varC1 varC2 ## 1 167.7662 28.33381 These are the same as the square of the standard deviations reported in the output, \\(12.9525^2\\) and \\(5.3230^2\\), resulting in the two eigenvalues, \\(167.767\\) and \\(28.334\\). 7.2.2 PCA on the Standardized Scores We almost always conduct a PCA on the standardized observed data rather than on the unstandardized data because it ensures that some variables, with excessive variance, do not take up all of the variance in the PCA solution. Let’s take a look at the standardized observed scores. stdz_scores &lt;- scale(head_dat[ ,c(&quot;head1&quot;, &quot;head2&quot;)], scale = TRUE) head(stdz_scores) ## head1 head2 ## [1,] 0.5408822 -0.4820596 ## [2,] 0.9506414 1.7091204 ## [3,] -0.4835159 0.1155349 ## [4,] -0.2786363 0.4143322 ## [5,] -0.9957149 -1.2788523 ## [6,] 2.2823588 0.8127286 These scores, by definition, have a mean of zero and a standard deviation of one. We can also recognize that (a) the correlation and covariance matrices are the same when we have standardized data, and (b) the eigen() function on this covariance matrix returns the same eigenvalues and eigenvectors as we will observe in our output below. This simply confirms that a PCA on the standardized observed scores will yield the same PCA results as a PCA on the correlation matrix. cor(stdz_scores) ## head1 head2 ## head1 1.0000000 0.7107518 ## head2 0.7107518 1.0000000 cov(stdz_scores) ## head1 head2 ## head1 1.0000000 0.7107518 ## head2 0.7107518 1.0000000 eigen(cov(stdz_scores)) ## eigen() decomposition ## $values ## [1] 1.7107518 0.2892482 ## ## $vectors ## [,1] [,2] ## [1,] 0.7071068 -0.7071068 ## [2,] 0.7071068 0.7071068 Let’s now go back to our prcomp() function and now use scale = TRUE in the function, which does the scaling for us. We can compare this output with the output above with the PCA on the standardized scores. pc_out &lt;- prcomp(~ head1 + head2, data = head_dat, retx = FALSE, # Not rotating scale = TRUE) # Standardize scores, then perform PCA. pc_out ## Standard deviations (1, .., p=2): ## [1] 1.307957 0.537818 ## ## Rotation (n x k) = (2 x 2): ## PC1 PC2 ## head1 -0.7071068 0.7071068 ## head2 -0.7071068 -0.7071068 summary(pc_out) ## Importance of components: ## PC1 PC2 ## Standard deviation 1.3080 0.5378 ## Proportion of Variance 0.8554 0.1446 ## Cumulative Proportion 0.8554 1.0000 The proportion of variance of each component is the same as it was with the previous model with unstandardized data. As we expect, however, the standard deviations of the components, which is the square root of the eigenvalues, differ from those of the unstandardized model. The eigenvalues of the PCA on the standardized scores are \\(1.3080^2\\) and \\(0.5378^2\\), or \\(1.711\\) and \\(0.289\\). These are the same as those we saw with the eigen() function on the covariance matrix. 7.2.2.1 Calculating component scores First let’s save and look at the unscaled eigenvectors from the standardized data. A &lt;- cbind(pc_out$rotation) A # The two eigenvectors ## PC1 PC2 ## head1 -0.7071068 0.7071068 ## head2 -0.7071068 -0.7071068 Here are the eigenvalues: pc_out$sdev^2 ## [1] 1.7107518 0.2892482 Here are the component scores from this model: head_dat[, c(&quot;C1.stdz&quot;, &quot;C2.stdz&quot;)] &lt;- stdz_scores %*% A head( head_dat ) ## head1 head2 C1.cent C2.cent C1.stdz C2.stdz ## 1 191 179 -0.1695614 7.160674 -0.04159384 0.7233291 ## 2 195 201 -18.8024312 -5.201210 -1.88073559 -0.5363257 ## 3 181 185 2.4345897 -4.206753 0.26020181 -0.4235929 ## 4 183 188 -1.1142355 -4.843808 -0.09595153 -0.4900027 ## 5 176 171 15.9928357 1.890292 1.60836191 0.2002084 ## 6 208 192 -21.3226862 10.408028 -2.18855730 1.0391855 Again, we see that the sample variances of the component scores are the same as the eigenvalues. head_dat %&gt;% summarize(varC1 = var(C1.stdz), varC2 = var(C2.stdz)) ## varC1 varC2 ## 1 1.710752 0.2892482 7.2.2.2 Using the rescaled eigenvectors to interpret the components To get the rescaled eigevectors (or “loadings”), we first get the square root of the eigenvalues. Here are the eigenvalues, again, but this time we’ll place them in a diagonal matrix, \\(\\mathbf{L}\\), which has zeros on the off diagonal because we can then use this for matrix multiplication. L &lt;- diag(pc_out$sdev^2) #Eigenvalues L # We&#39;ll use &quot;L&quot; for capital Lambda. This is the eigenvalue matrix ## [,1] [,2] ## [1,] 1.710752 0.0000000 ## [2,] 0.000000 0.2892482 Here are the scaled eigenvectors, with elements interpreted as correlations with component.35 A_star &lt;- A %*% (sqrt(L)) round(A_star, 3) ## [,1] [,2] ## head1 -0.925 0.38 ## head2 -0.925 -0.38 Component 1 has to do with head size. Component 2 is something else, with two opposing forces, maybe something to do with head shape. That’s about as much interpretation we can do with this very simple data set. 7.2.3 Let the Psych Package Do the Work Having done all that work, let’s use the psych package and let it report the rescaled eigenvalues (loadings). This, by default, uses the correlation matrix rather than the covariance matrix. We can also add a scores = TRUE argument to obtain the component scores; these component scores have been rescaled to be standardized. There seems to be a lot of standardization going on here—we’ve standardized the observed variables and now we’re standardizing the component scores, which is really for aiding interpretation because component scores themselves have no intrinsic scale; by standardizing them, we can at least interpret the component scores as standard-deviation units away from the mean. library(psych) pc_psy &lt;- psych::pca(head_dat[, c(&quot;head1&quot;, &quot;head2&quot;)], nfactors = 2, rotate = &#39;none&#39;, scores = TRUE) print(pc_psy, digits = 3) ## Principal Components Analysis ## Call: principal(r = r, nfactors = nfactors, residuals = residuals, ## rotate = rotate, n.obs = n.obs, covar = covar, scores = scores, ## missing = missing, impute = impute, oblique.scores = oblique.scores, ## method = method, use = use, cor = cor, correct = 0.5, weight = NULL) ## Standardized loadings (pattern matrix) based upon correlation matrix ## PC1 PC2 h2 u2 com ## head1 0.925 -0.38 1 0 1.33 ## head2 0.925 0.38 1 0 1.33 ## ## PC1 PC2 ## SS loadings 1.711 0.289 ## Proportion Var 0.855 0.145 ## Cumulative Var 0.855 1.000 ## Proportion Explained 0.855 0.145 ## Cumulative Proportion 0.855 1.000 ## ## Mean item complexity = 1.3 ## Test of the hypothesis that 2 components are sufficient. ## ## The root mean square of the residuals (RMSR) is 0 ## with the empirical chi square 0 with prob &lt; NA ## ## Fit based upon off diagonal values = 1 We see the loadings are the rescaled eigenvectors, which are what we want. They can be interpreted as the correlation of the observed variable with the component. 7.2.3.1 View the component scores We can obtain the component scores for each case from the outputted object. They are a matrix of however many components we have, which we can save to our data: head_dat[, c(&quot;C1.zscore&quot;, &quot;C2.zscore&quot;) ] &lt;- pc_psy$scores We can verify that these are indeed standardized component scores. head_dat %&gt;% dplyr::select(C1.zscore, C2.zscore) %&gt;% summarize(MC1 = round( mean(C1.zscore), 2), MC2 = round( mean(C2.zscore), 2), varC1 = var(C1.zscore), varC2 = var(C2.zscore)) ## MC1 MC2 varC1 varC2 ## 1 0 0 1 1 We also see that these scores correlate perfectly with the other component scores we calculated earlier from the coefficients. The difference in sign is because the prcomp() function arbitrarily assigns the sign. We also see that the two component scores, no matter how we calculate them, are orthogonal to each other. round( cor(head_dat[, c(&quot;C1.cent&quot;, &quot;C2.cent&quot;, &quot;C1.stdz&quot;, &quot;C2.stdz&quot;, &quot;C1.zscore&quot;, &quot;C2.zscore&quot;) ] ) , 2) ## C1.cent C2.cent C1.stdz C2.stdz C1.zscore C2.zscore ## C1.cent 1.00 0.00 1.00 0.01 -1.00 -0.01 ## C2.cent 0.00 1.00 -0.01 1.00 0.01 -1.00 ## C1.stdz 1.00 -0.01 1.00 0.00 -1.00 0.00 ## C2.stdz 0.01 1.00 0.00 1.00 0.00 -1.00 ## C1.zscore -1.00 0.01 -1.00 0.00 1.00 0.00 ## C2.zscore -0.01 -1.00 0.00 -1.00 0.00 1.00 7.3 Heptathlon example These are Olympic heptathlon data. Let’s use this as an example for a PCA. The steps in a PCA usually include the following, depending on our purpose: Perform data cleaning, to ensure the variables are in the logical direction and so forth Assessing the assumptions of linearity and multivariate normality and absence of outliers Fit the PCA model Determine the number of components to retain Refit the PCA model, using rotation if that is acceptable in your field Interpret the components Save the component scores if the intent is to use them for future analyses. 7.3.1 Data and data cleaning hepdat &lt;- read.csv(&quot;heptathlon.csv&quot;) str(hepdat) ## &#39;data.frame&#39;: 25 obs. of 9 variables: ## $ athlete : chr &quot;Joyner-Kersee (USA)&quot; &quot;John (GDR)&quot; &quot;Behmer (GDR)&quot; &quot;Sablovskaite (URS)&quot; ... ## $ hurdles : num 12.7 12.8 13.2 13.6 13.5 ... ## $ highjump: num 1.86 1.8 1.83 1.8 1.74 1.83 1.8 1.8 1.83 1.77 ... ## $ shot : num 15.8 16.2 14.2 15.2 14.8 ... ## $ run200m : num 22.6 23.6 23.1 23.9 23.9 ... ## $ longjump: num 7.27 6.71 6.68 6.25 6.32 6.33 6.37 6.47 6.11 6.28 ... ## $ javelin : num 45.7 42.6 44.5 42.8 47.5 ... ## $ run800m : num 129 126 124 132 128 ... ## $ score : int 7291 6897 6858 6540 6540 6411 6351 6297 6252 6252 ... Everitt and Hothorn (2011) reversed the scores of the events in which a lower score is a better score to make the direction of the scores consistent in terms of athletic performance. dat &lt;- hepdat %&gt;% mutate(hurdles = max(hurdles) - hurdles, run200m = max(run200m) - run200m, run800m = max(run800m) - run800m) I’m also going to save the variable names in case I need them because I’m too lazy to type them out. varbnames &lt;- names(dat)[2:8] 7.3.2 Examine assumption of linearity We can generate a scatterplot matrix using this simple code on the data with our variables. plot(dat[, varbnames]) The variables do not seem to show non-linear relationships. However, in many cells, except for in the javelin and shot-put events, there seems to be a single case that is an outlier. This might be the same athlete across events. I would guess that this athlete had a leg injury that prevented her from performing well in the events that required running. 7.3.3 Multivariate normality and outliers Let’s examine the Mahalanobis distances to see if we can identify this and any other outlying cases: varbs &lt;- cbind(dat[, varbnames]) distances &lt;- mahalanobis(varbs, center = colMeans(varbs), cov = cov(varbs)) dat$distances &lt;- distances # Let&#39;s check for those who exceed the p &lt; .001 criterion: dat$p &lt;- pchisq(distances, df = (ncol(varbs)-1), # df is number of variables - 1 lower.tail = FALSE) dat$outlier &lt;- ifelse(dat$p &lt; .001, 1, 0) # Temporarily sort the data by p-values and outlier status and get first n rows. dat %&gt;% arrange(desc(outlier), p) %&gt;% slice_head(n = 5) ## athlete hurdles highjump shot run200m longjump javelin run800m score distances ## 1 Launa (PNG) 0.00 1.50 11.78 0.45 4.88 46.38 0.00 4566 18.828022 ## 2 Joyner-Kersee (USA) 3.73 1.86 15.80 4.05 7.27 45.66 34.92 7291 10.153130 ## 3 Yuping (CHN) 2.49 1.86 14.21 1.61 6.40 38.60 16.76 6087 10.101416 ## 4 Hagger (GB) 2.95 1.80 12.75 1.14 6.34 35.76 24.95 5975 9.526282 ## 5 Scheider (SWI) 2.57 1.86 11.58 1.74 6.05 47.50 28.50 6137 9.179544 ## p outlier ## 1 0.004464155 0 ## 2 0.118349731 0 ## 3 0.120445638 0 ## 4 0.146071466 0 ## 5 0.163729573 0 library(car) car::qqPlot(distances, distribution = &quot;chisq&quot;, df = mean(distances), lwd = 1, grid = FALSE, main = &quot;Multi-normal Q-Q Plot on Residuals&quot;, xlab = expression(chi^2 * &quot; quantiles&quot;), ylab = expression(&quot;Mahalanobis distances &quot;^2)) ## [1] 25 8 We see that the 25th row in the data frame includes a case that is far above (or below) the rest. Let’s examine this row: dat[25, ] ## athlete hurdles highjump shot run200m longjump javelin run800m score distances p ## 25 Launa (PNG) 0 1.5 11.78 0.45 4.88 46.38 0 4566 18.82802 0.004464155 ## outlier ## 25 0 It appears that the athlete from Papua New Guinea was the outlier, though not enough to merit removal because she was within the error band and did not have a statistically significant Mahalanobis distance from the rest. However, let’s follow the text’s example and remove this observation from the data. We can compare the results with and without this athlete in a sensitivity analysis if we were doing a complete analysis. dat &lt;- dat[!dat$athlete == &quot;Launa (PNG)&quot;, ] We should also perform a test of multivariate normality. With this small data set, it will likely not result in a statistically significant difference. Let’s get to the analysis and pretend we had done that part. 7.3.4 Re-examine linearity and normality plot(dat[, varbnames], main = &quot;After removal of multivariate outlier&quot;) The scatterplots show that the removal of this outlier resulted in scatterplots with data that are dispersed across the ranges of the variables. Linearity seems to hold. varbs &lt;- cbind(dat[, varbnames]) distances &lt;- mahalanobis(varbs, center = colMeans(varbs), cov = cov(varbs)) dat$distances &lt;- distances library(car) car::qqPlot(distances, distribution = &quot;chisq&quot;, df = mean(distances), lwd = 1, grid = FALSE, main = &quot;After removal of multivariate outlier&quot;, xlab = expression(chi^2 * &quot; quantiles&quot;), ylab = expression(&quot;Mahalanobis distances &quot;^2)) ## [1] 8 15 The Q-Q plot of the multivariate distances suggested that the cases are arguably not violating the assumption of multivariate normality. 7.3.5 Fit the initial PCA fit.pca &lt;- psych::pca(dat[ ,varbnames], nfactors = length(varbnames), rotate = &quot;none&quot; ) print( fit.pca, digits = 2) ## Principal Components Analysis ## Call: principal(r = r, nfactors = nfactors, residuals = residuals, ## rotate = rotate, n.obs = n.obs, covar = covar, scores = scores, ## missing = missing, impute = impute, oblique.scores = oblique.scores, ## method = method, use = use, cor = cor, correct = 0.5, weight = NULL) ## Standardized loadings (pattern matrix) based upon correlation matrix ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 h2 u2 com ## hurdles 0.94 -0.05 -0.16 -0.03 -0.11 -0.29 -0.02 1 3.3e-16 1.3 ## highjump 0.65 0.62 -0.19 0.38 0.04 0.03 0.09 1 1.7e-15 2.9 ## shot 0.84 0.02 -0.14 -0.37 0.37 0.03 0.06 1 1.2e-15 1.9 ## run200m 0.89 -0.18 0.12 -0.16 -0.34 0.11 0.12 1 1.6e-15 1.6 ## longjump 0.94 0.02 -0.25 0.01 -0.07 0.13 -0.20 1 1.2e-15 1.3 ## javelin 0.50 0.31 0.80 -0.04 0.04 -0.02 -0.06 1 5.6e-16 2.1 ## run800m 0.63 -0.62 0.18 0.39 0.17 0.02 0.02 1 4.4e-16 3.0 ## ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 ## SS loadings 4.32 0.90 0.83 0.47 0.30 0.11 0.07 ## Proportion Var 0.62 0.13 0.12 0.07 0.04 0.02 0.01 ## Cumulative Var 0.62 0.75 0.86 0.93 0.97 0.99 1.00 ## Proportion Explained 0.62 0.13 0.12 0.07 0.04 0.02 0.01 ## Cumulative Proportion 0.62 0.75 0.86 0.93 0.97 0.99 1.00 ## ## Mean item complexity = 2 ## Test of the hypothesis that 7 components are sufficient. ## ## The root mean square of the residuals (RMSR) is 0 ## with the empirical chi square 0 with prob &lt; NA ## ## Fit based upon off diagonal values = 1 7.3.6 Determine the number of components to retain fit.pca$Vaccounted ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 ## SS loadings 4.3236422 0.8989944 0.8297417 0.46675769 0.29832218 0.11387578 0.068666022 ## Proportion Var 0.6176632 0.1284278 0.1185345 0.06667967 0.04261745 0.01626797 0.009809432 ## Cumulative Var 0.6176632 0.7460909 0.8646255 0.93130515 0.97392260 0.99019057 1.000000000 ## Proportion Explained 0.6176632 0.1284278 0.1185345 0.06667967 0.04261745 0.01626797 0.009809432 ## Cumulative Proportion 0.6176632 0.7460909 0.8646255 0.93130515 0.97392260 0.99019057 1.000000000 We see that the first variable explained 62% of the variance in the seven events. the second and third components explained an additional 13% and 12% each, whereas the fourth component only explained about 7% of the variation. With this, it may be worthwhile to retain the first three components. Let’s look at the eigenvalues, the scree plot, and the parallel analysis results. Looking at the output, where it says SS loadings, we can see the eigenvalues. Based on this solution, if we were to follow the Kaiser rule of eigenvalues \\(\\ge 1\\), a single component stands out above the rest. We wanted, we could verify this with the eigen() function. eigen(cor(dat[ ,varbnames])) ## eigen() decomposition ## $values ## [1] 4.32364217 0.89899445 0.82974172 0.46675769 0.29832218 0.11387578 0.06866602 ## ## $vectors ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## [1,] -0.4503876 0.05772161 -0.1739345 -0.04840598 -0.19889364 0.84665086 -0.06961672 ## [2,] -0.3145115 -0.65133162 -0.2088272 0.55694554 0.07076358 -0.09007544 0.33155910 ## [3,] -0.4024884 -0.02202088 -0.1534709 -0.54826705 0.67166466 -0.09886359 0.22904298 ## [4,] -0.4270860 0.18502783 0.1301287 -0.23095946 -0.61781764 -0.33279359 0.46971934 ## [5,] -0.4509639 -0.02492486 -0.2697589 0.01468275 -0.12151793 -0.38294411 -0.74940781 ## [6,] -0.2423079 -0.32572229 0.8806995 -0.06024757 0.07874396 0.07193437 -0.21108138 ## [7,] -0.3029068 0.65650503 0.1930020 0.57418128 0.31880178 -0.05217664 0.07718616 To continue the exercise, we can also examine the scree plot and conduct a parallel analysis. Before, we used the parallel() function from the nFactors package to conduct the parallel analysis. That is a better method for reporting, as we can be certain about the 95% cutoff. Another, much easier, option is to use the fa.parallel() function from the psych package. Because we’re using principal components analysis (not factor analysis), we need to specify the model using the argument fa = \"pc\". We’ll also specify the number of factors as the same as the number fo variables. set.seed(123) # To reproduce our randomly generated results. para.psych &lt;- psych::fa.parallel(dat[,varbnames], fa = &quot;pc&quot;, nfactors = length(varbnames)) ## Parallel analysis suggests that the number of factors = NA and the number of components = 1 These results clearly indicate that we can reduce the number of dimensions from seven to 1. To be sure, we should examine the 95 percentile with nFactors package’s parallel() function results. library(nFactors) n_i &lt;- nrow(dat) # The number of cases in our data n_p &lt;- length(varbnames) # The number of variables in our data set.seed(456) # To reproduce our randomly generated results. Eigs &lt;- fit.pca$values # The eigenvalues n_components &lt;- length(Eigs) # number of components paral &lt;- parallel(subject = n_i, # The number of cases in our data var = n_p, # The number of variables in our data rep = 100, quantile = .95, model = &quot;components&quot;) ParallelAna &lt;- data.frame(Ncomponent = 1:n_components, Eigs, RandEigM = paral$eigen$mevpea, RandEig95= paral$eigen$qevpea) ParallelAna &lt;- round(ParallelAna, 3) ParallelAna ## Ncomponent Eigs RandEigM RandEig95 ## 1 1 4.324 1.857 2.242 ## 2 2 0.899 1.447 1.639 ## 3 3 0.830 1.161 1.313 ## 4 4 0.467 0.940 1.063 ## 5 5 0.298 0.748 0.873 ## 6 6 0.114 0.528 0.680 ## 7 7 0.069 0.318 0.505 This code here might be easier to read, though less reproducible. I also changed the repetitions to 1000 to get a more stable result (though the conclusion is the same). set.seed(994) paral &lt;- nFactors::parallel(subject = 24, # The number of cases var = 7, # The number of variables rep = 1000, quantile = .95, model = &quot;components&quot;) ParallelAna &lt;- data.frame(Ncomponent = 1:7, # We have 7 components Eigs, RandEigM = paral$eigen$mevpea, RandEig95= paral$eigen$qevpea) ParallelAna &lt;- round(ParallelAna, 3) ParallelAna ## Ncomponent Eigs RandEigM RandEig95 ## 1 1 4.324 1.862 2.242 ## 2 2 0.899 1.446 1.683 ## 3 3 0.830 1.165 1.330 ## 4 4 0.467 0.938 1.075 ## 5 5 0.298 0.734 0.880 ## 6 6 0.114 0.540 0.696 ## 7 7 0.069 0.315 0.513 This result does suggest that at the second component, the randomly generated eigenvalue at the 95 percentile (1.639) exceeds that of our second observed eigenvalue of 0.899. 7.3.7 Refit the PCA model, using rotation if that is acceptable in your field Let’s refit the PCA model to have a single component. If we had a solution with more than two components, we could use rotation, such as with rotate = \"varimax\". Here, rotation does not matter because there is a single component. fit.pca1 &lt;- psych::pca(dat[ ,varbnames], nfactors = 1, rotate = &quot;varimax&quot;, scores = TRUE) print( fit.pca1, digits = 3) ## Principal Components Analysis ## Call: principal(r = r, nfactors = nfactors, residuals = residuals, ## rotate = rotate, n.obs = n.obs, covar = covar, scores = scores, ## missing = missing, impute = impute, oblique.scores = oblique.scores, ## method = method, use = use, cor = cor, correct = 0.5, weight = NULL) ## Standardized loadings (pattern matrix) based upon correlation matrix ## PC1 h2 u2 com ## hurdles 0.937 0.877 0.123 1 ## highjump 0.654 0.428 0.572 1 ## shot 0.837 0.700 0.300 1 ## run200m 0.888 0.789 0.211 1 ## longjump 0.938 0.879 0.121 1 ## javelin 0.504 0.254 0.746 1 ## run800m 0.630 0.397 0.603 1 ## ## PC1 ## SS loadings 4.324 ## Proportion Var 0.618 ## ## Mean item complexity = 1 ## Test of the hypothesis that 1 component is sufficient. ## ## The root mean square of the residuals (RMSR) is 0.099 ## with the empirical chi square 9.812 with prob &lt; 0.776 ## ## Fit based upon off diagonal values = 0.97 7.3.8 Interpret the components After examining the size of the eigenvalues, the scree plot, and the parallel test, we concluded that the eight variables can be reduced to a single dimension, which explains 62% of the variability in the observed variables. It appears as though all of the events contributed to this component. Particularly influential were the long jump and hurdles, with coefficients exceeding .90. The 200 meter race and the shot put also had strong influence on the component, with coefficients in the .80 to .89 range. The Javelin contributed least strongly to the component score. 7.3.9 Saving the scores We had asked for scores = TRUE when fitting the one-component model. We can attach the scores to the data frame. dat$C.score &lt;- fit.pca1$scores Let’s merge the scores into the dat dataframe so we can compare the component score with the scores allotted in the competition. dat &lt;- left_join(dat, hepdat) Let’s examine the correlation between the component score and the observed score from the competition: cor(dat[ , c(&quot;C.score&quot;, &quot;score&quot;)] ) ## C.score score ## C.score 1.0000000 0.9931168 ## score 0.9931168 1.0000000 We have good evidence to support the assertion that the final score used in the competition reflected the principal sources of variability in among the seven events. 7.4 Other example from later in the chapter (not assigned) 7.4.1 USairpollution Data USair &lt;- read.csv(&quot;USairpollution.csv&quot;) str(USair) ## &#39;data.frame&#39;: 41 obs. of 8 variables: ## $ city : chr &quot;Albany&quot; &quot;Albuquerque&quot; &quot;Atlanta&quot; &quot;Baltimore&quot; ... ## $ SO2 : int 46 11 24 47 11 31 110 23 65 26 ... ## $ temp : num 47.6 56.8 61.5 55 47.1 55.2 50.6 54 49.7 51.5 ... ## $ manu : int 44 46 368 625 391 35 3344 462 1007 266 ... ## $ popul : int 116 244 497 905 463 71 3369 453 751 540 ... ## $ wind : num 8.8 8.9 9.1 9.6 12.4 6.5 10.4 7.1 10.9 8.6 ... ## $ precip : num 33.36 7.77 48.34 41.31 36.11 ... ## $ predays: int 135 58 115 111 166 148 122 132 155 134 ... air &lt;- USair %&gt;% mutate(precip = max(precip) - precip) # Reverse scoring precipitation 7.5 Another resource There are other resources on principal components analysis. One site I found particularly useful in visualizing the components is on STHDA. If you find other resources to share with the class, please do! 7.6 References References Cattell, Raymond B. 1966. “The Scree Test for the Number of Factors.” Multivariate Behavioral Research 1 (2): 245–76. https://doi.org/10.1207/s15327906mbr0102_10. Everitt, Brian, and Torsten Hothorn. 2011. An Introduction to Applied Multivariate Analysis with r. Use r! New York: Springer. Horn, John L. 1965. “A Rationale and Test for the Number of Factors in Factor Analysis.” Psychometrika 30 (2): 179–85. https://doi.org/10.1007/BF02289447. Jolliffe, I. T. 1972. “Discarding Variables in a Principal Component Analysis. I: Artificial Data.” Journal of the Royal Statistical Society. Series C (Applied Statistics) 21 (2): 160–73. https://doi.org/10.2307/2346488. Kaiser, Henry F. 1974. “An Index of Factorial Simplicity.” Psychometrika 39 (1): 31–36. https://doi.org/10.1007/BF02291575. Raiche, Gilles, and David Magis. 2025. nFactors: Parallel Analysis and Other Non Graphical Solutions to the Cattell Scree Test. https://doi.org/10.32614/CRAN.package.nFactors. Revelle, William. 2025. Psych: Procedures for Psychological, Psychometric, and Personality Research. https://personality-project.org/r/psych/. The psych package also uses principal() for the same thing, as pca() wraps principal() into it.↩︎ We can also be lazy and use the cor_to_cov() function from the correlation package (R-correlation?).↩︎ There are packages we can use for this; for example, the factoextra package (R-factoextra?) has the fviz_screeplot() function.↩︎ Strictly speaking, we should only run this procedure one time—or with a single random seed so we can reproduce our one procedure—to avoid phishing for the desired number of components.↩︎ I’m sure I got some of those incorrect as I only have the labels that were presented in Everitt and Hothorn’s (2011) chapter.↩︎ There are other non-orthogonal rotations, which we will use in EFA.↩︎ Some functions, such as the psych pacakge’s pca() function will report PCA results based on the standardized scores by default.↩︎ We are doing double duty in this demonstration by squaring and then rooting but it is to demonstrate that there is a thing called the eigenvalue matrix, which has eigenvalues along the diagonal.↩︎ "],["week-07-exploratory-factor-analysis-part-1.html", "8 Week 07: Exploratory Factor Analysis, Part 1 8.1 Introductory comments 8.2 Preparatory steps 8.3 References", " 8 Week 07: Exploratory Factor Analysis, Part 1 8.1 Introductory comments In this handout, we will do the following: Carry out the preparatory steps to factor analysis Conduct a factor analysis with raw data and examine the output conduct a factor analysis on correlation data 8.1.1 Packages In this handout, we will use several packages. library(tidyverse) library(psych) library(car) library(mvnormtest) library(nFactors) library(EFA.MRFA) As is often the case with other operations in the R world, there exist different packages and functions for performing exploratory factor analysis (EFA). According to a review by Luo and colleagues (2019), the most frequently used and comprehensive package for EFA appears to be the psych package (Revelle 2025). Its fa() function can be used with raw data as well as with correlation or covariance matrices. The function looks like this: psych::fa(r = , nfactors = , n.obs = , fm = , rotate = ) The first argument, which can replace the r =, is for the data, whether it be a data frame of the raw data or a correlation or covariance matrix. The nfactors = argument has as its default 1 but we should determine this number based on our preparatory steps, which we’ll do. The n.obs = is required if we use a correlation or covariance matrix instead of raw data. If we use raw data, we exclude this argument. The fm = argument is for the factor method. There are several options described in the help file. Two common ones are fm = \"pa\" for the principal axis method of factor extraction, and fm = \"ml\" for maximum likelihood method of extraction, but others are available such as minimum residual solution, which is the default in this package. The rotate = argument is for specifying the rotation method. There are several options listed in the help file, including rotate = \"none\", which we can use in our preparatory steps, as well as rotate = \"promax\", \"oblimin\", and others for oblique rotations, which are frequently used if we assume that the constructs underlying the factors are related, as is often the case with studies in the social sciences. There are orthogonal rotations as well, including \"varimax\" and \"quartimax\", and so forth. These are appropriate if we assume that the correlations between the factors should be zero. As usual, we can look in the help files to find other available arguments and options. In the examples in this handout, we will use promax rotation because we will assume that the constructs underlying the factors are related (though we may find evidence suggesting otherwise). We will also use principal axis factoring. 8.1.2 EFA \\(\\ne\\) PCA Principal axis factoring (and any other EFA method) is not to be confused with principal components analysis (PCA), which strictly speaking is not a type of common factor analysis because it generates components rather than factors. Unlike factors, components include the unique variances of the observed variables. This is similar to how we calculate composite scores (on say a classroom test) by using a weighted sum of the variables—PCA finds the best possible weighted sum to explain the variance, including the error variance, in the set of data. PCA is appropriate when we are not interested in an underlying mental construct that explains the observed variables. When we assume that the factors do represent latent traits, common factor analysis is more appropriate than PCA because when we deal with factors, we assume that the factor is what is causing the observed scores to be high or low. 8.2 Preparatory steps Here are some preparatory steps we should consider before jumping into a factor analysis. Examine the data and the correlations Address statistical assumptions Determine the factorability of the data using the KMO and Bartlett tests Identify the number of factors to retain for the factor analysis 8.2.1 Examining the data Let’s import the data. This is a raw data set (rather than a covariance or correlation matrix), so each row represents a person. These data are described in the chapter. We have eight variables, each on a four-point Likert-type scale. Terms used to refer to the observed variables. In this handout, we call the observed variables items because they’re items on a survey. We can also call them subtests because items are smaller parts of a bigger test or survey. raw &lt;- read.csv(&quot;Week07_GSS_Science.csv&quot;) head(raw) ## PID NoIntrst Odd NoFun Boring Alone NoRelign Better Good Help ## 1 S0001 3 3 3 4 3 4 1 1 1 ## 2 S0002 3 2 3 4 3 2 1 2 1 ## 3 S0003 2 2 3 2 3 2 2 2 2 ## 4 S0004 2 3 3 3 1 3 2 2 2 ## 5 S0005 2 3 3 3 1 3 2 1 1 ## 6 S0006 2 3 2 4 3 3 1 2 1 We can also isolate our data frame to include only the columns with the observed data. In our data frame, we have an ID variable, PID, in the first column. So, we can use a -1 in the column index to remove the first column and save our data to a new object, dat. dat &lt;- raw[ , -1] head(dat) ## NoIntrst Odd NoFun Boring Alone NoRelign Better Good Help ## 1 3 3 3 4 3 4 1 1 1 ## 2 3 2 3 4 3 2 1 2 1 ## 3 2 2 3 2 3 2 2 2 2 ## 4 2 3 3 3 1 3 2 2 2 ## 5 2 3 3 3 1 3 2 1 1 ## 6 2 3 2 4 3 3 1 2 1 8.2.1.1 Descriptive statistics Let’s look at the descriptive statistics using describe() from the psych package. We can see our usual descriptive statistics. It seems as though all of our variables are on similar scales, from 1 to 4, with the medians being integers, which makes sense because these are from Likert-type-scale items. Each has a skew and kurtosis within the -2 to +2 range. psych::describe(dat) n M sd Med Min Max Skew Kurtosis seM NoIntrst 371 2.67 0.64 3 1 4 -0.39 0.16 0.03 Odd 371 2.67 0.68 3 1 4 -0.47 0.19 0.04 NoFun 371 2.83 0.64 3 1 4 -0.53 0.79 0.03 Boring 371 2.96 0.60 3 1 4 -0.37 0.98 0.03 Alone 371 2.85 0.67 3 1 4 -0.70 1.08 0.03 NoRelign 371 2.61 0.69 3 1 4 -0.39 0.00 0.04 Better 371 1.90 0.51 2 1 4 0.09 1.71 0.03 Good 371 1.85 0.52 2 1 4 0.17 1.97 0.03 Help 371 1.76 0.46 2 1 4 -0.61 0.74 0.02 8.2.1.1.1 Correlation matrix We should take a look at the correlations among our variables to determine if factor analysis is appropriate. We expect at least some of these to differ from zero. Also, if there are any correlations that are unexpectedly very high in magnitude, such as close to 1 or -1, the two variables may be too collinear, indicating that they are potentially redundant and that we may seek to remove one or combine them. If we are writing a report, we might report this matrix in an appendix. Matrices like this allow future researchers to perform factor analyses on our correlation data if they wish to confirm the results with different model specifications or software. We can also tentatively eyeball the correlations and see if they make sense given the theory (if one exists) or our general understanding of how we would expect the subtest scores to relate to one another in our population. R &lt;- cor(dat) round(R,3) Here is the prettified correlation matrix from these data.36 NoIntrst Odd NoFun Boring Alone NoRelign Better Good Help NoIntrst 1 Odd .504 1 NoFun .454 .388 1 Boring .286 .303 .261 1 Alone .351 .216 .260 .252 1 NoRelign .292 .311 .264 .090 .096 1 Better .123 .138 .113 -.136 .076 .067 1 Good .049 .029 .046 -.131 .031 .051 .470 1 Help .006 .027 .007 -.231 -.010 .046 .425 .402 1 8.2.2 Addressing assumptions The assumptions for factor analysis are, not surprisingly, linearity and multivariate normality, along with absence of outliers (Tabachnick and Fidell 2013).37 Linearity, outliers, and multivariate normality can all have an effect on the Pearson correlations that are analyzed and used in estimating the factor-analysis model. If these assumptions are not met, the legitimacy of our factor-analysis results can come into question. We should also examine whether there is multicollinearity among the variables. Unlike PCA, which is actually one solution for dealing with multicollinearity, EFA will not arrive at a stable solution if there is extreme multicollinearity in the model. This is because it uses matrix inversion, which uses the determinant of the correlation matrix. If that determinant of the correlation matrix is zero or very close to zero, it’s like balancing a mountain on a pin-needle fulcrum. 8.2.2.1 Linearity Linearity is perhaps the most salient assumption. The assumption is that the correlations among the observed variables and factors are linear. Non-linear relationships can result in misspecified correlations and therefore a misspecified factor-analysis model. We might consider examining scatter plots of all pairs of the observed variables. The Base R plot() function on the set of variables provides all possible bivariate scatterplots. plot(dat) Because each of these dependent variables (which we’re calling subtests or items because they’re items in a survey) is on an ordinal scale with four points, it is difficult to tell if there are nonlinear relationships. Among the nine variables, it seems that Help might have some nonlinear relations with the other variables but it is hard to tell because of the ordinal nature of the data. Note that with ordinal data having so few used points on the scale (in this case a maximum of four per variable), a garden-variety factor analysis will not be appropriate—we’ll practice dealing with that in the second part of our factor-analysis journey. This brings up an important point: Linearity rests on the assumption that the observed variables are continuous; that is, that our data are on interval- or ratio-level scales. Categorical and ordered categorical data are not well suited to factor analysis. Generally, if our data have fewer than five functioning categories, we have a hard time claiming that our data are continuous.38 8.2.2.2 Outliers and multivariate normality If we do a complete analysis, we should examine both univariate and multivariate outliers. For the univariate outliers, we can use standardized scores as was done in previous lessons or box-and-whisker plots. For now, let’s focus on the multivariate outliers, as these are often overlooked in research. Let’s examine the Mahalanobis distances to see if we can identify outliers and examine multivariate normality: varbs &lt;- cbind(dat) # Note that in our data, we&#39;re using all of the columns distances &lt;- mahalanobis(varbs, center = colMeans(varbs), cov = cov(varbs)) raw$distances &lt;- distances # Let&#39;s keep a record of those who exceed the p &lt; .001 criterion, if any: raw$p &lt;- pchisq(distances, df = (ncol(varbs)-1), # df is number of variables - 1 lower.tail = FALSE) We can see which cases in our data set are flagged as outliers. I think Base R is better here than tidyverse. # Let&#39;s use Base R to save a column that indicates whether the case is an outlier. raw$outlier &lt;- ifelse(raw$p &lt; .001, 1, 0) # Let&#39;s print those rows for which their outlier status is true: out.liars &lt;- raw[raw$outlier == 1, c(&quot;PID&quot;, &quot;distances&quot;, &quot;p&quot;, &quot;outlier&quot;)] out.liars ## PID distances p outlier ## 50 S0050 33.71409 4.576934e-05 1 ## 64 S0064 35.36739 2.291685e-05 1 ## 66 S0066 31.44294 1.171502e-04 1 ## 79 S0079 29.36322 2.737579e-04 1 ## 117 S0117 26.48447 8.674130e-04 1 ## 124 S0124 29.33548 2.768522e-04 1 ## 126 S0126 27.76881 5.202634e-04 1 ## 136 S0136 42.43203 1.123463e-06 1 ## 160 S0160 28.47775 3.914343e-04 1 ## 181 S0181 37.24053 1.039369e-05 1 ## 201 S0201 29.42374 2.671253e-04 1 ## 248 S0248 43.53517 6.963654e-07 1 ## 310 S0310 30.77297 1.541915e-04 1 ## 322 S0322 26.71397 7.920251e-04 1 ## 328 S0328 57.08836 1.730643e-09 1 ## 331 S0331 38.88572 5.161244e-06 1 ## 348 S0348 28.77977 3.465846e-04 1 There are 17 multivariate outliers. For convenience, we can sort those rows by their distances to see who are the most extreme outliers.39 out.liars %&gt;% arrange(desc(distances)) ## PID distances p outlier ## 1 S0328 57.08836 1.730643e-09 1 ## 2 S0248 43.53517 6.963654e-07 1 ## 3 S0136 42.43203 1.123463e-06 1 ## 4 S0331 38.88572 5.161244e-06 1 ## 5 S0181 37.24053 1.039369e-05 1 ## 6 S0064 35.36739 2.291685e-05 1 ## 7 S0050 33.71409 4.576934e-05 1 ## 8 S0066 31.44294 1.171502e-04 1 ## 9 S0310 30.77297 1.541915e-04 1 ## 10 S0201 29.42374 2.671253e-04 1 ## 11 S0079 29.36322 2.737579e-04 1 ## 12 S0124 29.33548 2.768522e-04 1 ## 13 S0348 28.77977 3.465846e-04 1 ## 14 S0160 28.47775 3.914343e-04 1 ## 15 S0126 27.76881 5.202634e-04 1 ## 16 S0322 26.71397 7.920251e-04 1 ## 17 S0117 26.48447 8.674130e-04 1 It seems like Person S0328 is the biggest outlier, S0248 is the second most outlying case, and so forth. Let’s examine the multivariate Q-Q plot based on the Mahalanobis distances:40 car::qqPlot(distances, distribution = &quot;chisq&quot;, df = mean(distances), lwd = 1, grid = FALSE, main = &quot;Multi-normal Q-Q Plot&quot;, xlab = expression(chi^2 * &quot; quantiles&quot;), ylab = expression(&quot;Mahalanobis distances &quot;^2)) ## [1] 328 248 If we are expected to also report a statistical test of normality, we can use the mulitvariate Shapiro-Wilk test that we used in Week 3. The mshapiro.test() function from the mvnormtest package (Jarek 2024). This function requires the data be arranged as rows instead of columns, so we’ll use the transpose function, t() within the test. library(mvnormtest) mshapiro.test( t(dat) ) ## ## Shapiro-Wilk normality test ## ## data: Z ## W = 0.92551, p-value = 1.251e-12 We see results that are consistent with our outliers and Mahalanobis distance Q-Q plot. The Shapiro-Wilk normality test (W = 0.93, p = &lt; .01) suggests the data are statistically significantly different from a normal distribution. This collection of evidence suggests that the multivariate normality assumption was not met. 8.2.2.2.1 What can we do We have some multivariate outliers. If our objective is to obtain a factor model to serve as evidence in appraising the validity of our score interpretations with this instrument as it is used with the broader population, we might opt to remove these observations before fitting the factor-analysis model. Some scholars may insist we retain outlying observations because they are indeed members of the population; nonetheless, they can have stronger effects on the model estimates than their non-outlying counterparts. On the other hand, if we intend to use the EFA to estimate these observations’ scores, we have little choice but to leave them in the data. Alternatively, we could take a sensitivity-analysis approach by comparing the results of two factor-analysis models, one with these cases removed and one with them included. If the results are arguably similar—such that we reach the same conclusions about the internal structure—we might then decide to retain our outliers as we could then report the results with greater confidence. Whatever our decision is, we should document what we have done. If we retain these outliers and do not perform that sensitivity analysis, we should report that the results of our factor analysis need to be taken with caution. Because our data are not normally distributed, we should avoid using maximum likelihood as the factor extraction method. Furthermore, we should report that our data are not likely normally distributed but that we proceeded with the factor analysis anyway. We could also offer a qualification such as “to the extent that normality fails, the solution is degraded but may still be worthwhile” (Tabachnick and Fidell 2013, 618). 8.2.2.3 Absence of perfect multicollinearity If we’re examining a correlation matrix and find two variables correlate very strongly, such as r = .97, this suggests there is some degree of mutlicollinearity. However, we should also consider that multicollinearity can go unnoticed in an eyeballing of the correlation matrix because it can be among combinations of variables. We can go back to our earlier studies in multiple regression and examine the squared multiple correlation (SMC) of each variable when we regress it on all the other variables. We can also examine the variance inflation factor (VIF), which is calculated from the SMC. With the SMC, if we find an SMC greater than .90 or .95, we have diagnosed multicollinearity. With VIF &gt; 10, which corresponds to an SMC of .90, we can also flag multicollinearity. For EFA, the computational issue has to do with whether we can estimate a factor-analytic structure from the data. If we have perfect multicollinearity in our data set, we will have what is called a singular matrix and a determinant of zero. In that case, we cannot trust the results of the EFA model. With this, an easy way to assess whether there is perfect multicollinearity is to calculate the determinant of the correlation matrix. If that is zero, we have perfect multicollinearity. If it is very close to zero, we should take a look at our data and consider asking why this multicollinearity is present. Let’s use the det() function: det(R) ## [1] 0.1918348 The determinant of the correlation matrix is positive and not very close to zero, so we’re in good shape. We can report something like “We calculated the determinant of the correlation matrix and found it to be positive, suggesting that there will not be problems in estimating the EFA model due to perfect multicollinearity.” If there is perfect multicollinearity, we should remove the variable that is perfectly linearly related with the others, as it is redundant. When strong multicollinearity is present, we have to use our judgment to determine whether the redundant variable(s) are legitimate for our theory or expectations about the data. If we find strong multicollinearity and we want to avoid their overrepresentation in the EFA mdoel, we can replace the set of multicollinear varaibles with a composite of them. For example, if we have three variables that are multicollinear, we can fit a PCA on those three variables and save each observation’s component score, then use that component score in our EFA model. What’s it look like when multicollinearity is present? To demonstrate what the determinant and EFA results would look like if we had perfect multicollinearity, let’s create a temporary data frame, bad, and create a new variable, GoodHelp that is a linear transformation of two of the existing variables (Good and Help), and calculate the determinant of that correlation matrix. bad &lt;- dat bad$GoodHelp &lt;- (bad$Help * .5) + (bad$Good * .5) Rbad &lt;- cor(bad) Notice that we might not observe this perfect multicollinearity in the correlation matrix. Our new variable correlates .81 and .86 with the two variables from which we created it. round(Rbad, 2)[8:10, 8:10] ## Good Help GoodHelp ## Good 1.00 0.40 0.86 ## Help 0.40 1.00 0.81 ## GoodHelp 0.86 0.81 1.00 However, the determinant is zero: det(Rbad) ## [1] 0 If we fit an EFA model, we’ll get errors that say the matrix was not positive definite or that the correlation matrix was singular. Let’s try this and see what happens: fa.bad &lt;- psych::fa(bad, nfactors = ncol(bad), rotate = &quot;none&quot;) ## In smc, smcs &lt; 0 were set to .0 ## In smc, smcs &lt; 0 were set to .0 ## In smc, smcs &lt; 0 were set to .0 ## In factor.scores, the correlation matrix is singular, the pseudo inverse is used Notice the warnings that the matrix was not positive definite or that the correlation matrix was singular. When we conduct an EFA (or CFA) and we have those kinds of messages, we should not trust the results. Sometimes, multicollinearity can go unnoticed if we only rely on the EFA output. Here, let’s again create a second version of our multicollinear variable, but this time make GoodHelp have extreme collinearity instead of perfect collinearity. # This is just for demonstration bad &lt;- dat set.seed(1122) # Creating a tiny random number for each row of the data frame: tiny.rand.num &lt;- rnorm(nrow(bad), 0, .005) # Creating a variable with very strong, but not perfect, collinearity: bad$GoodHelp &lt;-(bad$Help * .5) + (bad$Good * .5) + tiny.rand.num Rbad &lt;- cor(bad) det(Rbad) ## [1] 2.930893e-05 The determinant is very small. If this is the denominator of a fraction (e.g., when we take the inverse of a matrix), the result will be a very big number. If we conducted EFA on this output, we might not see the error but we will notice that the collinear variable and the two from which it came seem to overrepresent the first factor. This can result in a biased interpretation of what that factors mean; we would need to examine our theory of how we expect these items (or subtests) to be related to each other in our decision about how to deal with this strong multicollinearity. If we can remove the redundant variable, we should. fa.bad2 &lt;- psych::fa(bad, nfactors = ncol(bad), rotate = &quot;none&quot;) fa.bad2$loadings ## ## Loadings: ## MR1 MR2 MR3 MR4 MR5 MR6 MR7 MR8 MR9 MR10 ## NoIntrst 0.130 0.749 -0.191 -0.239 0.135 -0.238 ## Odd 0.130 0.726 -0.163 0.385 -0.212 -0.154 0.104 ## NoFun 0.115 0.643 -0.275 0.233 -0.330 ## Boring -0.225 0.562 0.411 0.424 0.186 0.180 ## Alone 0.430 0.119 0.109 0.173 -0.237 -0.111 0.169 0.279 ## NoRelign 0.119 0.413 -0.230 -0.204 -0.321 0.274 0.263 ## Better 0.663 -0.219 0.504 0.174 0.161 ## Good 0.829 0.469 -0.211 -0.193 ## Help 0.800 -0.137 -0.381 0.436 ## GoodHelp 0.973 -0.111 0.110 -0.132 ## ## MR1 MR2 MR3 MR4 MR5 MR6 MR7 MR8 MR9 MR10 ## SS loadings 2.830 2.213 0.643 0.538 0.449 0.385 0.306 0.258 0.153 0.000 ## Proportion Var 0.283 0.221 0.064 0.054 0.045 0.038 0.031 0.026 0.015 0.000 ## Cumulative Var 0.283 0.504 0.569 0.622 0.667 0.706 0.736 0.762 0.778 0.778 If we need to report the VIF If our reviewers demand we report the variance inflation factors, we can use the vif() function from the car package (Fox, Weisberg, and Price 2024). However, this approach assumes we have a regression model or an ANOVA, whereas in EFA, we merely have a bunch of observed variables. A workaround is to use a meaningless numeric variable, such as ID number or case number, on the set of variables and then use the vif() function on that model output. Those variables that exceed 10 are flagged as multicollinear—in our temporary, bad, data set, the last three variables clearly exceed this: bad$casenum &lt;- 1:nrow(bad) lm.for.vif &lt;- lm(casenum ~ NoIntrst + Odd + NoFun + Boring + Alone + NoRelign + Better + Good + Help + GoodHelp, data = bad) car::vif( lm.for.vif) ## NoIntrst Odd NoFun Boring Alone NoRelign Better Good ## 1.635752 1.509177 1.375369 1.278744 1.190307 1.160450 1.464816 2638.768269 ## Help GoodHelp ## 2047.808400 6545.267350 Back to our actual data, without the GoodHelp variable, none of our variables exceeds this VIF &gt; 10 criterion, so we can conclude that multicollinearity is not a problem: raw$casenum &lt;- 1:nrow(raw) lm.for.vif &lt;- lm(casenum ~ NoIntrst + Odd + NoFun + Boring + Alone + NoRelign + Better + Good + Help, data = raw) car::vif( lm.for.vif) ## NoIntrst Odd NoFun Boring Alone NoRelign Better Good Help ## 1.630878 1.508602 1.374342 1.276896 1.190307 1.159588 1.457194 1.374892 1.349730 8.2.3 Determining factorability The Kaiser-Meyer-Olkin (KMO) statistic predicts if data are likely to factor well given the correlations and partial correlations. The statistic, ranging from 0 to 1, roughly estimates the proportion of variance in the data that might be explained by factors. If we use Kaiser’s (1974) guidelines, a suggested cutoff for determining factorability of the sample data is \\(\\text{KMO} \\geq .60\\). psych::KMO(dat) ## Kaiser-Meyer-Olkin factor adequacy ## Call: psych::KMO(r = dat) ## Overall MSA = 0.75 ## MSA for each item = ## NoIntrst Odd NoFun Boring Alone NoRelign Better Good Help ## 0.76 0.76 0.82 0.77 0.79 0.81 0.68 0.68 0.71 The total KMO is 0.75, indicating that, based on this test, we can probably conduct a factor analysis. With the Bartlett’s sphericity test, the null hypothesis is that the correlation matrix of the sample data comes from a population in which the variables have zero correlations with each other. In other words, the correlation matrix in the population is an identity matrix. Since we do need a set of correlated variables to perform a factor analysis, we want H0 to be rejected. psych::cortest.bartlett(dat) ## $chisq ## [1] 604.5854 ## ## $p.value ## [1] 2.276163e-104 ## ## $df ## [1] 36 The test suggests we can reject the null hypothesis that the data are not collinear. This is no surprise to us if we saw in our correlation matrix that there are at least some moderate correlations in our data. Nonetheless, given this and the results of the KMO test, we can proceed with factor analysis. 8.2.4 Determining how many factors to retain It is important to use multiple methods to determine how many factors to retain. Some researchers (not I) use the eigenvalue-greater-than-one approach, also called the K1 criterion named after the Kaiser’s (1974) work, to determining the number of factors to retain. It is important to note that this method was developed for principal components analysis rather than factor analysis. If you do use that approach, interpret the results with caution. It is a good idea to accompany that method with at least one of these other three methods. 8.2.4.1 Parallel analysis We used the parallel analysis (Horn 1965) in the unit on PCA. It provides a method for comparing our data to randomly generated data sets having the same number of persons and subtests. After ranking these randomly generated data from low to high, the procedure can estimate the eigenvalue at each \\(n\\)th factor at the 95 percentile level. We compare the eigenvalues calculated from our observed data with those of the randomly generated data. If with a given model, such as one with \\(n\\) factors, 95% of the randomly drawn samples have eigenvalues that exceed what we observe in our data, we can assume that the factor structure in our data lies outside that 95 percent range (in other words, our eigenvalues do not likely to occur by dumb luck). If this occurs at the \\(n\\)th factor, we can assume that the earlier number of factors explained more of the common variance than would occur by chance. We can use the parallel() function from the nFactors package (Raiche and Magis 2025) to perform a parallel analysis. The parallel analysis takes random draws of data and estimates the eigenvalues that would occur randomly in each random draw. The mean of the random draws is reported, along with the 95 percentile. We want to use the 95 percentile. The first step is to compute the eigenvalues. The eigenvalues in the parallel analysis with factor analysis are from the reduced correlation matrix, meaning that the diagonal of the correlation matrix excludes the subtests’ unique variances. To distinguish these eigenvalues from their counterparts from an unreduced matrix—the initial eigenvalues—we can labels these as reduced or extracted eigenvalues. library(nFactors) # help(package=&quot;nFactors&quot;) n_p &lt;- sum(complete.cases(dat)) # The number of persons in our data n_nu &lt;- ncol(dat) # The number of variables in our data set.seed(123) # To reproduce our randomly generated results. ReducedEig &lt;- eigenComputes(dat, model = &quot;factors&quot;, use = &quot;complete&quot;) n_factors &lt;- length(ReducedEig) paral &lt;- parallel(subject = n_p, var = n_nu, rep = 100, quantile = .95, model = &quot;factors&quot;) ParallelAna &lt;- data.frame(Nfactor = 1:n_factors, ReducedEig, RandEigM = paral$eigen$mevpea, RandEig95= paral$eigen$qevpea) ParallelAna &lt;- round(ParallelAna, 3) ParallelAna # write.csv(ParallelAna,&quot;ParallelAnalysis.csv&quot;,row.names = FALSE) ## Nfactor ReducedEig RandEigM RandEig95 ## 1 1 1.816 0.268 0.346 ## 2 2 1.227 0.181 0.237 ## 3 3 0.127 0.119 0.171 ## 4 4 -0.047 0.064 0.108 ## 5 5 -0.090 0.013 0.050 ## 6 6 -0.114 -0.031 -0.002 ## 7 7 -0.148 -0.083 -0.047 ## 8 8 -0.179 -0.136 -0.100 ## 9 9 -0.237 -0.202 -0.160 In our output, we can use the rightmost column and identify at what point 95% of the randomly drawn data’s eigenvalues exceed our reduced-eigenvalue estimates. We see that it is at factor number \\(3\\) that our observed reduced eigenvalue (\\(0.127\\)) is exceeded by the randomly generated eigenvalue (\\(0.171\\)). Based on this, we step down to one factor below this and decide we should retain \\(2\\) factors.41 8.2.4.1.1 Scree plot We can use the objects we created in the parallel analysis to plot the factor numbers and eigenvalues (or reduced eigenvalues, in this case). This is a scree plot. The term comes from boulders falling down a cliff and collecting at the bottom. library(ggplot2) scree &lt;- data.frame(Factor_n = as.factor(1:n_factors), Eigenvalue = ReducedEig) ggplot(scree, aes(x = Factor_n, y = Eigenvalue, group = 1)) + geom_point() + geom_line() + xlab(&quot;Number of factors&quot;) + labs( title = &quot;Scree Plot&quot;, subtitle = &quot;(Based on the reduced correlation matrix)&quot;) We see that most of the “debris” from the third factor and onward form a kind of even slope. This suggests that the number of factors above this third one might be optimal. Scree plots are somewhat subjective in their interpretation. One could argue that this one has as an inflection point between the third and fourth factor, suggesting we retain three factors instead of just two. Optionally, we can can generate a scree plot that uses the unreduced correlation matrix, which is what other programs such as SPSS and SAS produce. Let’s use the fa() function from the psych package to perform a factor analysis solely to extract the eigenvalues at this stage. One of the arguments is nfactors =, which we do not know at this stage, so we specify it as the number of subtests, which is the number of columns in our data frame ncol(dat): fafitfree &lt;- psych::fa(dat, nfactors = ncol(dat), rotate = &quot;none&quot;) n_factors &lt;- length(fafitfree$e.values) scree &lt;- data.frame( Factor_n = as.factor(1:n_factors), Eigenvalue = fafitfree$e.values) ggplot(scree, aes(x = Factor_n, y = Eigenvalue, group = 1)) + geom_point() + geom_line() + xlab(&quot;Number of factors&quot;) + ylab(&quot;Initial eigenvalue&quot;) + labs( title = &quot;Scree Plot&quot;, subtitle = &quot;(Based on the unreduced correlation matrix)&quot;) The shape of this plot is similar to that generated from the reduced correlation matrix (though the eigenvalues on the Y axis are larger because the unique variance has not yet been removed from the correlation matrix). If we also opted to use the eigenvalue-greater-than-one approach, we could use this plot and see that Factor 3 is below this criterion.42 Finally, we can use the psych package’s fa.parallel() function, which produces the parallel analysis and the scree plot all in one. I would ignore the horizontal line at the Eigenvalue = 1 K1 criterion as that is for eigenvalues from the unreduced correlation matrix or for principal components analysis, not for the reduced correlation matrix, which is what we have here. set.seed(123) # To reproduce our randomly generated parallel data sets. para.psych &lt;- psych::fa.parallel(dat, fa = &quot;fa&quot;, nfactors = length(names(dat))) ## Parallel analysis suggests that the number of factors = 2 and the number of components = NA 8.2.4.1.2 Hull method We can use the Hull method (Lorenzo-Seva, Timmerman, and Kiers 2011) to identify the number of factors at which the fit of the model and the parsimony of the model are a good balance. In a perfect model, we would have a large number of factors but such a model would be inefficient if there is shared variance among the subtests. We can use the hullEFA() function from the EFA.MRFA package (Navarro-Gonzalez and Lorenzo-Seva 2021). library(EFA.MRFA) hullEFA(dat, index_hull = &quot;CFI&quot;) ## HULL METHOD - CFI INDEX ## ## q f g st ## 0 0.0000 36 0.0000 ## 1 0.6930 27 1.9389 ## 2 1.0107 19 433.6852 ## 3 1.0114 12 0.0000 ## ## Number of advised dimensions: 2 ## ## ----------------------------------------------- We included one argument in that function, index_hull = \"CFI\", which sets the index to be the comparative fit index (CFI).43 There are other fit indexes available, as described in the help files (and in Lorenzo-Seva, Timmerman, and Kiers 2011). The CFI is a global fit index that we will use in confirmatory factor analysis. The logical range of the CFI is between 0 and 1, with the best possible fit being 1. We see the estimate at the model with two factors is already at 1 (even estimated to be higher than the logical maximum), so it is a clear indication that more than two factors will not improve the fit. In the output, each row is its own model. The first row is the null model, with 0 factors, the second, a model with 1 factor, and so forth. The f column provides the estimate of the fit index (CFI in our case) for each model. The g column provides the degrees of freedom from this model.44 With more factors, we have fewer degrees of freedom because we are estimating more factor loadings. We want the best return for our use of these degrees of freedom—the best bang for our buck. The rightmost column is the Hull test value. The highest value indicates the optimal number of factors to retain. The graph from the output is provided for us to interpret where there is a leveling off of model fit as we include more factors. The results here are consistent with those of the parallel analysis and the scree plot. We might report something like this: Based on the parallel analysis from the nFactors package (Raiche and Magis 2025), the scree plot with the reduced correlation matrix, and the Hull test from the EFA.MRFA package (Navarro-Gonzalez and Lorenzo-Seva 2021), we determined that the optimal number of factors to retain for our exploratory factor analysis is two. A limitation of the Hull analysis from this package at this time, it seems, is that it requires a raw data set with complete data rather than a correlation matrix. 8.2.5 Factor analysis with a data matrix (raw data) Using the fa() function from the psych package, we can now specify a two factor solution. The first argument is the data set; the second is the number of factors. Let’s use the principal axis factor extraction method (fa = \"pa\"), which is very common. Note that if we had multivariate normality, we could use maximum likelihood estimation, (fa = \"ml\"), but that assumption was not met. Let’s also start with Promax rotation (rather than varimax, which is what we used in PCA). Promax rotation is very common and is an oblique rotation—in other words, it permits correlations among the factors. We select this because factors tend to be correlated in education, psychology, and the other social sciences. If you encounter research that uses orthogonal rotation in the social sciences without also investigating oblique rotation, ask why they assume the factors are assumed to be unrelated; be skeptical. 8.2.5.1 Determine how much of the variance is explained by the n factors Let’s fit the EFA model without rotation to obtain the proportion of variance explained by our two factors. # help(&quot;fa&quot;, package=&quot;psych&quot;) fafit.norot &lt;- psych::fa(dat, nfactors = 2, fm = &quot;pa&quot;, rotate = &quot;none&quot;) print( fafit.norot, digits = 4) ## Factor Analysis using method = pa ## Call: psych::fa(r = dat, nfactors = 2, rotate = &quot;none&quot;, fm = &quot;pa&quot;) ## Standardized loadings (pattern matrix) based upon correlation matrix ## PA1 PA2 h2 u2 com ## NoIntrst 0.7470 -0.0788 0.5643 0.4357 1.022 ## Odd 0.6615 -0.0596 0.4411 0.5589 1.016 ## NoFun 0.6074 -0.0587 0.3724 0.6276 1.019 ## Boring 0.3910 -0.3563 0.2798 0.7202 1.983 ## Alone 0.4123 -0.0674 0.1745 0.8255 1.053 ## NoRelign 0.3870 0.0081 0.1498 0.8502 1.001 ## Better 0.2479 0.6645 0.5030 0.4970 1.273 ## Good 0.1392 0.6235 0.4081 0.5919 1.099 ## Help 0.0732 0.6327 0.4057 0.5943 1.027 ## ## PA1 PA2 ## SS loadings 1.9234 1.3754 ## Proportion Var 0.2137 0.1528 ## Cumulative Var 0.2137 0.3665 ## Proportion Explained 0.5831 0.4169 ## Cumulative Proportion 0.5831 1.0000 ## ## Mean item complexity = 1.2 ## Test of the hypothesis that 2 factors are sufficient. ## ## df null model = 36 with the objective function = 1.6511 with Chi Square = 604.5854 ## df of the model are 19 and the objective function was 0.0619 ## ## The root mean square of the residuals (RMSR) is 0.0286 ## The df corrected root mean square of the residuals is 0.0394 ## ## The harmonic n.obs is 371 with the empirical chi square 21.8428 with prob &lt; 0.2921 ## The total n.obs was 371 with Likelihood Chi Square = 22.5947 with prob &lt; 0.2557 ## ## Tucker Lewis Index of factoring reliability = 0.98797 ## RMSEA index = 0.02242 and the 90 % confidence intervals are 0 0.05298 ## BIC = -89.8131 ## Fit based upon off diagonal values = 0.9863 ## Measures of factor score adequacy ## PA1 PA2 ## Correlation of (regression) scores with factors 0.8795 0.8414 ## Multiple R square of scores with factors 0.7736 0.7079 ## Minimum correlation of possible factor scores 0.5471 0.4157 We can see, where it says Cumulative Var, that the proportion of variance explained is .3665, or 36.65%. 8.2.5.2 Fit the EFA with rotation, for interpretation # help(&quot;fa&quot;, package=&quot;psych&quot;) fafit &lt;- psych::fa(dat, nfactors = 2, fm = &quot;pa&quot;, rotate = &quot;promax&quot;) n_factors &lt;- length(fafit$e.values) fafit ## Factor Analysis using method = pa ## Call: psych::fa(r = dat, nfactors = 2, rotate = &quot;promax&quot;, fm = &quot;pa&quot;) ## Standardized loadings (pattern matrix) based upon correlation matrix ## PA1 PA2 h2 u2 com ## NoIntrst 0.75 0.05 0.56 0.44 1.0 ## Odd 0.66 0.05 0.44 0.56 1.0 ## NoFun 0.61 0.04 0.37 0.63 1.0 ## Boring 0.45 -0.29 0.28 0.72 1.7 ## Alone 0.42 0.00 0.17 0.83 1.0 ## NoRelign 0.38 0.07 0.15 0.85 1.1 ## Better 0.13 0.70 0.50 0.50 1.1 ## Good 0.03 0.64 0.41 0.59 1.0 ## Help -0.03 0.64 0.41 0.59 1.0 ## ## PA1 PA2 ## SS loadings 1.91 1.39 ## Proportion Var 0.21 0.15 ## Cumulative Var 0.21 0.37 ## Proportion Explained 0.58 0.42 ## Cumulative Proportion 0.58 1.00 ## ## With factor correlations of ## PA1 PA2 ## PA1 1 0 ## PA2 0 1 ## ## Mean item complexity = 1.1 ## Test of the hypothesis that 2 factors are sufficient. ## ## df null model = 36 with the objective function = 1.65 with Chi Square = 604.59 ## df of the model are 19 and the objective function was 0.06 ## ## The root mean square of the residuals (RMSR) is 0.03 ## The df corrected root mean square of the residuals is 0.04 ## ## The harmonic n.obs is 371 with the empirical chi square 21.84 with prob &lt; 0.29 ## The total n.obs was 371 with Likelihood Chi Square = 22.59 with prob &lt; 0.26 ## ## Tucker Lewis Index of factoring reliability = 0.988 ## RMSEA index = 0.022 and the 90 % confidence intervals are 0 0.053 ## BIC = -89.81 ## Fit based upon off diagonal values = 0.99 ## Measures of factor score adequacy ## PA1 PA2 ## Correlation of (regression) scores with factors 0.88 0.84 ## Multiple R square of scores with factors 0.77 0.71 ## Minimum correlation of possible factor scores 0.54 0.42 The matrix of factor loadings is presented. These are also called pattern coefficients for oblique rotations such as this one. The factors are the columns, which here have a “PA” affixed to the factor number to remind us that principal axis factoring was used. The qualitative names of these factors that eventually go into a report are usually determined by the researcher and their interpretations of the types of subtests (or items) that are reflected by the factor. The h2 and u2 are communalities and unique variances.45 We can prepare this output so it is more readable. For instance, we can ask that the loadings that do not exceed a cut-off value, such as \\(.32\\), be excluded from the printout.46 This cut-off criterion is a decision rule we should establish before conducting our analysis. We can can also sort the report so that the subtests are grouped by their strongest factor and in the order of loadings within each factor. print(fafit, cut = .32, sort = TRUE, digits = 3) ## Factor Analysis using method = pa ## Call: psych::fa(r = dat, nfactors = 2, rotate = &quot;promax&quot;, fm = &quot;pa&quot;) ## Standardized loadings (pattern matrix) based upon correlation matrix ## item PA1 PA2 h2 u2 com ## NoIntrst 1 0.750 0.564 0.436 1.01 ## Odd 2 0.662 0.441 0.559 1.01 ## NoFun 3 0.609 0.372 0.628 1.01 ## Boring 4 0.445 0.280 0.720 1.71 ## Alone 5 0.418 0.175 0.825 1.00 ## NoRelign 6 0.380 0.150 0.850 1.07 ## Better 7 0.696 0.503 0.497 1.07 ## Good 8 0.638 0.408 0.592 1.01 ## Help 9 0.636 0.406 0.594 1.01 ## ## PA1 PA2 ## SS loadings 1.908 1.391 ## Proportion Var 0.212 0.155 ## Cumulative Var 0.212 0.367 ## Proportion Explained 0.578 0.422 ## Cumulative Proportion 0.578 1.000 ## ## With factor correlations of ## PA1 PA2 ## PA1 1.000 0.003 ## PA2 0.003 1.000 ## ## Mean item complexity = 1.1 ## Test of the hypothesis that 2 factors are sufficient. ## ## df null model = 36 with the objective function = 1.651 with Chi Square = 604.585 ## df of the model are 19 and the objective function was 0.062 ## ## The root mean square of the residuals (RMSR) is 0.029 ## The df corrected root mean square of the residuals is 0.039 ## ## The harmonic n.obs is 371 with the empirical chi square 21.843 with prob &lt; 0.292 ## The total n.obs was 371 with Likelihood Chi Square = 22.595 with prob &lt; 0.256 ## ## Tucker Lewis Index of factoring reliability = 0.988 ## RMSEA index = 0.0224 and the 90 % confidence intervals are 0 0.053 ## BIC = -89.813 ## Fit based upon off diagonal values = 0.986 ## Measures of factor score adequacy ## PA1 PA2 ## Correlation of (regression) scores with factors 0.878 0.843 ## Multiple R square of scores with factors 0.770 0.711 ## Minimum correlation of possible factor scores 0.541 0.423 8.2.5.2.1 More on the percentage of variance accounted for Some advisers and journals like to see a table of proportions of variances, such as Table 12.9 in the reading. We can isolate information from the factor analysis output to get the proportion of variance. PcntVarTable &lt;- data.frame(Factor = 1:n_factors, Eigenval = fafit$e.values, PcntVar = fafit$e.values / n_factors * 100) PcntVarTable$Cumul_Pcnt_var &lt;- cumsum(PcntVarTable$PcntVar) PcntVarTable %&gt;% mutate(across(where(is.numeric), round, 2)) # An alternative to the line obove is Base R: # PcntVarTable[2:4] &lt;- round(PcntVarTable[2:4],2) # PcntVarTable ## Factor Eigenval PcntVar Cumul_Pcnt_var ## 1 1 2.53 28.09 28.09 ## 2 2 1.95 21.69 49.78 ## 3 3 0.95 10.60 60.38 ## 4 4 0.74 8.17 68.55 ## 5 5 0.67 7.42 75.97 ## 6 6 0.62 6.92 82.88 ## 7 7 0.58 6.42 89.30 ## 8 8 0.51 5.68 94.99 ## 9 9 0.45 5.01 100.00 Here’s the Extraction sums of squared loadings table that Bandalos exemplifies in Table 12.7 and which is automatically generated in SPSS. The eigenvalues here are from the EFA model with the specified number of factors before rotation is applied. SS.loadings &lt;- fafit$values[1:2] # We only extracted the first two, so use 1:2 PcntVar &lt;- 100 * (SS.loadings / n_factors ) Cumul_Pcnt_var &lt;- cumsum(PcntVar) data.frame(SS.loadings, PcntVar, Cumul_Pcnt_var) %&gt;% mutate(across(where(is.numeric), round, 3)) ## SS.loadings PcntVar Cumul_Pcnt_var ## 1 1.923 21.371 21.371 ## 2 1.375 15.282 36.653 We can see this also if we fit the EFA on the data without rotation: fafit.unrot &lt;- psych::fa(dat, nfactors = 2, fm = &quot;pa&quot;, rotate = &quot;none&quot;) # no rotation SS.loadings &lt;- diag( t(fafit.unrot$loadings) %*% fafit.unrot$loadings ) PcntVar &lt;- 100 * (SS.loadings / n_factors ) Cumul_Pcnt_var &lt;- cumsum(PcntVar) data.frame(SS.loadings, PcntVar, Cumul_Pcnt_var) %&gt;% mutate(across(where(is.numeric), round, 3)) ## SS.loadings PcntVar Cumul_Pcnt_var ## PA1 1.923 21.371 21.371 ## PA2 1.375 15.282 36.653 8.2.5.3 Reduced correlation matrix Notice that we can reproduced the original correlation matrix from the matrix of factor loadings (pattern coefficients), the correlation matrix among the factors, and the residuals. Bandalos uses \\(\\mathbf{\\Lambda}\\) (uppercase lambda) for the factor loadings matrix, \\(\\mathbf{\\Phi}\\) (uppercase Phi) for the matrix of correlations among the factors, and \\(\\mathbf{\\Theta}\\) (uppercase theta) for the residuals. Unfortunately, this symbol system differs from the notation used in Everitt and Hothorn when we discussed PCA. We will see these symbols again in the CFA chapter. Equation 2.2 in Bandalos (p. 309) is this: \\[ \\mathbf{\\hat{\\Sigma}}_x = \\mathbf{\\Lambda} \\mathbf{\\Phi} \\mathbf{\\Lambda}^\\prime + \\mathbf{\\Theta}\\] (I added a hat to \\(\\mathbf{\\Sigma}\\) because it is an estimate of the population matrix, not the actual population matrix.) Here is the \\(\\Lambda\\) matrix, which is our columns of factor loadings: cbind(fafit$loadings) ## PA1 PA2 ## NoIntrst 0.74965067 0.04542267 ## Odd 0.66211762 0.05019876 ## NoFun 0.60867412 0.04221005 ## Boring 0.44526412 -0.28699704 ## Alone 0.41774831 0.00149331 ## NoRelign 0.38013411 0.07174633 ## Better 0.13275747 0.69628707 ## Good 0.03249128 0.63789387 ## Help -0.03408238 0.63612802 Here is \\(\\Phi\\): fafit$Phi ## PA1 PA2 ## PA1 1.000000000 0.003239124 ## PA2 0.003239124 1.000000000 Notice that the factors very weakly correlate. This is unusual in social-science data. Let’s calculate the reduced correlation matrix: corr_reduced &lt;- fafit$loadings %*% fafit$Phi %*% t(fafit$loadings) round(corr_reduced, 3) ## NoIntrst Odd NoFun Boring Alone NoRelign Better Good Help ## NoIntrst 0.564 0.499 0.458 0.320 0.313 0.288 0.133 0.055 0.005 ## Odd 0.499 0.441 0.405 0.280 0.277 0.256 0.124 0.055 0.011 ## NoFun 0.458 0.405 0.372 0.258 0.254 0.235 0.112 0.048 0.007 ## Boring 0.320 0.280 0.258 0.280 0.185 0.148 -0.140 -0.168 -0.197 ## Alone 0.313 0.277 0.254 0.185 0.175 0.159 0.057 0.015 -0.012 ## NoRelign 0.288 0.256 0.235 0.148 0.159 0.150 0.101 0.059 0.033 ## Better 0.133 0.124 0.112 -0.140 0.057 0.101 0.503 0.449 0.439 ## Good 0.055 0.055 0.048 -0.168 0.015 0.059 0.449 0.408 0.405 ## Help 0.005 0.011 0.007 -0.197 -0.012 0.033 0.439 0.405 0.406 Notice that if we add the residuals, we get the original, observed correlation matrix: Theta &lt;- fafit$residual round( corr_reduced + Theta , 3) ## NoIntrst Odd NoFun Boring Alone NoRelign Better Good Help ## NoIntrst 1.000 0.504 0.454 0.286 0.351 0.292 0.123 0.049 0.006 ## Odd 0.504 1.000 0.388 0.303 0.216 0.311 0.138 0.029 0.027 ## NoFun 0.454 0.388 1.000 0.261 0.260 0.264 0.113 0.046 0.007 ## Boring 0.286 0.303 0.261 1.000 0.252 0.090 -0.136 -0.131 -0.231 ## Alone 0.351 0.216 0.260 0.252 1.000 0.096 0.076 0.031 -0.010 ## NoRelign 0.292 0.311 0.264 0.090 0.096 1.000 0.067 0.051 0.046 ## Better 0.123 0.138 0.113 -0.136 0.076 0.067 1.000 0.470 0.425 ## Good 0.049 0.029 0.046 -0.131 0.031 0.051 0.470 1.000 0.402 ## Help 0.006 0.027 0.007 -0.231 -0.010 0.046 0.425 0.402 1.000 Also, notice that the diagonal of the reduced correlation matrix contains the communalities of the EFA model: diag(corr_reduced) ## NoIntrst Odd NoFun Boring Alone NoRelign Better Good Help ## 0.5642599 0.4411350 0.3724323 0.2797996 0.1745199 0.1498262 0.5030391 0.4080985 0.4056800 This is the same as that from the EFA output: fafit$communality ## NoIntrst Odd NoFun Boring Alone NoRelign Better Good Help ## 0.5642599 0.4411350 0.3724323 0.2797996 0.1745199 0.1498262 0.5030391 0.4080985 0.4056800 This reduced correlation is what distinguishes EFA from PCA. The factors are based on this reduced correlation matrix, after removing the error. 8.2.5.4 Examining pattern coefficients (the factor loadings) We can now see which subtests load on which factors. In a diagram, these would be the loadings on the arrows from the factor to each subtest. Factor loadings are also called pattern coefficients (the latter being a more accurate term for oblique rotations). The squared pattern coefficients represent the percentage of variance in the observed subtest that is explained by the factor after partialing out the variance explained by the other factors. The fafit$loadings reports the salient pattern coefficients. In the output, none of the coefficients seems to be complex in this example. We see that none has a loading on another factor with an absolute value greater than our specified cutoff of \\(.32\\). fafit$loadings ## ## Loadings: ## PA1 PA2 ## NoIntrst 0.750 ## Odd 0.662 ## NoFun 0.609 ## Boring 0.445 -0.287 ## Alone 0.418 ## NoRelign 0.380 ## Better 0.133 0.696 ## Good 0.638 ## Help 0.636 ## ## PA1 PA2 ## SS loadings 1.908 1.390 ## Proportion Var 0.212 0.154 ## Cumulative Var 0.212 0.366 We can isolate the part of the output that has all of the values and round them to a certain number of decimal places if we choose. round(fafit$loadings[1:n_factors,],3) ## PA1 PA2 ## NoIntrst 0.750 0.045 ## Odd 0.662 0.050 ## NoFun 0.609 0.042 ## Boring 0.445 -0.287 ## Alone 0.418 0.001 ## NoRelign 0.380 0.072 ## Better 0.133 0.696 ## Good 0.032 0.638 ## Help -0.034 0.636 Let’s assign these loadings to an object called PattM to remind us this is the pattern matrix. We’ll use this in our report. PattM &lt;- fafit$loadings[1:n_factors,] #write.csv(PatternM,&quot;PatternMatrix.csv&quot;,row.names=FALSE) If we use these same data with another program, such as SPSS, and produce the pattern matrix, the result will be close but may not be precisely the same as that produced here because of then different algorithms used in the programs (Revelle 2025).47 We can look at the factor diagram using the fa.diagram() function in the psych package. This will display the loadings (i.e., the pattern coefficients in this oblique model). We use the cut = .32 argument to display only those parameters with an absolute value exceeding the criterion we decided a priori. We use the simple = FALSE argument in case any variable has a loading on more than one factor that exceeds our cut-off criterion (setting it to TRUE will display only the variable’s largest loading, which can hide important information if there are complex loadings). fa.diagram(fafit, digits = 2, main = &quot;Factor Diagram&quot;, cut = .32, simple = F, errors = T) Also, notice that there does not appear to be a correlation between the two factors, at least not one that exceeds our cut value. The interfactor correlations make up the off-diagonals in the Phi matrix. round(fafit$Phi,3) ## PA1 PA2 ## PA1 1.000 0.003 ## PA2 0.003 1.000 The correlation between the two factors is indeed very weak. Let’s examine the structure matrix of the coefficients. fafit$Structure[1:n_factors,] ## PA1 PA2 ## NoIntrst 0.74979780 0.047850884 ## Odd 0.66228022 0.052343438 ## NoFun 0.60881084 0.044181626 ## Boring 0.44433450 -0.285554773 ## Alone 0.41775315 0.002846448 ## NoRelign 0.38036651 0.072977632 ## Better 0.13501283 0.696717085 ## Good 0.03455750 0.637999111 ## Help -0.03202188 0.636017628 We can examine the structure matrix.48 In this example, the correlation between the factors is so low that the pattern and structure matrices are very similar. The structure coefficients include the correlation with the factor without partialling out the other factors, so they are often greater in magnitude than the pattern coefficients. When we use oblique rotation, it is good practice to present both the pattern and structure coefficients. These are so similar that we should probably re-run the model with orthogonal rotation. This pattern would be very unexpected in a study in which we hypothesize that the two constructs underlying these factors are theoretically related. Table 8.1: Pattern and structure coefficients of subtests on each factor Pattern coefficients Structure coefficients Subtest F1 F2 F1 F2 NoIntrst .750 .045 .750 .048 Odd .662 .050 .662 .052 NoFun .609 .042 .609 .044 Boring .445 -.287 .444 -.286 Alone .418 .001 .418 .003 NoRelign .380 .072 .380 .073 Better .133 .696 .135 .697 Good .032 .638 .035 .638 Help -.034 .636 -.032 .636 Note. Coefficients with absolute value &gt; .32 are boldfaced. 8.2.6 Factor analysis with correlation data Sometimes we only have access to correlation or covariance data. We can perform factor analysis on correlation matrices using nearly the same set of operations.49 8.2.6.1 Setting up the data If we had a full (symmetrical) matrix in a CSV file, we can import these into R. However, it will not be in a matrix format but as a data frame, so we need to use the as.matrix() function and provide names for the columns and rows.50 R_df &lt;- read.csv(&quot;Week08_GSS_ScienceCorrs.csv&quot;) R &lt;- as.matrix(R_df) rownames(R) &lt;- colnames(R) &lt;- names(R_df) R ## NoIntrst Odd NoFun Boring Alone NoRelign Better Good Help ## NoIntrst 1.000 0.504 0.454 0.286 0.351 0.292 0.123 0.049 0.006 ## Odd 0.504 1.000 0.388 0.303 0.216 0.311 0.138 0.029 0.027 ## NoFun 0.454 0.388 1.000 0.261 0.260 0.264 0.113 0.046 0.007 ## Boring 0.286 0.303 0.261 1.000 0.252 0.090 -0.136 -0.131 -0.231 ## Alone 0.351 0.216 0.260 0.252 1.000 0.096 0.076 0.031 -0.010 ## NoRelign 0.292 0.311 0.264 0.090 0.096 1.000 0.067 0.051 0.046 ## Better 0.123 0.138 0.113 -0.136 0.076 0.067 1.000 0.470 0.425 ## Good 0.049 0.029 0.046 -0.131 0.031 0.051 0.470 1.000 0.402 ## Help 0.006 0.027 0.007 -0.231 -0.010 0.046 0.425 0.402 1.000 Now we have a matrix, called R, which we can use in our factor analysis functions. One more step is that we need to specify the n-size, if we have access to that information. Let’s say we knew it was 372. n_p &lt;- 372 8.2.6.2 Preparatory steps If all we have is the correlation matrix and n-sizes, we won’t be able to report the descriptive statistics or the assumptions having to do with linearity. We can use most of the same procedures to determine the factorability and the number of factors to retain. Paradoxically, the eigenComputes() function returns inflated reduced eigenvalues if we use the cor = TRUE argument. If we set it to FALSE, we observe similar results as with our raw data. For now, it seems that the Hull method from the EFA.MRFA package is not available with correlation data. The results are not reported here. library(nFactors) library(ggplot2) library(&quot;psych&quot;) KMO(R) cortest.bartlett(R, n = n_p) ReducedEig &lt;- eigenComputes(R, cor = FALSE, model = &quot;factors&quot;) n_nu &lt;- ncol(R) # The number of variables in our data n_factors &lt;- length(ReducedEig) set.seed(123) paral &lt;- parallel(subject = n_p, var = n_nu, rep = 100, quantile = .95, model = &quot;factors&quot;) ParallelAna &lt;- data.frame(Nfactor= 1:n_factors, ReducedEig, RandEigM = paral$eigen$mevpea, RandEig95 = paral$eigen$qevpea) ParallelAna &lt;- round(ParallelAna, 3) ParallelAna # Scree plot scree &lt;- data.frame(Factor_n = as.factor(1:n_factors), Eigenvalue = ReducedEig) ggplot(scree, aes(x = Factor_n, y = Eigenvalue, group = 1)) + geom_point() + geom_line() + xlab(&quot;Number of factors&quot;) + ylab(&quot;Eigenvalue&quot;) + labs( title = &quot;Scree Plot&quot;, subtitle = &quot;(Based on the reduced correlation matrix)&quot;) 8.2.6.3 Performing the factor analysis We can use the same fa() function we used with the raw data set, but instead use the correlation matrix and specify the number of observations using the n.obs = argument. The results will not be identical to those in our raw data set because our correlation matrix was based on rounded values, which is typical when we use data from publications or reports. library(&quot;psych&quot;) fafit2 &lt;- fa(R, nfactors = 2, n.obs = n_p, fm = &quot;pa&quot;, rotate = &quot;promax&quot;) fafit2 8.3 References References Fox, John, Sanford Weisberg, and Brad Price. 2024. Car: Companion to Applied Regression. https://r-forge.r-project.org/projects/car/. Horn, John L. 1965. “A Rationale and Test for the Number of Factors in Factor Analysis.” Psychometrika 30 (2): 179–85. https://doi.org/10.1007/BF02289447. Jarek, Sławomir. 2024. Mvnormtest: Normality Test for Multivariate Variables. https://doi.org/10.32614/CRAN.package.mvnormtest. Kaiser, Henry F. 1974. “An Index of Factorial Simplicity.” Psychometrika 39 (1): 31–36. https://doi.org/10.1007/BF02291575. Kuhn, Max, Simon Jackson, and Jorge Cimentada. 2025. Corrr: Correlations in r. https://github.com/tidymodels/corrr. Lorenzo-Seva, Urbano, Marieke E. Timmerman, and Henk A. L. Kiers. 2011. “The Hull Method for Selecting the Number of Common Factors.” Multivariate Behavioral Research 46 (2): 340–64. https://doi.org/10.1080/00273171.2011.564527. Luo, Lan, Cara Arizmendi, and Kathleen M. Gates. 2019. “Exploratory Factor Analysis (EFA) Programs in R.” Structural Equation Modeling: A Multidisciplinary Journal 26 (5): 819–26. https://doi.org/10.1080/10705511.2019.1615835. Navarro-Gonzalez, David, and Urbano Lorenzo-Seva. 2021. EFA.MRFA: Dimensionality Assessment Using Minimum Rank Factor Analysis. https://doi.org/10.32614/CRAN.package.EFA.MRFA. Raiche, Gilles, and David Magis. 2025. nFactors: Parallel Analysis and Other Non Graphical Solutions to the Cattell Scree Test. https://doi.org/10.32614/CRAN.package.nFactors. Revelle, William. 2025. Psych: Procedures for Psychological, Psychometric, and Personality Research. https://personality-project.org/r/psych/. Tabachnick, Barbara G., and Linda S. Fidell. 2013. Using Multivariate Statistics. 6th ed. Boston: Pearson Education. We can use the corrr package (Kuhn, Jackson, and Cimentada 2025) to get prettified correlation outputs.↩︎ As with all of our analyses, missing data can be a problem, too.↩︎ There are alternative methods we can use with data that do not meet this assumption. One is to use a polychoric correlation, which we do in the second part of our factor-analysis journey. We can also use polytomous item-response modeling such as the partial credit model or the graded response model.↩︎ Here’s the code for printing an ordered set using the Base R approach, which is a little convoluted but is useful if tidyverse misbehaves: out.liars[ order(out.liars$distances, decreasing = TRUE) , ].↩︎ We can also use the psych package’s outlier() function to plot multivariate outliers. psych::outlier(dat).↩︎ The psych package and EFA.MRFA package also offer functions for parallel analysis. Another option is to use the paran package, where our code might look like this: paran(dat, centile = 95, cfa = TRUE, all = TRUE, graph = TRUE, seed = 123). In those results, we can report the number of adjusted eigenvalues above zero—that adjustment is explained in the documents cited in the package’s help files. If we have a correlation matrix instead of raw data, we should replace the first argument with two arguments to specify the name of our correlation matrix and the number of observations in our data, such as mat = R, n = n_p.↩︎ The reduced eigenvalues are not appropriate for the K1 criterion.↩︎ We could also set the extraction method to be estr = \"ML\" rather than the default, which is estr = \"ULS\". Also, this function seems to require a raw data set (not a correlation or covariance matrices) and have zero missing data, so some data cleaning may be required before using this function.↩︎ We can calculate the degrees of freedom as the number of variance and covariance elements in our raw data minus the number of parameters in the model. The number of elements is \\(\\frac{\\nu(\\nu+1)}{2}\\), where \\(\\nu\\) is the number of subtests. With our \\(9\\) subtests, we have \\(45\\) elements. This is the number of elements in the variance-covariance matrix that include the diagonal and one side of the off-diagonal—in other words, all of the variances and unique covariances. The number of parameters is \\(\\nu + \\nu{m}-\\frac{m(m-1)}{2}\\), where \\(m\\) is the number of factors. This corresponds with the number of error estimates (\\(\\nu\\)), the number of loadings on the factors (\\(\\nu{m}\\)), and the number of covariances among the factors (\\(\\frac{m(m-1)}{2}\\)). With a two-factor model with \\(9\\) subtests, we have \\(9\\) error estimates, \\(18\\) loadings, and \\(1\\) covariance that is allowed between the two factors, so the number of parameters is \\(9 + 2*9 -1 = 26\\) and the degrees of freedom is \\(45 - 26 = 19\\).↩︎ Yes, “communalities”, with a “u”, is the correct spelling.↩︎ The value \\(.32\\) is handy because we can interpret its squared value, \\(0.1024\\), as the minimum proportion of variance (i.e., 10%) in the observed variable (subtests) we wish to consider to be salient enough for consideration.↩︎ For example, according to Revelle (2025), SPSS uses Kaiser normalization whereas the psych packages fa() function does not.↩︎ We could reproduce the structure coefficient matrix with matrix multiplication of the pattern-coefficient matrix and \\(\\Phi\\) PattM %*% fafit$Phi.↩︎ If all we have is the correlation matrix, we are, of course, not able to estimate persons’ factor scores because we have no person-level data.↩︎ In this step, we named the columns and rows of the matrix using colnames() and rownames() but we can also use the dimnames = argument in the as.matrix() function.↩︎ "],["week-08-exploratory-factor-analysis-part-2.html", "9 Week 08: Exploratory Factor Analysis, Part 2 9.1 Factor analysis with correlation data 9.2 EFA with categorical data 9.3 Practice 9.4 References", " 9 Week 08: Exploratory Factor Analysis, Part 2 Conducing EFA with correlations and categorical data, and saving factor scores 9.1 Factor analysis with correlation data Sometimes we only have access to correlation or covariance data. We can perform factor analysis on correlation matrices using nearly the same set of operations.51 9.1.1 Setting up the data If we had a full (symmetrical) matrix in a CSV file, we can import these into R. However, it will not be in a matrix format but as a data frame, so we need to use the as.matrix() function and provide names for the columns and rows. R_df &lt;- read.csv(&quot;Week08_GSS_ScienceCorrs.csv&quot;) R &lt;- as.matrix(R_df) rownames(R) &lt;- colnames(R) &lt;- names(R_df) names(R_df) ## [1] &quot;NoIntrst&quot; &quot;Odd&quot; &quot;NoFun&quot; &quot;Boring&quot; &quot;Alone&quot; &quot;NoRelign&quot; &quot;Better&quot; &quot;Good&quot; &quot;Help&quot; R ## NoIntrst Odd NoFun Boring Alone NoRelign Better Good Help ## NoIntrst 1.000 0.504 0.454 0.286 0.351 0.292 0.123 0.049 0.006 ## Odd 0.504 1.000 0.388 0.303 0.216 0.311 0.138 0.029 0.027 ## NoFun 0.454 0.388 1.000 0.261 0.260 0.264 0.113 0.046 0.007 ## Boring 0.286 0.303 0.261 1.000 0.252 0.090 -0.136 -0.131 -0.231 ## Alone 0.351 0.216 0.260 0.252 1.000 0.096 0.076 0.031 -0.010 ## NoRelign 0.292 0.311 0.264 0.090 0.096 1.000 0.067 0.051 0.046 ## Better 0.123 0.138 0.113 -0.136 0.076 0.067 1.000 0.470 0.425 ## Good 0.049 0.029 0.046 -0.131 0.031 0.051 0.470 1.000 0.402 ## Help 0.006 0.027 0.007 -0.231 -0.010 0.046 0.425 0.402 1.000 Now we have a matrix, called R, which we can use in our factor analysis functions. One more step is that we need to specify the n-size, if we have access to that information. Let’s say we knew it was 372. n_p &lt;- 372 9.1.1.1 Some extra stuff: Importing half the correlation matrix Alternatively, let’s say we were given a file that included only one side of the correlation matrix. Or, we got lazy and instead of manually entering all of the correlations on both sides, we only want to manually enter them on one side, perhaps to avoid typos in our data entry. In this situation, we can import our one-sided correlation matrix and then let R do the work. First, we should be sure the other side has NAs in the cells on the blank side of the matrix. These NAs are easier to type into a CSV file than numbers. RLower &lt;- read.csv(&quot;Week08_LowerCorr_GSS_Sci.csv&quot;) RLower &lt;- as.matrix(RLower) RLower ## NoIntrst Odd NoFun Boring Alone NoRelign Better Good Help ## [1,] 1.000 NA NA NA NA NA NA NA NA ## [2,] 0.504 1.000 NA NA NA NA NA NA NA ## [3,] 0.454 0.388 1.000 NA NA NA NA NA NA ## [4,] 0.286 0.303 0.261 1.000 NA NA NA NA NA ## [5,] 0.351 0.216 0.260 0.252 1.000 NA NA NA NA ## [6,] 0.292 0.311 0.264 0.090 0.096 1.000 NA NA NA ## [7,] 0.123 0.138 0.113 -0.136 0.076 0.067 1.000 NA NA ## [8,] 0.049 0.029 0.046 -0.131 0.031 0.051 0.470 1.000 NA ## [9,] 0.006 0.027 0.007 -0.231 -0.010 0.046 0.425 0.402 1 We can use the forceSymmetric() function from the Matrix (Bates, Maechler, and Jagan 2025) package to turn our one-sided matrix into a symmetric matrix. Also, because the output from that function is a special kind of output, rather than a simple matrix, let’s change its type back to a matrix using R base’s as.matrix() function. Finally, we’ll name the row and columns, using the column names in the RLower data that we imported. library(Matrix) # help(&quot;forceSymmetric&quot;, package = &quot;Matrix&quot;) R &lt;- Matrix::forceSymmetric(RLower, uplo=&quot;L&quot;) R &lt;- as.matrix(R) rownames(R) &lt;- colnames(R) &lt;- colnames(RLower) R ## NoIntrst Odd NoFun Boring Alone NoRelign Better Good Help ## NoIntrst 1.000 0.504 0.454 0.286 0.351 0.292 0.123 0.049 0.006 ## Odd 0.504 1.000 0.388 0.303 0.216 0.311 0.138 0.029 0.027 ## NoFun 0.454 0.388 1.000 0.261 0.260 0.264 0.113 0.046 0.007 ## Boring 0.286 0.303 0.261 1.000 0.252 0.090 -0.136 -0.131 -0.231 ## Alone 0.351 0.216 0.260 0.252 1.000 0.096 0.076 0.031 -0.010 ## NoRelign 0.292 0.311 0.264 0.090 0.096 1.000 0.067 0.051 0.046 ## Better 0.123 0.138 0.113 -0.136 0.076 0.067 1.000 0.470 0.425 ## Good 0.049 0.029 0.046 -0.131 0.031 0.051 0.470 1.000 0.402 ## Help 0.006 0.027 0.007 -0.231 -0.010 0.046 0.425 0.402 1.000 Okay, back to the task at hand: 9.1.2 Preparatory steps If all we have is the correlation matrix and n-sizes, we won’t be able to report the descriptive statistics or the assumptions having to do with linearity. We can use most of the same procedures to determine the factorability and the number of factors to retain. 9.1.2.1 Factorability We can start with the determinant, to see if the EFA will work. This should be positive. det(R) ## [1] 0.1915743 We can examine the KMO and Bartlett tests for factorability. The Bartlett test with the correlation also requires we specify the n-size. KMO(R) ## Kaiser-Meyer-Olkin factor adequacy ## Call: KMO(r = R) ## Overall MSA = 0.75 ## MSA for each item = ## NoIntrst Odd NoFun Boring Alone NoRelign Better Good Help ## 0.76 0.76 0.82 0.77 0.79 0.81 0.68 0.69 0.71 psych::cortest.bartlett(R, n = n_p) # We had set n_p in the previous code, this is the n-size ## $chisq ## [1] 606.7354 ## ## $p.value ## [1] 8.250062e-105 ## ## $df ## [1] 36 These are the same results we saw with the raw data. 9.1.2.2 Number of factors As we did with the raw data, we can use the eigenComputes() function from the nFactors package to conduct the parallel test. Paradoxically, we set the cor = argument to FALSE. If we set it to TRUE (which is the default), it assumes we are analyzing raw data and that we want it to first turn it into a correlation matrix. We already have a correlation matrix, so we set this to FALSE. library(nFactors) ReducedEig &lt;- eigenComputes(R, cor = FALSE, model = &quot;factors&quot;) n_nu &lt;- ncol(R) # The number of variables in our data n_factors &lt;- length(ReducedEig) set.seed(123) paral &lt;- parallel(subject = n_p, var = n_nu, rep = 1000, quantile = .95, model = &quot;factors&quot;) ParallelAna &lt;- data.frame(Nfactor= 1:n_factors, ReducedEig, RandEigM = paral$eigen$mevpea, RandEig95 = paral$eigen$qevpea) ParallelAna &lt;- round(ParallelAna, 3) ParallelAna ## Nfactor ReducedEig RandEigM RandEig95 ## 1 1 1.816 0.271 0.351 ## 2 2 1.228 0.185 0.245 ## 3 3 0.127 0.119 0.170 ## 4 4 -0.047 0.063 0.105 ## 5 5 -0.090 0.011 0.048 ## 6 6 -0.113 -0.037 -0.003 ## 7 7 -0.148 -0.086 -0.051 ## 8 8 -0.179 -0.136 -0.098 ## 9 9 -0.237 -0.196 -0.154 We observe similar results as with our raw data. The third observed (reduced) eigenvalue is lower than its counterpart at the 95th percentile of the randomly generated data sets. The Hull method seems to require a data matrix of the raw data, so this method for examining number of factors is not available with correlation data. We can get the scree plot using same output from the eigenComputes() function we used above (as we did with the raw data in the previous lesson): scree &lt;- data.frame(Factor_n = as.factor(1:n_factors), Eigenvalue = ReducedEig) ggplot(scree, aes(x = Factor_n, y = Eigenvalue, group = 1)) + geom_point() + geom_line() + xlab(&quot;Number of factors&quot;) + ylab(&quot;Eigenvalue&quot;) + labs( title = &quot;Scree Plot&quot;, subtitle = &quot;(Based on the reduced correlation matrix)&quot;) 9.1.3 Performing the factor analysis We can use the same psych::fa() function we used with the raw data set, but instead use the correlation matrix. We need to specify the number of observations using the n.obs = argument. fafit2 &lt;- psych::fa(R, n.obs = n_p, nfactors = 2, fm = &quot;pa&quot;, rotate = &quot;promax&quot;) The results will not be identical to those in our raw data set because our correlation matrix was based on rounded values, which is typical when we use data from publications or reports. print(fafit2, digits = 2, cut = sqrt(.10) ) ## Factor Analysis using method = pa ## Call: psych::fa(r = R, nfactors = 2, n.obs = n_p, rotate = &quot;promax&quot;, ## fm = &quot;pa&quot;) ## Standardized loadings (pattern matrix) based upon correlation matrix ## PA1 PA2 h2 u2 com ## NoIntrst 0.75 0.56 0.44 1.0 ## Odd 0.66 0.44 0.56 1.0 ## NoFun 0.61 0.37 0.63 1.0 ## Boring 0.45 0.28 0.72 1.7 ## Alone 0.42 0.17 0.83 1.0 ## NoRelign 0.38 0.15 0.85 1.1 ## Better 0.70 0.50 0.50 1.1 ## Good 0.64 0.41 0.59 1.0 ## Help 0.64 0.41 0.59 1.0 ## ## PA1 PA2 ## SS loadings 1.91 1.39 ## Proportion Var 0.21 0.15 ## Cumulative Var 0.21 0.37 ## Proportion Explained 0.58 0.42 ## Cumulative Proportion 0.58 1.00 ## ## With factor correlations of ## PA1 PA2 ## PA1 1 0 ## PA2 0 1 ## ## Mean item complexity = 1.1 ## Test of the hypothesis that 2 factors are sufficient. ## ## df null model = 36 with the objective function = 1.65 with Chi Square = 606.74 ## df of the model are 19 and the objective function was 0.06 ## ## The root mean square of the residuals (RMSR) is 0.03 ## The df corrected root mean square of the residuals is 0.04 ## ## The harmonic n.obs is 372 with the empirical chi square 21.89 with prob &lt; 0.29 ## The total n.obs was 372 with Likelihood Chi Square = 22.66 with prob &lt; 0.25 ## ## Tucker Lewis Index of factoring reliability = 0.988 ## RMSEA index = 0.023 and the 90 % confidence intervals are 0 0.053 ## BIC = -89.8 ## Fit based upon off diagonal values = 0.99 ## Measures of factor score adequacy ## PA1 PA2 ## Correlation of (regression) scores with factors 0.88 0.84 ## Multiple R square of scores with factors 0.77 0.71 ## Minimum correlation of possible factor scores 0.54 0.42 9.2 EFA with categorical data Let’s use last week’s data, which was about the attitudes towards scientists. raw &lt;- read.csv(&quot;Week07_GSS_Science.csv&quot;) dat &lt;- raw[ , -1] # Removing the ID column, so we have just the variables 9.2.1 Describing categorical data Because our data are ordered categorical—that is, they’re on an ordinal type of scale—with only four points, we should present the item-level descriptive statistics as the frequency of responses to each of the scale points. A really simple function for this is table(). It provides the frequency of responses for each unique value in the column. We would use this for each variable, sequentially. table(dat[, &quot;NoIntrst&quot;]) ## ## 1 2 3 4 ## 13 117 219 22 We can calculate the proportions using the prop.table() or proportions() function around this: prop.table( table(dat[, &quot;NoIntrst&quot;]) ) ## ## 1 2 3 4 ## 0.03504043 0.31536388 0.59029650 0.05929919 9.2.1.1 Plotting categorical data If we wanted to create an output that included all of the items, we can write our own function or we can use the ExpCustomStat() function from the SmartEDA package (Dayanand Ubrangala et al. 2024): library(SmartEDA) freqout &lt;- SmartEDA::ExpCustomStat(dat, Cvar = colnames(dat), stat = c(&quot;Count&quot;,&quot;prop&quot;), gpby = FALSE) names(freqout) &lt;- c(&quot;Category&quot;, &quot;Item&quot;, &quot;Frequency&quot;, &quot;Percent&quot;) head(freqout, 10) ## Category Item Frequency Percent ## &lt;int&gt; &lt;char&gt; &lt;int&gt; &lt;num&gt; ## 1: 3 NoIntrst 219 59.03 ## 2: 2 NoIntrst 117 31.54 ## 3: 1 NoIntrst 13 3.50 ## 4: 4 NoIntrst 22 5.93 ## 5: 3 Odd 217 58.49 ## 6: 2 Odd 110 29.65 ## 7: 1 Odd 19 5.12 ## 8: 4 Odd 25 6.74 ## 9: 3 NoFun 243 65.50 ## 10: 2 NoFun 79 21.29 The order of the categories is wonkey. I can use the arrange() function to reorder but we have to keep the order within each item. freqout_alpha &lt;- freqout %&gt;% arrange(Item, Category) head(freqout_alpha, 10) ## Category Item Frequency Percent ## &lt;int&gt; &lt;char&gt; &lt;int&gt; &lt;num&gt; ## 1: 1 Alone 16 4.31 ## 2: 2 Alone 65 17.52 ## 3: 3 Alone 247 66.58 ## 4: 4 Alone 43 11.59 ## 5: 1 Better 67 18.06 ## 6: 2 Better 276 74.39 ## 7: 3 Better 26 7.01 ## 8: 4 Better 2 0.54 ## 9: 1 Boring 5 1.35 ## 10: 2 Boring 58 15.63 That rearranged our items in alphabetical order, which might be fine for our purposes. If we want to keep the original order, we might have to create a temporary variable to sort by, doing something like this: freqout &lt;- freqout %&gt;% mutate(temp = row_number()) %&gt;% # Creating a temporary sorting variable group_by(Item) %&gt;% mutate(temp = min(temp)) %&gt;% # Each item has the same sorting variable value arrange(temp, Category) %&gt;% # Do the sorting. dplyr::select(-temp) # Removing the sorting variable head(freqout, 10) ## # A tibble: 10 × 4 ## # Groups: Item [3] ## Category Item Frequency Percent ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 NoIntrst 13 3.5 ## 2 2 NoIntrst 117 31.5 ## 3 3 NoIntrst 219 59.0 ## 4 4 NoIntrst 22 5.93 ## 5 1 Odd 19 5.12 ## 6 2 Odd 110 29.6 ## 7 3 Odd 217 58.5 ## 8 4 Odd 25 6.74 ## 9 1 NoFun 11 2.96 ## 10 2 NoFun 79 21.3 We can use that outputted object with ggplot to create a column plot of the category responses for each variable: library(ggplot2) library(gridExtra) freqout %&gt;% # This mutate function is optional, just to get the items to be in the order we want: mutate(Item = factor(Item, levels = unique(freqout$Item) )) %&gt;% ggplot( aes(x = Category, y = Frequency)) + geom_col() + facet_wrap(. ~ Item, nrow = 3 ) # ggsave(&quot;freqout_GSS_cat.png&quot;) This plot illustrates the low frequency of responses to some of the items’ categories. Generally, we want to see a spread of responses across the categories. If we want something simpler, we can examine the plot for each item using the prop.table(table()) code around the item. For the first item, it would be this: barplot(prop.table(table(dat$NoIntrst))) If we needed a table to report the descriptive, frequency, statistics, we might use this approach to format it: freqout.table &lt;- freqout %&gt;% pivot_wider(names_from = Category, values_from = c(Frequency, Percent) ) Then we can change the names and rearrange the columns. names(freqout.table) &lt;- c(&quot;Item&quot;, &quot;n.1&quot;, &quot;n.2&quot;, &quot;n.3&quot;, &quot;n.4&quot;, &quot;Perc.1&quot;, &quot;Perc.2&quot;, &quot;Perc.3&quot;, &quot;Perc.4&quot;) # Here, we&#39;re just reordering the columns so it is a little easier to interpret: freqout.table &lt;- freqout.table[, c(&quot;Item&quot;, &quot;n.1&quot;, &quot;Perc.1&quot;, &quot;n.2&quot;, &quot;Perc.2&quot;, &quot;n.3&quot;, &quot;Perc.3&quot;, &quot;n.4&quot;, &quot;Perc.4&quot;)] %&gt;% data.frame() freqout.table ## Item n.1 Perc.1 n.2 Perc.2 n.3 Perc.3 n.4 Perc.4 ## 1 NoIntrst 13 3.50 117 31.54 219 59.03 22 5.93 ## 2 Odd 19 5.12 110 29.65 217 58.49 25 6.74 ## 3 NoFun 11 2.96 79 21.29 243 65.50 38 10.24 ## 4 Boring 5 1.35 58 15.63 254 68.46 54 14.56 ## 5 Alone 16 4.31 65 17.52 247 66.58 43 11.59 ## 6 NoRelign 21 5.66 123 33.15 205 55.26 22 5.93 ## 7 Better 67 18.06 276 74.39 26 7.01 2 0.54 ## 8 Good 79 21.29 271 73.05 18 4.85 3 0.81 ## 9 Help 94 25.34 274 73.85 2 0.54 1 0.27 # write.csv(freqout.table, &quot;freqout.table.csv&quot;, row.names = FALSE) 9.2.1.2 Specifying the variable type as ordinal Another way we could obtain the frequency counts is to specify that our items are ordered factors (i.e., ordered categorical variables), then use the summary() function on them.52 53 datF &lt;- data.frame(lapply(dat, ordered, levels = c(1:4)) ) summary(datF) ## NoIntrst Odd NoFun Boring Alone NoRelign Better Good Help ## 1: 13 1: 19 1: 11 1: 5 1: 16 1: 21 1: 67 1: 79 1: 94 ## 2:117 2:110 2: 79 2: 58 2: 65 2:123 2:276 2:271 2:274 ## 3:219 3:217 3:243 3:254 3:247 3:205 3: 26 3: 18 3: 2 ## 4: 22 4: 25 4: 38 4: 54 4: 43 4: 22 4: 2 4: 3 4: 1 There are other packages and functions. The DescTools package (Signorell 2025) is one example (results are not printed here): library(DescTools) DescTools::Desc(datF$NoIntrst, plotit = TRUE) 9.2.1.3 Getting the correlation matrix For the factor analysis, because our data are ordered categorical and not continuous, we should use the a polychoric matrix instead of the raw values. We can directly specify this in the fa() function using the cor = \"poly\" argument.54 Because we don’t know how many factors to retain yet, we do not need to specify the nfactors = argument yet. We can find the polchoric matrix as part of the output. library(psych) temp &lt;- psych::fa(dat, fm = &quot;pa&quot;, rotate = &quot;promax&quot;, cor = &quot;poly&quot;) n_p &lt;- nrow(raw) R_poly &lt;- temp$r round( R_poly, 3) Notice that the correlations differ from those we observed with the Pearson correlations that we used in the garden-variety factor analysis. 9.2.1.4 Appraising the factorability and determining the number of factors to retain We can use this polychoric correlation matrix in our examination of the factorability and in assessing how many factors to retain, just as we would do with a Pearson correlation matrix. Here is the determinant: det(R_poly) ## [1] 0.06958345 This is positive and not too close to zero. psych::KMO(R_poly) ## Kaiser-Meyer-Olkin factor adequacy ## Call: psych::KMO(r = R_poly) ## Overall MSA = 0.76 ## MSA for each item = ## NoIntrst Odd NoFun Boring Alone NoRelign Better Good Help ## 0.77 0.76 0.85 0.77 0.78 0.82 0.70 0.70 0.74 psych::cortest.bartlett(R_poly, n = n_p) ## $chisq ## [1] 975.9178 ## ## $p.value ## [1] 1.773981e-181 ## ## $df ## [1] 36 KMO test result is .76, which is greater than .60 threshold. The Bartlett test was statistically significant, suggesting that the data do not come from a population that an identity matrix (1s on the diagonal and 0s on the off-diagonal) would describe. With these three results, we observe that the data are factorable. Let’s see how many factors we should retain: library(nFactors) ReducedEig &lt;- eigenComputes(R_poly, cor = FALSE, model = &quot;factors&quot;) n_nu &lt;- ncol(R_poly) # The number of variables in our data n_factors &lt;- length(ReducedEig) set.seed(345) paral &lt;- parallel(subject = n_p, var = n_nu, rep = 1000, quantile = .95, model = &quot;factors&quot;) ParallelAna &lt;- data.frame(Nfactor= 1:n_factors, ReducedEig, RandEigM = paral$eigen$mevpea, RandEig95 = paral$eigen$qevpea) ParallelAna &lt;- round(ParallelAna, 3) ParallelAna ## Nfactor ReducedEig RandEigM RandEig95 ## 1 1 2.245 0.272 0.360 ## 2 2 1.697 0.187 0.245 ## 3 3 0.202 0.120 0.171 ## 4 4 -0.020 0.063 0.103 ## 5 5 -0.059 0.012 0.052 ## 6 6 -0.105 -0.037 -0.005 ## 7 7 -0.138 -0.087 -0.053 ## 8 8 -0.146 -0.138 -0.099 ## 9 9 -0.255 -0.196 -0.155 Here is our scree plot: scree &lt;- data.frame(Factor_n = as.factor(1:n_factors), Eigenvalue = ReducedEig) ggplot(scree, aes(x = Factor_n, y = Eigenvalue, group = 1)) + geom_point() + geom_line() + xlab(&quot;Number of factors&quot;) + ylab(&quot;Eigenvalue&quot;) + labs( title = &quot;Scree Plot&quot;, subtitle = &quot;(Based on the reduced correlation matrix)&quot;) We can also use the psych package’s fa.parallel() function on the polychoric correlation matrix. Similar to if we use the psych::fa() function, with the psych::fa.parallel() function, if we use a correlation matrix as input, we need to specify the number of observations, using the n.obs = argument. We can also set our factor estimation method to be principal axis factoring using fm = \"pa\". We can make sure we get the factors and not the principal components with fa = \"fa\". Another option is to set the SMC = TRUE. This sets the squared multiple correlations (i.e., the square of the factor loadings) to replace the diagonal of the matrix, making it a reduced correlation matrix, which is what we did just previously with the nFactors package. William Revelle, however in the psych help files, makes the point that the random data’s eigenvalues would not be estimated based on the reduced correlation matrix, which implies we should leave SMC = FALSE, which is the default in this function. Finally, I noticed that the default quantile for the parallel analysis is indeed at .95, which was a topic of discussion in the previous class; here, I added quant = .95 to make this explicit even though it is the default. If you want the median of the randomly generated data, use quant = .50. Finally, ignore the horizontal line at 1.00, as this is for PCA, not EFA. I emailed Revelle about this, so it might be fixed.55 # You can try the following, as an alternative, but it probably will throw an error. # fa.parallel(dat, fm = &quot;pa&quot;, fa = &quot;fa&quot;,cor = &quot;poly&quot;, quant = .95) # We can, instead, use the polychoric correlation matrix as the input: para.psyc &lt;- fa.parallel(R_poly, n.obs = 372, fm = &quot;pa&quot;, fa = &quot;fa&quot;, SMC = FALSE, # TRUE returns the eigenvalues of the reduced correlation. quant = .95) ## Parallel analysis suggests that the number of factors = 3 and the number of components = NA print(para.psyc) ## Call: fa.parallel(x = R_poly, n.obs = 372, fm = &quot;pa&quot;, fa = &quot;fa&quot;, SMC = FALSE, ## quant = 0.95) ## Parallel analysis suggests that the number of factors = 3 and the number of components = NA ## ## Eigen Values of ## ## eigen values of factors ## [1] 2.31 1.29 0.17 0.03 -0.01 -0.10 -0.23 -0.55 -0.61 ## ## eigen values of simulated factors ## [1] 0.37 0.18 0.12 0.07 0.02 -0.03 -0.07 -0.11 -0.18 ## ## eigen values of components ## [1] 2.85 2.26 0.95 0.67 0.58 0.54 0.44 0.39 0.32 ## ## eigen values of simulated components ## [1] NA At this point, we have a judgment call to make. The scree plot seems to support retaining two factors but the parallel test indicates we should retain three. At this point, we could argue for parsimony and decide to use only two factors because of the shape of the scree plot and because the eigenvalues of the simulated data and observed data at the third eigenvalue are so close. Let’s look at the cumulative proportion of variance explained by three factors. library(&quot;psych&quot;) fafit_norot &lt;- fa(dat, nfactors = 3, fm = &quot;pa&quot;, rotate = &quot;none&quot;, cor = &quot;poly&quot;) print(fafit_norot, cut = .32, sort = TRUE, digits = 4) ## Factor Analysis using method = pa ## Call: fa(r = dat, nfactors = 3, rotate = &quot;none&quot;, fm = &quot;pa&quot;, cor = &quot;poly&quot;) ## Standardized loadings (pattern matrix) based upon correlation matrix ## item PA1 PA2 PA3 h2 u2 com ## NoIntrst 1 0.8128 0.6611 0.3389 1.001 ## Odd 2 0.7354 0.5718 0.4282 1.114 ## NoFun 3 0.6673 0.4467 0.5533 1.006 ## Boring 4 0.5089 -0.3750 0.4454 0.5546 2.231 ## Alone 5 0.4683 0.3213 0.3260 0.6740 1.809 ## NoRelign 6 0.4399 -0.3218 0.2997 0.7003 1.864 ## Better 7 0.7785 0.6496 0.3504 1.145 ## Help 9 0.7455 0.5601 0.4399 1.016 ## Good 8 0.7273 0.5478 0.4522 1.072 ## ## PA1 PA2 PA3 ## SS loadings 2.3585 1.8382 0.3115 ## Proportion Var 0.2621 0.2042 0.0346 ## Cumulative Var 0.2621 0.4663 0.5009 ## Proportion Explained 0.5232 0.4078 0.0691 ## Cumulative Proportion 0.5232 0.9309 1.0000 ## ## Mean item complexity = 1.4 ## Test of the hypothesis that 3 factors are sufficient. ## ## df null model = 36 with the objective function = 2.6652 with Chi Square = 975.9178 ## df of the model are 12 and the objective function was 0.0726 ## ## The root mean square of the residuals (RMSR) is 0.0204 ## The df corrected root mean square of the residuals is 0.0354 ## ## The harmonic n.obs is 371 with the empirical chi square 11.1321 with prob &lt; 0.5176 ## The total n.obs was 371 with Likelihood Chi Square = 26.4302 with prob &lt; 0.009325 ## ## Tucker Lewis Index of factoring reliability = 0.95368 ## RMSEA index = 0.05687 and the 90 % confidence intervals are 0.0271 0.08662 ## BIC = -44.5642 ## Fit based upon off diagonal values = 0.9955 ## Measures of factor score adequacy ## PA1 PA2 PA3 ## Correlation of (regression) scores with factors 0.9156 0.9030 0.5858 ## Multiple R square of scores with factors 0.8383 0.8154 0.3432 ## Minimum correlation of possible factor scores 0.6767 0.6307 -0.3137 We see that 50.09% of the variance is explained by three factors. 9.2.1.5 Factor analysis on the polychoric matrix We can perform the factor analysis on our polychoric correlation matrix, R_poly. Alternatively, we can use the raw data and specify cor = \"poly\". If our data were dichotomous, this would be a tetrachoric matrix, cor = \"tet\". library(&quot;psych&quot;) fafit &lt;- fa(dat, nfactors = 3, fm = &quot;pa&quot;, rotate = &quot;promax&quot;, cor = &quot;poly&quot;) print(fafit, cut = .32, sort = TRUE, digits = 2) ## Factor Analysis using method = pa ## Call: fa(r = dat, nfactors = 3, rotate = &quot;promax&quot;, fm = &quot;pa&quot;, cor = &quot;poly&quot;) ## Standardized loadings (pattern matrix) based upon correlation matrix ## item PA2 PA1 PA3 h2 u2 com ## Better 7 0.81 0.65 0.35 1.0 ## Good 8 0.76 0.55 0.45 1.0 ## Help 9 0.70 0.56 0.44 1.2 ## Odd 2 0.66 0.57 0.43 1.1 ## NoRelign 6 0.65 0.30 0.70 1.2 ## NoIntrst 1 0.49 0.41 0.66 0.34 2.0 ## NoFun 3 0.45 0.45 0.55 1.7 ## Alone 5 0.63 0.33 0.67 1.1 ## Boring 4 0.56 0.45 0.55 1.4 ## ## PA2 PA1 PA3 ## SS loadings 1.8 1.49 1.22 ## Proportion Var 0.2 0.17 0.14 ## Cumulative Var 0.2 0.37 0.50 ## Proportion Explained 0.4 0.33 0.27 ## Cumulative Proportion 0.4 0.73 1.00 ## ## With factor correlations of ## PA2 PA1 PA3 ## PA2 1.00 0.11 -0.13 ## PA1 0.11 1.00 0.63 ## PA3 -0.13 0.63 1.00 ## ## Mean item complexity = 1.3 ## Test of the hypothesis that 3 factors are sufficient. ## ## df null model = 36 with the objective function = 2.67 with Chi Square = 975.92 ## df of the model are 12 and the objective function was 0.07 ## ## The root mean square of the residuals (RMSR) is 0.02 ## The df corrected root mean square of the residuals is 0.04 ## ## The harmonic n.obs is 371 with the empirical chi square 11.13 with prob &lt; 0.52 ## The total n.obs was 371 with Likelihood Chi Square = 26.43 with prob &lt; 0.0093 ## ## Tucker Lewis Index of factoring reliability = 0.954 ## RMSEA index = 0.057 and the 90 % confidence intervals are 0.027 0.087 ## BIC = -44.56 ## Fit based upon off diagonal values = 1 ## Measures of factor score adequacy ## PA2 PA1 PA3 ## Correlation of (regression) scores with factors 0.90 0.88 0.85 ## Multiple R square of scores with factors 0.82 0.78 0.73 ## Minimum correlation of possible factor scores 0.63 0.55 0.46 We can print the factor loadings. Setting simple = FALSE specifies that we want to see if there are complex loadings. fa.diagram(fafit, cut = .32, digits = 2, simple = FALSE) We see that the NoIntrst subtest now has some clear complexity. It loads on the second and third factor almost evenly. We should ask if this aligns with the underlying theory about how this item is expected to relate to the latent variables. We can report the inter-factor correlations and the pattern and structure matrices (or factor matrix if we had specified an orthogonal rotation). round(fafit$Phi,3) ## PA2 PA1 PA3 ## PA2 1.000 0.110 -0.131 ## PA1 0.110 1.000 0.626 ## PA3 -0.131 0.626 1.000 We see weak correlations between Factor 1 and Factors 2 and 3, r_{F1,F2} = .11 and r_{F1,F3} = -.13. Factors 2 and 3 have a moderately positive correlation, r_{F1,F2} = .63. PattM &lt;- fafit$loadings[1:9,] StructM &lt;- fafit$Structure[1:9,] Here’s the pattern matrix, which is what we use for interpretation. round(PattM, 2) ## PA2 PA1 PA3 ## NoIntrst 0.08 0.49 0.41 ## Odd 0.01 0.66 0.14 ## NoFun 0.04 0.45 0.28 ## Boring -0.25 0.05 0.56 ## Alone 0.09 -0.09 0.63 ## NoRelign -0.03 0.65 -0.19 ## Better 0.81 0.03 0.11 ## Good 0.76 -0.08 0.09 ## Help 0.70 0.07 -0.18 Here’s the structure matrix. We can see very slight differences. They’re close because the factors are uncorrelated. round(StructM, 2) ## PA2 PA1 PA3 ## NoIntrst 0.09 0.75 0.70 ## Odd 0.07 0.75 0.55 ## NoFun 0.05 0.63 0.56 ## Boring -0.32 0.37 0.62 ## Alone 0.00 0.32 0.56 ## NoRelign 0.06 0.53 0.22 ## Better 0.80 0.18 0.02 ## Good 0.74 0.06 -0.06 ## Help 0.73 0.03 -0.23 9.2.1.6 The polycor package, if needed Sometimes a different approach to estimating the polychoric correlation matrix is required if those of the psych package do not provide a polychoric correlation. The polycor package (Fox 2022) offers more functionality, but can require more processing time—sometimes several minutes. 56 We first change our variable types to be ordered factors instead of numeric. We did this above, but here is the code again. This will ensure that we estimate the polychoric instead of the Pearson correlations in the hetcor() function. We use the ordered() function inside the lapply() function. datF &lt;- data.frame(lapply(dat, ordered, levels = c(1:4)) ) Now we can use the package and the function to acquire the polychoric correlation matrix, which we can subsequently use with our psych::fa() function. library(polycor) #This can take several minutes! hetout &lt;- polycor::hetcor(datF, ML = TRUE) R_poly &lt;- as.matrix(hetout$correlations) n_nu &lt;- hetout$n fafit &lt;- fa(R_poly, n.obs = n_p, nfactors = 2, fm = &quot;pa&quot;, rotate = &quot;promax&quot;) 9.2.2 Factor scores When we have a raw data set, for example with each row representing a person, we can estimate those persons’ factor scores.57 A large advantage to using factor scores over raw composite scores is that the factor scores are calculated after removing the unique item variances.58 Factor_scores &lt;- psych::factor.scores(dat, fafit)$scores head(Factor_scores) ## PA2 PA1 PA3 ## [1,] -2.2738807 1.0862526 0.9536342 ## [2,] -1.5026730 -0.5697761 1.2872092 ## [3,] 0.5386996 -1.0731917 -0.9764344 ## [4,] 0.3387576 0.3248921 -1.4942695 ## [5,] -1.1997520 0.3274958 -1.1398391 ## [6,] -1.6211552 -0.4228087 0.3491049 Here, we see the factor scores of Person 1 on Line 1, Person 2 on Line 2, and so forth. We can attach the person IDs to this vector so we can use it in our future analyses.59 df.factor.scores &lt;- data.frame(PID = raw$PID, Factor_scores) head(df.factor.scores) %&gt;% arrange(desc(PA2)) ## PID PA2 PA1 PA3 ## 1 S0003 0.5386996 -1.0731917 -0.9764344 ## 2 S0004 0.3387576 0.3248921 -1.4942695 ## 3 S0005 -1.1997520 0.3274958 -1.1398391 ## 4 S0002 -1.5026730 -0.5697761 1.2872092 ## 5 S0006 -1.6211552 -0.4228087 0.3491049 ## 6 S0001 -2.2738807 1.0862526 0.9536342 We can see that Person ID S0003 had the highest score on Factor 1. We can save these scores to columns in our original data frame so we can use them later. raw[, c(&quot;F1&quot;, &quot;F2&quot;, &quot;F3&quot;)] &lt;- Factor_scores If we look at the descriptive statistics of these factor scores, we see that the they are centered around \\(0\\) and the variance of each factor is \\(1\\). This is the result of fixing the factor means and variances to these values in exploratory-factor-analysis models, which in turn allows the computation of the common and unique variances of the subtests. desc &lt;- psych::describe(raw[, c(&quot;F1&quot;, &quot;F2&quot;, &quot;F3&quot;)]) print(desc, digits = 3) ## vars n mean sd median trimmed mad min max range skew kurtosis se ## F1 1 371 0 1 0.360 0.074 0.306 -2.407 5.148 7.554 -0.313 2.039 0.052 ## F2 2 371 0 1 0.178 0.032 0.723 -3.621 2.822 6.443 -0.381 1.138 0.052 ## F3 3 371 0 1 0.124 0.004 0.854 -3.690 3.053 6.743 -0.138 0.934 0.052 9.2.2.1 Which factor scores to use? If we read the help file for the factor.scores() function, we see there are several methods available, with the method = argument. Tucker (1971) identified a difference between Bartlett scores and regression (Thurstone) scores (the latter being the default from the psych package) from factor analysis. The conclusion was that when we plan to use factor scores as predictors in a regression, we should use the regression scores; when we plan to use the scores as a dependent variable, we should use the Bartlett scores. (This does not help us when we have the variable as a mediator, where it is both a dependent and an independent variable—we’re in the dark.) For information on how the scores are calculated, we can read DiStefano and colleagues (2009). For more on which factor scores to use, I think Skrondal and Laake (2001) provided more on this. 9.3 Practice Try out the EFA procedures from last week and this week with a new raw data set, Week08_GoalOrientation_Raw.csv. This might require a little more subjective interpretation in identifying the number of factors to retain. However, the results are more interesting, particularly with the \\(\\Phi\\) matrix. If we use four factors with principal axis factoring and a garden-variety factor analysis (with Pearson correlations), we can obtain the following pattern and structure coefficients: raw2 &lt;- read.csv(&quot;Week08_GoalOrientation_Raw.csv&quot;) dat2 &lt;- raw2[ , -1] psych::describe(dat2) ## vars n mean sd median trimmed mad min max range skew kurtosis se ## PAP1 1 1022 5.15 1.55 5 5.32 1.48 1 7 6 -0.75 0.12 0.05 ## PAP2 2 1022 5.20 1.53 5 5.38 1.48 1 7 6 -0.85 0.24 0.05 ## PAP3 3 1022 4.91 1.55 5 5.03 1.48 1 7 6 -0.51 -0.32 0.05 ## PAV4 4 1022 4.86 1.94 5 5.05 2.97 1 7 6 -0.60 -0.82 0.06 ## PAV5 5 1022 4.71 1.68 5 4.82 1.48 1 7 6 -0.43 -0.62 0.05 ## PAV6 6 1022 4.19 1.72 4 4.19 1.48 1 7 6 -0.05 -0.87 0.05 ## MAV7 7 1022 5.10 1.42 5 5.22 1.48 1 7 6 -0.59 -0.06 0.04 ## MAV8 8 1022 4.14 1.50 4 4.14 1.48 1 7 6 -0.06 -0.54 0.05 ## MAV9 9 1022 3.89 1.60 4 3.88 1.48 1 7 6 0.06 -0.64 0.05 ## MAP10 10 1022 5.76 1.18 6 5.90 1.48 1 7 6 -1.03 1.22 0.04 ## MAP11 11 1022 6.21 0.98 7 6.36 0.00 1 7 6 -1.22 1.34 0.03 ## MAP12 12 1022 5.89 1.20 6 6.06 1.48 1 7 6 -1.13 1.21 0.04 9.3.1 Summary This week and last week, we reviewed the preliminary steps to factor analysis including examining the data and the assumptions for factor analysis, how to test for factorability, and how to determine the number of factors to retain. We also used the fa() function from the psych package and manipulated the output to extract specific information such as the pattern coefficients. We also practiced fitting a factor-analysis model to a correlation matrix. We determined that with our data, which do not seem to fit the assumption of linearity because the subtests only have four points, we should use the polychoric correlation matrix for more appropriately fitting the factor-analysis model. We conducted the factor analysis and then estimated persons’ factor scores. 9.4 References References Bates, Douglas, Martin Maechler, and Mikael Jagan. 2025. Matrix: Sparse and Dense Matrix Classes and Methods. https://Matrix.R-forge.R-project.org. Dayanand Ubrangala, Kiran R, Ravi Prasad Kondapalli, and Sayan Putatunda. 2024. SmartEDA: Summarize and Explore the Data. https://daya6489.github.io/SmartEDA/. DiStefano, Christine, Min Zhu, and Diana Mîndrilã. 2009. “Understanding and Using Factor Scores: Considerations for the Applied Researcher” 14 (20). https://doi.org/10.7275/DA8T-4G52. Fox, John. 2022. Polycor: Polychoric and Polyserial Correlations. https://r-forge.r-project.org/projects/polycor/. Signorell, Andri. 2025. DescTools: Tools for Descriptive Statistics. https://andrisignorell.github.io/DescTools/. Skrondal, Anders, and Petter Laake. 2001. “Regression Among Factor Scores.” Psychometrika 66 (4): 563–75. https://doi.org/10.1007/BF02296196. Tucker, Ledyard R. 1971. “Relations of Factor Score Estimates to Their Use.” Psychometrika 36 (4): 427–36. https://doi.org/10.1007/BF02291367. If all we have is the correlation matrix, we are, obviously, not able to estimate persons’ factor scores because we have no person-level data.↩︎ This lapply() function may be new to you. It is similar to apply() in that it applies another function across the columns of data. Here, we’re applying the ordered() function across each column of data. The ordered() function is a special case of the factor() function. We use this to create ordered factors.↩︎ Note that the word factor is not the same as a factor in factor analysis; it is a factor-type of variable, as in in analysis of variance framework.↩︎ As is explained in the help files for the fa() function, this argument is cor = \"tet\" for tetrachoric correlations if our data are dichotomous and cor = \"mixed\" if our data include a mix of variables, such as if some are dichotomous, polytomous, and continuous.↩︎ In the function, where it says if (fa != \"pc\" &amp;&amp; plot) abline(h = 1) it should say if (fa != \"fa\" &amp;&amp; plot) abline(h = 1).↩︎ We can look through the help file for this function, help(\"hetcor\" ,package = \"polycor\"), to see more options.↩︎ We can even do this if we used a polychoric correlation matrix in our factor analysis model, as long as the raw data were included in the fa() function.↩︎ There are a few types of factor scores available. The default is the regression-based, Thurstone, scores. Bartlett scores are another option. DiStefano and colleagues (2009) provide more information. ↩︎ We must be careful when we assign person IDs from our raw data frame to this scored data frame. We need to be sure that any observations that may have been removed during the factor-analysis process are excluded from the names that we assign to the PID column name in the scored Factor_scores data frame. This might happen if there are missing data and a person gets excluded from the factor analysis estimation. In this example, we do not observe this problem because all persons’ scores on all factors were estimated. Also, at the time of this writing, any row with missing data will also have a corresponding row with NAs.↩︎ "],["week-09-confirmatory-factor-analysis.html", "10 Week 09: Confirmatory Factor Analysis 10.1 Preparatory steps 10.2 Fitting a CFA model 10.3 Using alternative model specifications 10.4 Estimating factor scores 10.5 CFA with categorical data 10.6 Way TL;DR Summary 10.7 References", " 10 Week 09: Confirmatory Factor Analysis Fitting CFA to continuous data and categorical data The lavaan package (Rosseel, Jorgensen, and De Wilde 2024) is well developed and frequently used for estimating confirmatory factor analysis (CFA) models. Accompanying lavaan, the semTools package (Jorgensen et al. 2025) provides additional resources such as for comparing models and for estimating internal consistency reliability coefficients. In this handout, we will do the following: carry out the preparatory steps to CFA, conduct a CFA and examine the fit indices, parameter estimates, and other information contained in the CFA fitted object, use alternative model specifications and conduct a model comparison, estimate and save persons’ factor scores, conduct a CFA with categorical data. 10.1 Preparatory steps First let’s read in our data and take a look at the descriptive statistics and the correlation matrix. raw &lt;-read.csv(&quot;Week08_GoalOrientation_Raw.csv&quot;) str(raw) ## &#39;data.frame&#39;: 1022 obs. of 13 variables: ## $ PID : chr &quot;S0001&quot; &quot;S0002&quot; &quot;S0003&quot; &quot;S0004&quot; ... ## $ PAP1 : int 6 7 6 6 7 7 7 7 7 5 ... ## $ PAP2 : int 7 7 5 6 7 7 7 7 7 6 ... ## $ PAP3 : int 6 7 5 6 7 7 7 5 7 5 ... ## $ PAV4 : int 6 7 5 6 7 7 7 7 7 6 ... ## $ PAV5 : int 5 2 5 6 4 4 7 6 4 7 ... ## $ PAV6 : int 1 7 6 6 7 7 7 7 7 3 ... ## $ MAV7 : int 7 6 5 4 3 6 7 5 4 6 ... ## $ MAV8 : int 6 2 6 1 6 7 7 3 4 7 ... ## $ MAV9 : int 6 2 4 1 5 7 7 1 5 7 ... ## $ MAP10: int 7 7 7 6 7 7 7 7 7 6 ... ## $ MAP11: int 7 7 7 6 7 7 7 7 7 7 ... ## $ MAP12: int 7 7 6 5 7 7 7 7 7 7 ... dat &lt;- raw[, -1] Notice that this is the same data set we used in class during Week 8. With these data, there are four hypothesized constructs, which are described in Bandalos (2018, 335–37). The first three letters of each item’s name reminds us which construct it is intended to represent. 10.1.1 Describing the data Here are the descriptive statistics, treating the data as continuous. The low-fidelity version of this function is simply psych::describe(dat). The fancy code is optional, but nice for making it pretty. I learned about the flextable package (Gohel and Skintzos 2025) from Havi. .cl-e2e081b0{}.cl-e2d78c5e{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-e2dae57a{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-e2db021c{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-e2db0226{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-e2db0227{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}nmeansdmedianminmaxskewkurtosisse1,0225.151.55517-0.750.120.051,0225.201.53517-0.850.240.051,0224.911.55517-0.51-0.320.051,0224.861.94517-0.60-0.820.061,0224.711.68517-0.43-0.620.051,0224.191.72417-0.05-0.870.051,0225.101.42517-0.59-0.060.041,0224.141.50417-0.06-0.540.051,0223.891.604170.06-0.640.051,0225.761.18617-1.031.220.041,0226.210.98717-1.221.340.031,0225.891.20617-1.131.210.04 As usual, we can use the cor() and cov() functions to obtain the correlation and covariance matrices. R &lt;- cor(dat) Here is some optional formatting code. If this causes problems in R, just use cor() and cov(). R &lt;- cor(dat) # Removing the upper diagonal from the cov matrix: R[upper.tri(R, diag = FALSE)] &lt;- NA # Making the output pretty: R %&gt;% as.data.frame() %&gt;% rownames_to_column(&quot;var&quot;) %&gt;% mutate(across(where(is.numeric), round, 3 )) %&gt;% # rounding numeric columns flextable() %&gt;% autofit() %&gt;% fit_to_width(7.5) .cl-e32e5642{}.cl-e32498aa{font-family:'Arial';font-size:9pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-e3280904{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-e328090e{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-e3282934{width:0.595in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-e3282935{width:0.54in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-e328293e{width:0.505in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-e328293f{width:0.526in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-e3282940{width:0.595in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-e3282948{width:0.595in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-e3282949{width:0.54in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-e328294a{width:0.505in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-e3282952{width:0.526in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-e3282953{width:0.595in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-e3282954{width:0.595in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-e3282955{width:0.54in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-e328295c{width:0.505in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-e328295d{width:0.526in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-e328295e{width:0.595in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-e328295f{width:0.595in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-e3282966{width:0.54in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-e3282967{width:0.505in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-e3282968{width:0.526in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-e3282969{width:0.595in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}varPAP1PAP2PAP3PAV4PAV5PAV6MAV7MAV8MAV9MAP10MAP11MAP12PAP11.000PAP20.6901.000PAP30.7020.7121.000PAV40.1660.2450.1881.000PAV50.1630.2480.1790.3801.000PAV60.2930.4210.4160.4820.3771.000MAV7-0.0400.0680.0210.2270.3280.2381.000MAV80.1430.2120.2170.2180.2800.3250.3861.000MAV90.0780.1450.1630.1190.2540.3010.3350.6511.000MAP100.2800.2540.2230.0390.0660.0590.0180.1520.1291.000MAP110.1820.2020.1860.012-0.0080.0510.0520.2020.1770.5581.00MAP120.1120.1540.1280.0290.0210.0840.1600.2210.2050.4830.581 # Because we just removed the upper matrix from R, for making the printout # pretty, we can get back to the full R matrix if we need to using the # force `Matrix::symmetric()` function: library(Matrix) # help(&quot;forceSymmetric&quot;, package = &quot;Matrix&quot;) R &lt;- Matrix::forceSymmetric(R, uplo=&quot;L&quot;) R &lt;- as.matrix(R) We notice that there is variation in the magnitude of these correlations. We see that the items hypothesized to load on the same constructs tend to more strongly correlate with each other than with the other items in the data set. Our descriptive statistics suggest the scale goes from 1 to 7. We should examine how many functioning scale points are used in each item. We also see that most of the items have negative skew, with most of the means and medians being higher than the midpoint of 4. MAP11 seems to have the strongest skew and smallest standard deviation. Here is the covariance matrix, using the same kind of pretty code. If this causes problems in R, just use cov(dat): S &lt;- cov(dat) # Removing the upper diagonal from the cov matrix: S[upper.tri(S, diag = FALSE)] &lt;- NA # Making the output pretty: S %&gt;% as.data.frame() %&gt;% rownames_to_column(&quot;var&quot;) %&gt;% mutate(across(where(is.numeric), round, 3 )) %&gt;% # rounding numeric columns flextable() %&gt;% autofit() %&gt;% fit_to_width(7.5) .cl-e37b1a5e{}.cl-e3719fe2{font-family:'Arial';font-size:9pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-e3750e70{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-e3750e71{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-e3752eb4{width:0.595in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-e3752ebe{width:0.54in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-e3752ebf{width:0.505in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-e3752ec0{width:0.526in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-e3752ec8{width:0.595in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-e3752ec9{width:0.595in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-e3752eca{width:0.54in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-e3752ed2{width:0.505in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-e3752ed3{width:0.526in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-e3752ed4{width:0.595in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-e3752ed5{width:0.595in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-e3752ed6{width:0.54in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-e3752edc{width:0.505in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-e3752edd{width:0.526in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-e3752ede{width:0.595in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-e3752ee6{width:0.595in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-e3752ee7{width:0.54in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-e3752ee8{width:0.505in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-e3752ef0{width:0.526in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-e3752ef1{width:0.595in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}varPAP1PAP2PAP3PAV4PAV5PAV6MAV7MAV8MAV9MAP10MAP11MAP12PAP12.393PAP21.6312.337PAP31.6891.6932.417PAV40.4980.7260.5683.758PAV50.4240.6370.4661.2372.822PAV60.7781.1041.1101.6041.0872.945MAV7-0.0870.1480.0460.6270.7850.5822.028MAV80.3320.4870.5070.6360.7080.8380.8272.262MAV90.1940.3560.4080.3700.6850.8300.7641.5722.576MAP100.5100.4580.4090.0890.1310.1200.0310.2700.2441.389MAP110.2750.3030.2830.022-0.0120.0860.0720.2970.2790.6440.957MAP120.2070.2820.2370.0670.0420.1720.2720.3970.3930.6810.6791.432 We’ll look at this observed covariance matrix when we compare the reproduced covariance matrix to it. 10.1.1.1 An article reported R and the SDs, so how can I get S? If you only have access to the correlation matrix and standard deviations, you can get the covariance matrix: R_lower &lt;- read.csv(&quot;Week09_BandalosTable13.3.csv&quot;) R_lower &lt;- as.matrix(R_lower) SDs &lt;- c(1.55, 1.53, 1.56, 1.94, 1.68, 1.72, 1.42, 1.50, 1.61, 1.18, 0.98, 1.20) D &lt;- diag(SDs) S.from.R &lt;- D %*% R %*% D round(S.from.R, 2) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] ## [1,] 2.40 1.64 1.70 0.50 0.43 0.78 -0.09 0.33 0.20 0.51 0.28 0.21 ## [2,] 1.64 2.34 1.70 0.73 0.64 1.11 0.15 0.49 0.36 0.46 0.30 0.28 ## [3,] 1.70 1.70 2.43 0.57 0.47 1.12 0.05 0.51 0.41 0.41 0.28 0.24 ## [4,] 0.50 0.73 0.57 3.76 1.24 1.61 0.63 0.63 0.37 0.09 0.02 0.07 ## [5,] 0.43 0.64 0.47 1.24 2.82 1.09 0.78 0.71 0.69 0.13 -0.01 0.04 ## [6,] 0.78 1.11 1.12 1.61 1.09 2.96 0.58 0.84 0.83 0.12 0.09 0.17 ## [7,] -0.09 0.15 0.05 0.63 0.78 0.58 2.02 0.82 0.76 0.03 0.07 0.27 ## [8,] 0.33 0.49 0.51 0.63 0.71 0.84 0.82 2.25 1.57 0.27 0.30 0.40 ## [9,] 0.20 0.36 0.41 0.37 0.69 0.83 0.76 1.57 2.59 0.24 0.28 0.40 ## [10,] 0.51 0.46 0.41 0.09 0.13 0.12 0.03 0.27 0.24 1.39 0.65 0.68 ## [11,] 0.28 0.30 0.28 0.02 -0.01 0.09 0.07 0.30 0.28 0.65 0.96 0.68 ## [12,] 0.21 0.28 0.24 0.07 0.04 0.17 0.27 0.40 0.40 0.68 0.68 1.44 10.1.1.2 Describing from a categorical perspective We did this in the last session, which has some easier code to get the descriptive statistics item-by-item. Here’s the fancy code for getting the plot of all the items’ responses: library(SmartEDA) freqout &lt;- SmartEDA::ExpCustomStat(dat, Cvar = colnames(dat), stat = c(&quot;Count&quot;,&quot;prop&quot;), gpby = FALSE) names(freqout) &lt;- c(&quot;Category&quot;, &quot;Item&quot;, &quot;Frequency&quot;, &quot;Percent&quot;) freqout_alpha &lt;- freqout %&gt;% arrange(Item, Category) # head(freqout_alpha, 10) library(ggplot2) library(gridExtra) freqout %&gt;% # This mutate function is optional, just to get the items to be in the order we want: mutate(Item = factor(Item, levels = unique(freqout$Item) )) %&gt;% ggplot( aes(x = Category, y = Frequency)) + geom_col() + facet_wrap(. ~ Item, nrow = 3 ) # ggsave(&quot;freqout_GSS_cat.png&quot;) It seems like 75% of our variables have ample responses in all of the categories. We might be concerned with MAP11 and maybe with MAP10 and MAP12 because there are not many responses in the lower two categories. If there were only four categories being used, we should treat these data as categorical. 10.1.1.2.1 The easiest way to examine counts If all we’re looking for are the frequency of responses to each category in each item, a simple approach is to specify our items as being ordered factors instead of numeric and using the summary() function on our data. datF &lt;- data.frame(lapply(dat, ordered)) summary(datF) ## PAP1 PAP2 PAP3 PAV4 PAV5 PAV6 MAV7 MAV8 MAV9 MAP10 MAP11 MAP12 ## 1: 35 1: 29 1: 31 1: 84 1: 47 1: 64 1: 13 1: 42 1: 80 1: 6 1: 1 1: 5 ## 2: 32 2: 47 2: 46 2: 82 2: 76 2:133 2: 46 2:118 2:132 2: 10 2: 1 2: 10 ## 3: 71 3: 56 3:109 3: 93 3:117 3:159 3: 79 3:174 3:197 3: 26 3: 13 3: 26 ## 4:162 4:145 4:182 4:114 4:180 4:216 4:154 4:261 4:260 4: 96 4: 45 4: 88 ## 5:262 5:252 5:265 5:185 5:237 5:206 5:314 5:248 5:185 5:230 5:166 5:199 ## 6:222 6:267 6:203 6:189 6:193 6:125 6:225 6:111 6:100 6:333 6:278 6:289 ## 7:238 7:226 7:186 7:275 7:172 7:119 7:191 7: 68 7: 68 7:321 7:518 7:405 10.1.1.3 Addressing assumptions With linearity, we could examine the bivariate plots all pairs of variables using the plot(dat) function but this will be uninterpretable because of the small size of each of the plots. Because we have 12 variables, there will be \\(12(12-1) / 2 = 66\\) pairwise plots. If we wanted to examine a specific pair of items, we can use the geom_smooth() function in GGplot2. First, we create a line that is allowed to curve so that it best describes the nonlinear relationship between the two variables. For this, we use the method = \"loess\" argument. We can think about that line as lots of little, local, regressions on the data at each small increment in the data. After that, we overlay a line that is forced to be linear. If the linear line reasonably matches the loess line, we can argue for linearity. Here’s an example with two items that seem to have a small amount of non-linearity, though probably not enough to hurt this assumption: ggplot(dat, aes(x = MAV9, y = MAP12)) + geom_point() + geom_smooth(method = &quot;loess&quot;, se = FALSE) + geom_smooth(method = &quot;lm&quot;, color = &quot;brown&quot;) We can detect multivariate outliers using our usual procedures: varbs &lt;- cbind(dat) # Note that in our data, we&#39;re using all of the columns distances &lt;- mahalanobis(varbs, center = colMeans(varbs), cov = cov(varbs)) raw$distances &lt;- distances # Let&#39;s keep a record of those who exceed the p &lt; .001 criterion, if any: raw$p &lt;- pchisq(distances, df = (ncol(varbs)-1), # df is number of variables - 1 lower.tail = FALSE) We can see which cases in our data set are flagged as outliers. I think Base R is better here than tidyverse. # Let&#39;s use Base R to save a column that indicates whether the case is an outlier. raw$outlier &lt;- ifelse(raw$p &lt; .001, 1, 0) # Let&#39;s print those rows for which their outlier status is true: out.liars &lt;- raw[raw$outlier == 1, c(&quot;PID&quot;, &quot;distances&quot;, &quot;p&quot;, &quot;outlier&quot;)] out.liars ## PID distances p outlier ## 125 S0125 43.56740 8.648903e-06 1 ## 172 S0172 35.93353 1.735968e-04 1 ## 194 S0194 43.25519 9.804388e-06 1 ## 199 S0199 32.92628 5.408306e-04 1 ## 296 S0296 34.65967 2.819395e-04 1 ## 371 S0371 41.84562 1.722452e-05 1 ## 393 S0393 32.09814 7.355548e-04 1 ## 452 S0452 60.02265 9.182751e-09 1 ## 486 S0486 35.56532 1.998225e-04 1 ## 497 S0497 31.51915 9.106198e-04 1 ## 694 S0694 35.55273 2.007845e-04 1 ## 705 S0705 37.99227 7.847168e-05 1 ## 712 S0712 44.19936 6.705832e-06 1 ## 743 S0743 35.43172 2.102671e-04 1 ## 760 S0760 34.56138 2.926267e-04 1 ## 823 S0823 32.29947 6.827257e-04 1 ## 840 S0840 53.85349 1.255264e-07 1 ## 850 S0850 32.34198 6.720523e-04 1 ## 988 S0988 59.38391 1.206490e-08 1 Dang! There are a lot of multivariate outliers. We can count how many there and see there are 19: nrow(out.liars) ## [1] 19 For convenience, we can sort those rows by their distances to see who are the most extreme outliers.60 out.liars %&gt;% arrange(desc(distances)) ## PID distances p outlier ## 1 S0452 60.02265 9.182751e-09 1 ## 2 S0988 59.38391 1.206490e-08 1 ## 3 S0840 53.85349 1.255264e-07 1 ## 4 S0712 44.19936 6.705832e-06 1 ## 5 S0125 43.56740 8.648903e-06 1 ## 6 S0194 43.25519 9.804388e-06 1 ## 7 S0371 41.84562 1.722452e-05 1 ## 8 S0705 37.99227 7.847168e-05 1 ## 9 S0172 35.93353 1.735968e-04 1 ## 10 S0486 35.56532 1.998225e-04 1 ## 11 S0694 35.55273 2.007845e-04 1 ## 12 S0743 35.43172 2.102671e-04 1 ## 13 S0296 34.65967 2.819395e-04 1 ## 14 S0760 34.56138 2.926267e-04 1 ## 15 S0199 32.92628 5.408306e-04 1 ## 16 S0850 32.34198 6.720523e-04 1 ## 17 S0823 32.29947 6.827257e-04 1 ## 18 S0393 32.09814 7.355548e-04 1 ## 19 S0497 31.51915 9.106198e-04 1 It seems like Person S0452 is the most extreme outlier, S0988 is the second most extreme outlier, and so forth. Let’s examine the multivariate Q-Q plot based on the Mahalanobis distances:61 car::qqPlot(distances, distribution = &quot;chisq&quot;, df = mean(distances), lwd = 1, grid = FALSE, main = &quot;Multi-normal Q-Q Plot&quot;, xlab = expression(chi^2 * &quot; quantiles&quot;), ylab = expression(&quot;Mahalanobis distances &quot;^2)) ## [1] 452 988 If we are expected to also report a statistical test of normality, we can use the mulitvariate Shapiro-Wilk test that we used in Week 3. The mshapiro.test() function from the mvnormtest package (Jarek 2024). This function requires the data be arranged as rows instead of columns, so we’ll use the transpose function, t() within the test. library(mvnormtest) mshapiro.test( t(dat) ) ## ## Shapiro-Wilk normality test ## ## data: Z ## W = 0.97116, p-value = 2.257e-13 We see results that are consistent with our outliers and Mahalanobis distance Q-Q plot. The Shapiro-Wilk normality test (W = 0.97, p = &lt; .01) suggests the data are statistically significantly different from a normal distribution. Another multivariate normality test we might encounter in research is the Mardia test. We can use the mardiaKurtosis() function from the semTools package (Jorgensen et al. 2025). 62 library(semTools) mardia_out &lt;- mardiaKurtosis(dat, use = &quot;everything&quot;) round(mardia_out,3) ## b2d z p ## 195.531 24.007 0.000 Even though the absolute values of the skew and kurtosis of each of the variables in our earlier descriptive statistics were all below 2.0, there appears to be a non-normal distribution. With the Mardia kurtosis z-statistic \\((z = 24.01, p &lt; .05)\\) being statistically significant, we see more evidence that the assumption of multivariate normality was not met. This statistical significance is not surprising because we have a large sample size. Among all of the sources of evidence, we can probably trust the plots over these statistical tests of the multivariate normality; nonetheless, we will encounter these in publications and possibly from reviewers requesting them. This collection of evidence suggests that the multivariate normality assumption was not met. This result should inform our decisions about which estimation method to use and whether to use adjustments. Instead of using maximum likelihood estimation, we can use a robust estimation approach, specifically the “maximum likelihood robust” (MLR) method.63 10.2 Fitting a CFA model Before we fit the CFA model using the lavaan package, we need to provide the CFA model specification. We are specifying four factors, which we’re naming PAP, PAV, MAV, and MAP. We indicate which variables load on which factor with the =~ operator in lavaan’s language. The default is to set each factor’s metric to be on the scale of its first variable. library(lavaan) mymod &lt;- &quot;PAP =~ PAP1 + PAP2 + PAP3 PAV =~ PAV4 + PAV5 + PAV6 MAV =~ MAV7 + MAV8 + MAV9 MAP =~ MAP10 + MAP11 + MAP12&quot; Now that we have the model specification saved to our mymod object, we can fit the model to our data using the cfa() function. The first argument is the model specification. In the data = argument, we include the raw data set. We use the estimator = \"MLR\" to specify the robust maximum likelihood estimation method. We can use the mimic = \"Mplus\" argument if we wish to acquire output that is similar to what it would be if we were to use Mplus software. cfa_fit &lt;- cfa(mymod, data = dat, estimator = &quot;MLR&quot;, mimic = &quot;Mplus&quot;) At this point, we should look at whether there were any errors when we attempt to fit the model. For instance, if we get a warning that says The variance-covariance matrix of the estimated parameters (vcov) does not appear to be positive definite! there is something wrong with our model specification. More than likely, our model is underidentified and we need to go back to our specification. If we did not have access to the raw data but only had the covariance matrix, S, we can include the sample.cov = argument in place of the data = argument. We also need to specify the number of persons, which here we’ll assign to the object n_p, which is 1022.64 S &lt;- cov(dat) n_p &lt;- nrow(dat) cfa_fit_S &lt;- cfa(mymod, sample.cov = S, sample.nobs = n_p, mimic = &quot;Mplus&quot;) We will use the earlier specification, with the raw data and the robust estimation. We can use the summary() function on our object that contains the CFA result. We can also ask for the fit indices, standardized coefficients, and the r-square of the loadings using these respective three arguments. summary(cfa_fit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) ## lavaan 0.6-19 ended normally after 46 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 42 ## ## Number of observations 1022 ## Number of missing patterns 1 ## ## Model Test User Model: ## Standard Scaled ## Test Statistic 283.977 260.087 ## Degrees of freedom 48 48 ## P-value (Chi-square) 0.000 0.000 ## Scaling correction factor 1.092 ## Yuan-Bentler correction (Mplus variant) ## ## Model Test Baseline Model: ## ## Test statistic 4421.866 3657.087 ## Degrees of freedom 66 66 ## P-value 0.000 0.000 ## Scaling correction factor 1.209 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.946 0.941 ## Tucker-Lewis Index (TLI) 0.926 0.919 ## ## Robust Comparative Fit Index (CFI) 0.947 ## Robust Tucker-Lewis Index (TLI) 0.927 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -20006.917 -20006.917 ## Scaling correction factor 1.227 ## for the MLR correction ## Loglikelihood unrestricted model (H1) -19864.929 -19864.929 ## Scaling correction factor 1.155 ## for the MLR correction ## ## Akaike (AIC) 40097.835 40097.835 ## Bayesian (BIC) 40304.875 40304.875 ## Sample-size adjusted Bayesian (SABIC) 40171.478 40171.478 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.069 0.066 ## 90 Percent confidence interval - lower 0.062 0.058 ## 90 Percent confidence interval - upper 0.077 0.073 ## P-value H_0: RMSEA &lt;= 0.050 0.000 0.000 ## P-value H_0: RMSEA &gt;= 0.080 0.013 0.001 ## ## Robust RMSEA 0.069 ## 90 Percent confidence interval - lower 0.061 ## 90 Percent confidence interval - upper 0.077 ## P-value H_0: Robust RMSEA &lt;= 0.050 0.000 ## P-value H_0: Robust RMSEA &gt;= 0.080 0.012 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.046 0.046 ## ## Parameter Estimates: ## ## Standard errors Sandwich ## Information bread Observed ## Observed information based on Hessian ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## PAP =~ ## PAP1 1.000 1.255 0.812 ## PAP2 1.035 0.037 28.243 0.000 1.298 0.850 ## PAP3 1.052 0.038 27.683 0.000 1.320 0.850 ## PAV =~ ## PAV4 1.000 1.144 0.591 ## PAV5 0.772 0.062 12.356 0.000 0.884 0.526 ## PAV6 1.195 0.082 14.615 0.000 1.368 0.797 ## MAV =~ ## MAV7 1.000 0.657 0.462 ## MAV8 1.956 0.169 11.600 0.000 1.285 0.855 ## MAV9 1.845 0.160 11.547 0.000 1.212 0.756 ## MAP =~ ## MAP10 1.000 0.814 0.691 ## MAP11 0.974 0.056 17.430 0.000 0.793 0.811 ## MAP12 1.041 0.066 15.885 0.000 0.847 0.708 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## PAP ~~ ## PAV 0.751 0.074 10.112 0.000 0.523 0.523 ## MAV 0.197 0.038 5.123 0.000 0.238 0.238 ## MAP 0.308 0.046 6.719 0.000 0.301 0.301 ## PAV ~~ ## MAV 0.378 0.056 6.693 0.000 0.502 0.502 ## MAP 0.080 0.038 2.133 0.033 0.086 0.086 ## MAV ~~ ## MAP 0.159 0.024 6.558 0.000 0.297 0.297 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .PAP1 5.155 0.048 106.570 0.000 5.155 3.334 ## .PAP2 5.201 0.048 108.813 0.000 5.201 3.404 ## .PAP3 4.915 0.049 101.118 0.000 4.915 3.163 ## .PAV4 4.860 0.061 80.186 0.000 4.860 2.508 ## .PAV5 4.713 0.053 89.744 0.000 4.713 2.807 ## .PAV6 4.192 0.054 78.131 0.000 4.192 2.444 ## .MAV7 5.103 0.045 114.616 0.000 5.103 3.585 ## .MAV8 4.135 0.047 87.939 0.000 4.135 2.751 ## .MAV9 3.890 0.050 77.534 0.000 3.890 2.425 ## .MAP10 5.756 0.037 156.210 0.000 5.756 4.886 ## .MAP11 6.209 0.031 203.007 0.000 6.209 6.350 ## .MAP12 5.889 0.037 157.389 0.000 5.889 4.923 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .PAP1 0.816 0.063 12.990 0.000 0.816 0.341 ## .PAP2 0.649 0.068 9.558 0.000 0.649 0.278 ## .PAP3 0.671 0.079 8.483 0.000 0.671 0.278 ## .PAV4 2.445 0.153 15.983 0.000 2.445 0.651 ## .PAV5 2.038 0.116 17.564 0.000 2.038 0.723 ## .PAV6 1.071 0.139 7.692 0.000 1.071 0.364 ## .MAV7 1.594 0.081 19.625 0.000 1.594 0.787 ## .MAV8 0.609 0.085 7.123 0.000 0.609 0.269 ## .MAV9 1.104 0.100 11.048 0.000 1.104 0.429 ## .MAP10 0.725 0.062 11.609 0.000 0.725 0.522 ## .MAP11 0.327 0.033 9.775 0.000 0.327 0.342 ## .MAP12 0.713 0.057 12.606 0.000 0.713 0.498 ## PAP 1.575 0.114 13.872 0.000 1.000 1.000 ## PAV 1.310 0.148 8.832 0.000 1.000 1.000 ## MAV 0.432 0.070 6.200 0.000 1.000 1.000 ## MAP 0.663 0.067 9.850 0.000 1.000 1.000 ## ## R-Square: ## Estimate ## PAP1 0.659 ## PAP2 0.722 ## PAP3 0.722 ## PAV4 0.349 ## PAV5 0.277 ## PAV6 0.636 ## MAV7 0.213 ## MAV8 0.731 ## MAV9 0.571 ## MAP10 0.478 ## MAP11 0.658 ## MAP12 0.502 10.2.1 Making sure the model was correctly estimated The first place our eyes should go in the output is the first line. With luck, it will report that lavaan ended normally after some number of iterations. Our output suggests the model ended normally, so we are in good shape. We should then make sure the model included all of our data by seeing if the Number of observations is what we expect it to be, which it is in this case (1022). Additionally, we should be sure that the number of parameters is what we expect. We see that there are 42 parameters, which is what we expect if we include the means of the items. This is because there are 8 pattern coefficients, 12 item error variances, 4 factor variances, and 6 factor covariances, as well as 12 item means, the latter of which is excluded from our calculation of the degrees of freedom. The degrees of freedom is the number unique cells in the sample covariance matrix minus the number of parameters. The number of unique cells is \\(\\frac{\\nu(\\nu + 1)}{2} = 78\\), where \\(\\nu\\) is the number of variables (i.e., the 12 items). The number of parameters excluding the item means is \\(30\\), making our degrees of freedom \\(48\\). 10.2.2 Evaluating the model fit We can proceed to look at the model fit and parameter estimates. Looking at the fit statistics, we see there is some degree of mispecification in this model. Because we used robust estimation, we read the indices under the Robust column in the output. The chi-square fit index of \\(260.087\\), with its \\(48\\) degrees of freedom, suggests the model does not fit. The global fit indexes are borderline okay. Neither the comparative fit index, \\(0.947\\), nor the root mean square error of approximation, \\(0.069\\), exceed the traditional criteria for “good” fit (CFI \\(\\ge .95\\) and RMSEA \\(\\le .06\\)) but they are close. The standardized root mean square residual of \\(0.046\\) does meet the criterion (SRMR \\(\\le .08\\)). If we seek to save these fit indexes to an object, we can use the fitMeasures() function. These have .scaled and .robust suffixes because we used robust maximum likelihood as the estimation method. If we simply used estimator = \"ML\", which is the default, we would remove those suffixes. fit_stats &lt;- fitMeasures(cfa_fit, c(&quot;chisq.scaled&quot;,&quot;df.scaled&quot;, &quot;pvalue.scaled&quot;, &quot;cfi.robust&quot;, &quot;tli.robust&quot;, &quot;rmsea.robust&quot;, &quot;srmr&quot;)) fit_stats ## chisq.scaled df.scaled pvalue.scaled cfi.robust tli.robust rmsea.robust srmr ## 260.087 48.000 0.000 0.947 0.927 0.069 0.046 10.2.3 Interpretting the parameter estimates Looking back at the main output, we see that the unstandardized parameter estimates are in the Estimate column. The first pattern coefficient for each factor is \\(1.00\\). This is because the default specification is to fix the first variable to be the reference variable, thereby setting the metric of the factor to be the same as that variable. The other coefficients on that factor are interpreted as their expected change in their units based on a one-unit change in the factor, where that unit is the same as that of the reference variable. In the covariances section, we see the covariances among the factors. The standardized column shows the correlations among the factors. We see that these are all statistically significant and that the correlation between PAV and MAP is the weakest. We might want to evaluate the degree to which these results are consistent with our theory about the strength and direction of these latent-variable relationships. In the variances section of the output, we see that the variances of the factors are in the units of their first loading manifest variables. For example, the variance of the first factor is \\(1.575\\). In this section of the output, we also see the variances of the manifest variables (i.e., the observed variables that the factors predict). These are the error (or residual) variances. In the intercepts section, we see the estimates of the means of each manifest variable and factor. We can see that the factors are centered around zero. In evaluating the fit of the CFA model, we are not usually interested in this section. For all of the parameter estimates, there are versions of their estimates in a completely standardized model, in the Std.all column. The observed variables and the factors are scaled as z-scores. In interpreting the first item’s standardized pattern coefficient, \\(\\lambda_{1,1}\\), we can say that the expected amount of change is \\(0.812\\) standard-deviation units for each \\(1\\) standard-deviation unit change in the factor score. With this particular model, the \\(R^2\\) estimates of the observed variables are the same as the squared standardized loadings. For instance, \\(\\lambda_{1,1}^2 = R_{\\text{PAP}1}^2 = 0.659\\). And, as mentioned in the covariances section, any standardized covariances are interpreted as correlations. The Std.lv column includes the estimates when the factors (the latent variables) are standardized but the observed variables are not. These pattern coefficients are interpreted as changes in the observed variable, on its original scale, for each \\(1\\) standard-deviation unit change in the factor score. 10.2.4 Further inspecting our model 10.2.4.1 Looking at the model’s covariance matrices There are several covariances. In addition to the sample covariance matrix from the raw data \\((\\text{S})\\), there is the model-implied covariance matrix \\((\\Sigma({\\hat{\\theta}}))\\), the residual covariance matrix \\((\\Theta_{\\delta})\\), and the standardized residual covariance matrix. We can get the model-implied covariance matrix from the fitted output using the fitted() function. We can isolate the covariance by attaching $cov to this object. Sigma_theta &lt;- fitted(cfa_fit)$cov Sigma_theta ## PAP1 PAP2 PAP3 PAV4 PAV5 PAV6 MAV7 MAV8 MAV9 MAP10 MAP11 MAP12 ## PAP1 2.391 ## PAP2 1.629 2.335 ## PAP3 1.657 1.715 2.414 ## PAV4 0.751 0.777 0.790 3.754 ## PAV5 0.580 0.600 0.610 1.011 2.819 ## PAV6 0.897 0.928 0.944 1.565 1.209 2.942 ## MAV7 0.197 0.203 0.207 0.378 0.292 0.451 2.026 ## MAV8 0.384 0.398 0.404 0.739 0.570 0.883 0.844 2.260 ## MAV9 0.363 0.375 0.382 0.697 0.538 0.833 0.796 1.558 2.573 ## MAP10 0.308 0.319 0.324 0.080 0.062 0.096 0.159 0.311 0.293 1.388 ## MAP11 0.300 0.310 0.316 0.078 0.060 0.093 0.155 0.303 0.286 0.646 0.956 ## MAP12 0.320 0.332 0.337 0.083 0.064 0.100 0.165 0.323 0.305 0.690 0.672 1.431 This seems to be similar to the sample covariance matrix, \\(\\text{S}\\). S_sample &lt;- inspect(cfa_fit, &#39;sampstat&#39;)$cov S_sample ## PAP1 PAP2 PAP3 PAV4 PAV5 PAV6 MAV7 MAV8 MAV9 MAP10 MAP11 MAP12 ## PAP1 2.391 ## PAP2 1.629 2.335 ## PAP3 1.687 1.691 2.414 ## PAV4 0.497 0.726 0.567 3.754 ## PAV5 0.424 0.637 0.466 1.236 2.819 ## PAV6 0.778 1.103 1.109 1.602 1.086 2.942 ## MAV7 -0.087 0.148 0.046 0.626 0.784 0.581 2.026 ## MAV8 0.331 0.487 0.507 0.635 0.707 0.837 0.827 2.260 ## MAV9 0.194 0.356 0.408 0.369 0.685 0.829 0.764 1.571 2.573 ## MAP10 0.509 0.458 0.409 0.089 0.131 0.120 0.031 0.270 0.243 1.388 ## MAP11 0.275 0.302 0.283 0.022 -0.012 0.086 0.072 0.297 0.278 0.643 0.956 ## MAP12 0.207 0.281 0.237 0.067 0.042 0.172 0.272 0.397 0.393 0.680 0.679 1.431 The discrepancy between \\(\\Sigma({\\hat{\\theta}})\\) and \\(\\text{S}\\) is the residual matrix. This discrepancy is the basis for the fit statistics. We can calculate it as S_sample - Sigma_theta given the objects we just created. Alternatively, we can get \\(\\Theta_{\\delta}\\) using the resid() function: theta_delta &lt;- resid(cfa_fit, type = &quot;raw&quot;) theta_delta ## $type ## [1] &quot;raw&quot; ## ## $cov ## PAP1 PAP2 PAP3 PAV4 PAV5 PAV6 MAV7 MAV8 MAV9 MAP10 MAP11 MAP12 ## PAP1 0.000 ## PAP2 0.000 0.000 ## PAP3 0.030 -0.023 0.000 ## PAV4 -0.253 -0.051 -0.222 0.000 ## PAV5 -0.156 0.037 -0.144 0.224 0.000 ## PAV6 -0.119 0.175 0.165 0.037 -0.122 0.000 ## MAV7 -0.284 -0.056 -0.161 0.248 0.492 0.130 0.000 ## MAV8 -0.053 0.089 0.102 -0.103 0.137 -0.046 -0.017 0.000 ## MAV9 -0.169 -0.020 0.026 -0.328 0.147 -0.004 -0.033 0.013 0.000 ## MAP10 0.201 0.139 0.085 0.009 0.069 0.024 -0.128 -0.041 -0.050 0.000 ## MAP11 -0.025 -0.008 -0.033 -0.056 -0.073 -0.007 -0.082 -0.006 -0.007 -0.003 0.000 ## MAP12 -0.114 -0.050 -0.100 -0.017 -0.023 0.072 0.106 0.073 0.088 -0.009 0.007 0.000 ## ## $mean ## PAP1 PAP2 PAP3 PAV4 PAV5 PAV6 MAV7 MAV8 MAV9 MAP10 MAP11 MAP12 ## 0 0 0 0 0 0 0 0 0 0 0 0 Finally, if we need access to the standardized residuals, we can use the resid() function with the type = \"standardized\": std_resids &lt;- resid(cfa_fit, type = &quot;standardized&quot;) std_resids ## $type ## [1] &quot;standardized&quot; ## ## $cov ## PAP1 PAP2 PAP3 PAV4 PAV5 PAV6 MAV7 MAV8 MAV9 MAP10 MAP11 MAP12 ## PAP1 0.000 ## PAP2 0.001 0.000 ## PAP3 1.010 -0.926 0.000 ## PAV4 -3.637 -0.767 -3.379 0.000 ## PAV5 -2.247 0.600 -2.190 2.907 0.000 ## PAV6 -2.613 3.469 3.507 1.055 -3.494 0.000 ## MAV7 -4.237 -0.861 -2.424 2.911 6.958 1.867 0.000 ## MAV8 -1.247 2.349 2.468 -1.831 2.123 -1.247 -0.669 0.000 ## MAV9 -2.957 -0.368 0.467 -5.143 2.089 -0.075 -1.021 0.593 0.000 ## MAP10 4.720 3.313 1.945 0.152 1.237 0.539 -2.591 -1.167 -1.125 0.000 ## MAP11 -0.844 -0.287 -1.181 -1.275 -1.691 -0.242 -2.078 -0.269 -0.235 -0.354 0.000 ## MAP12 -2.560 -1.202 -2.456 -0.264 -0.391 1.654 2.145 2.186 2.250 -0.857 1.089 0.000 ## ## $mean ## PAP1 PAP2 PAP3 PAV4 PAV5 PAV6 MAV7 MAV8 MAV9 MAP10 MAP11 MAP12 ## 0 0 0 0 0 0 0 0 0 0 0 0 Because these are in z-score units, those standardized errors that are greater in absolute value than 2 are unexpected if the model has good fit. Although these can be inflated because of small standard deviations, some of the larger estimates are worth looking into. For example, with PAV5 and PAV7 there’s a z-score of 6.96, which is pretty big. It is a positive value indicating that the model-implied covariance (\\(0.292\\)) underestimated the covariance between these two variables. 10.2.5 Further inspecting our model’s parameters 10.2.5.1 Seeing which parameters were estimated Sometimes, we want to see what all of the estimated parameters are in the model. We can use the inspect() function on our model’s output. This will report which parameters are estimated and enumerate them (from 1 up to 42 in our model). By default, some parameters are fixed and others are estimated. For example, with our specification, each factor’s first observed variable’s loading is not estimated because it is fixed to 1 in order to place the factor on a scale. We can see this in the $lambda part of the output, where those parameters are not enumerated but are instead marked as 0, indicating they were not estimated. inspect(cfa_fit) ## $lambda ## PAP PAV MAV MAP ## PAP1 0 0 0 0 ## PAP2 1 0 0 0 ## PAP3 2 0 0 0 ## PAV4 0 0 0 0 ## PAV5 0 3 0 0 ## PAV6 0 4 0 0 ## MAV7 0 0 0 0 ## MAV8 0 0 5 0 ## MAV9 0 0 6 0 ## MAP10 0 0 0 0 ## MAP11 0 0 0 7 ## MAP12 0 0 0 8 ## ## $theta ## PAP1 PAP2 PAP3 PAV4 PAV5 PAV6 MAV7 MAV8 MAV9 MAP10 MAP11 MAP12 ## PAP1 9 ## PAP2 0 10 ## PAP3 0 0 11 ## PAV4 0 0 0 12 ## PAV5 0 0 0 0 13 ## PAV6 0 0 0 0 0 14 ## MAV7 0 0 0 0 0 0 15 ## MAV8 0 0 0 0 0 0 0 16 ## MAV9 0 0 0 0 0 0 0 0 17 ## MAP10 0 0 0 0 0 0 0 0 0 18 ## MAP11 0 0 0 0 0 0 0 0 0 0 19 ## MAP12 0 0 0 0 0 0 0 0 0 0 0 20 ## ## $psi ## PAP PAV MAV MAP ## PAP 21 ## PAV 25 22 ## MAV 26 28 23 ## MAP 27 29 30 24 ## ## $nu ## intrcp ## PAP1 31 ## PAP2 32 ## PAP3 33 ## PAV4 34 ## PAV5 35 ## PAV6 36 ## MAV7 37 ## MAV8 38 ## MAV9 39 ## MAP10 40 ## MAP11 41 ## MAP12 42 ## ## $alpha ## intrcp ## PAP 0 ## PAV 0 ## MAV 0 ## MAP 0 We also see in the $theta portion of the output that all of the variances in the \\(\\Theta_{\\delta}\\) matrix are estimated whereas none of the covariances is. The parameters in the $psi part of the output are the estimated variances and covariances among the factors.65 By default, these are all estimated.66 10.2.5.2 Optional: Saving parameter estimates to a data frame Though we can see the parameter estimates in the model output, we can also get them in data-frame format. This is convenient if we wish to save them to a data frame or export them to a CSV file. We see there are three operators here. We used the =~ in our model specification to refer to the factor-item loadings. The ~~ refers to variances and covariances, and the ~1 refers to the intercepts, which we can interpret in this model as the means. The lhs and rhs are the left and right hand sides of the operator, which we can use to filter which parameters we would like to deal with from the resulting data frame (as we do below). params &lt;- parameterEstimates(cfa_fit) params ## lhs op rhs est se z pvalue ci.lower ci.upper ## 1 PAP =~ PAP1 1.000 0.000 NA NA 1.000 1.000 ## 2 PAP =~ PAP2 1.035 0.037 28.243 0.000 0.963 1.106 ## 3 PAP =~ PAP3 1.052 0.038 27.683 0.000 0.978 1.127 ## 4 PAV =~ PAV4 1.000 0.000 NA NA 1.000 1.000 ## 5 PAV =~ PAV5 0.772 0.062 12.356 0.000 0.650 0.895 ## 6 PAV =~ PAV6 1.195 0.082 14.615 0.000 1.035 1.355 ## 7 MAV =~ MAV7 1.000 0.000 NA NA 1.000 1.000 ## 8 MAV =~ MAV8 1.956 0.169 11.600 0.000 1.625 2.286 ## 9 MAV =~ MAV9 1.845 0.160 11.547 0.000 1.532 2.158 ## 10 MAP =~ MAP10 1.000 0.000 NA NA 1.000 1.000 ## 11 MAP =~ MAP11 0.974 0.056 17.430 0.000 0.865 1.084 ## 12 MAP =~ MAP12 1.041 0.066 15.885 0.000 0.912 1.169 ## 13 PAP1 ~~ PAP1 0.816 0.063 12.990 0.000 0.693 0.939 ## 14 PAP2 ~~ PAP2 0.649 0.068 9.558 0.000 0.516 0.782 ## 15 PAP3 ~~ PAP3 0.671 0.079 8.483 0.000 0.516 0.826 ## 16 PAV4 ~~ PAV4 2.445 0.153 15.983 0.000 2.145 2.744 ## 17 PAV5 ~~ PAV5 2.038 0.116 17.564 0.000 1.811 2.265 ## 18 PAV6 ~~ PAV6 1.071 0.139 7.692 0.000 0.798 1.344 ## 19 MAV7 ~~ MAV7 1.594 0.081 19.625 0.000 1.435 1.753 ## 20 MAV8 ~~ MAV8 0.609 0.085 7.123 0.000 0.441 0.776 ## 21 MAV9 ~~ MAV9 1.104 0.100 11.048 0.000 0.908 1.299 ## 22 MAP10 ~~ MAP10 0.725 0.062 11.609 0.000 0.603 0.847 ## 23 MAP11 ~~ MAP11 0.327 0.033 9.775 0.000 0.262 0.393 ## 24 MAP12 ~~ MAP12 0.713 0.057 12.606 0.000 0.602 0.824 ## 25 PAP ~~ PAP 1.575 0.114 13.872 0.000 1.352 1.797 ## 26 PAV ~~ PAV 1.310 0.148 8.832 0.000 1.019 1.600 ## 27 MAV ~~ MAV 0.432 0.070 6.200 0.000 0.295 0.568 ## 28 MAP ~~ MAP 0.663 0.067 9.850 0.000 0.531 0.795 ## 29 PAP ~~ PAV 0.751 0.074 10.112 0.000 0.605 0.896 ## 30 PAP ~~ MAV 0.197 0.038 5.123 0.000 0.121 0.272 ## 31 PAP ~~ MAP 0.308 0.046 6.719 0.000 0.218 0.398 ## 32 PAV ~~ MAV 0.378 0.056 6.693 0.000 0.267 0.488 ## 33 PAV ~~ MAP 0.080 0.038 2.133 0.033 0.007 0.154 ## 34 MAV ~~ MAP 0.159 0.024 6.558 0.000 0.111 0.206 ## 35 PAP1 ~1 5.155 0.048 106.570 0.000 5.060 5.249 ## 36 PAP2 ~1 5.201 0.048 108.813 0.000 5.107 5.294 ## 37 PAP3 ~1 4.915 0.049 101.118 0.000 4.820 5.010 ## 38 PAV4 ~1 4.860 0.061 80.186 0.000 4.741 4.979 ## 39 PAV5 ~1 4.713 0.053 89.744 0.000 4.610 4.816 ## 40 PAV6 ~1 4.192 0.054 78.131 0.000 4.087 4.297 ## 41 MAV7 ~1 5.103 0.045 114.616 0.000 5.015 5.190 ## 42 MAV8 ~1 4.135 0.047 87.939 0.000 4.043 4.227 ## 43 MAV9 ~1 3.890 0.050 77.534 0.000 3.792 3.989 ## 44 MAP10 ~1 5.756 0.037 156.210 0.000 5.684 5.829 ## 45 MAP11 ~1 6.209 0.031 203.007 0.000 6.149 6.269 ## 46 MAP12 ~1 5.889 0.037 157.389 0.000 5.816 5.963 ## 47 PAP ~1 0.000 0.000 NA NA 0.000 0.000 ## 48 PAV ~1 0.000 0.000 NA NA 0.000 0.000 ## 49 MAV ~1 0.000 0.000 NA NA 0.000 0.000 ## 50 MAP ~1 0.000 0.000 NA NA 0.000 0.000 10.2.5.3 Generating path diagrams The semPlot package (Epskamp 2025) provides a function for plotting the output from lavaan’s CFA() function. We can generate the standardized and unstandardized solutions.67 There are several arguments available, which are described in the help files. semPlot::semPaths(cfa_fit, what = &quot;est&quot;, intercept = FALSE, rotation = 2, fade = F, edge.color = &quot;black&quot;, curvePivot = T, sizeMan = 8, residuals = F, curvature = 2.5, title = FALSE, sizeMan2 = 3, sizeLat2 = 8, label.cex = 1, edge.label.cex = 1.2) title(&quot;Unstandardized solution&quot;, line = 1) The dotted line indicates the path coefficient was not estimated because the factor was scaled to that item. We could specify the model with factor variances scaled to 1 and generate a new path model. 10.2.5.4 Examining modification indices Notice that if we permitted the errors to correlate between PAV5 and MAV7, we would improve our chi-square by 40.375. The estimated parameter change (epc) is 0.386, which is what the error covariance is estimated to be if we permitted this to be estimated in the model. Mod_inds &lt;- modindices(cfa_fit, standardized = TRUE, minimum.value = 3.84, sort. = TRUE) Mod_inds[1:10, ] ## lhs op rhs mi epc sepc.lv sepc.all sepc.nox ## 126 PAV5 ~~ MAV7 40.375 0.386 0.386 0.214 0.214 ## 125 PAV5 ~~ PAV6 33.826 -0.591 -0.591 -0.400 -0.400 ## 60 PAV =~ PAP1 30.608 -0.235 -0.269 -0.174 -0.174 ## 121 PAV4 ~~ MAV9 23.206 -0.310 -0.310 -0.189 -0.189 ## 53 PAP =~ PAV6 22.759 0.326 0.409 0.239 0.239 ## 57 PAP =~ MAP10 20.215 0.126 0.158 0.135 0.135 ## 98 PAP2 ~~ PAP3 19.871 -0.363 -0.363 -0.551 -0.551 ## 95 PAP1 ~~ MAP10 19.295 0.133 0.133 0.174 0.174 ## 88 PAP1 ~~ PAP3 17.587 0.311 0.311 0.420 0.420 ## 117 PAV4 ~~ PAV5 17.394 0.378 0.378 0.169 0.169 Because we used the sort. = TRUE argument, the parameters are ranked by modification index. We asked for only the first 10, though there seem to be many more. The ~~ symbolizes covariance; the =~ symbolizes the loading of an observed variable on a factor. With our current specification, the model is particularly failing to account for error covariances between PAV5 and two other items, MAV7 and PAV6. There also seems to be some unexpected relationship between PAP1 and the PAV factor that the model is failing to account for. There are several other sources of variance that seem to explain our model’s lack of fit. These modification indices are useful for understanding the sources of misfit in our model and might be used to critique the current model’s application to real data. 10.3 Using alternative model specifications 10.3.1 Rescaling the factors to be standardized If we wish to set our factor scales to have a variance of 1 instead of forcing them to be on the scale of the first variable, we can use the NA* as a prefix to the first observed variable in a factor to tell lavaan to estimate that parameter. We then should set the variance of the factor to 1 using the ~~ operator with *1. Let’s do this with the PAV factor using NA*PAV4 along with PAV ~~ 1*PAV: library(lavaan) mymod2 &lt;- &quot;PAP =~ PAP1 + PAP2 + PAP3 PAV =~ NA*PAV4 + PAV5 + PAV6 MAV =~ MAV7 + MAV8 + MAV9 MAP =~ MAP10 + MAP11 + MAP12 PAV ~~ 1*PAV &quot; cfa_fit2 &lt;- cfa(mymod2, data = dat, estimator = &quot;MLR&quot;, mimic = &quot;Mplus&quot;) We can use the summary() function on our model output, as we did with the earlier model. We can also look at the specific parameters.68 mod2_ests &lt;- parameterEstimates(cfa_fit2) library(dplyr) mod2_ests %&gt;% filter(op == &quot;=~&quot;, lhs == &quot;PAV&quot;) mod2_ests %&gt;% filter(op == &quot;~~&quot;, lhs == &quot;PAV&quot;) ## lhs op rhs est se z pvalue ci.lower ci.upper ## 1 PAV =~ PAV4 1.144 0.065 17.664 0 1.017 1.271 ## 2 PAV =~ PAV5 0.884 0.070 12.623 0 0.746 1.021 ## 3 PAV =~ PAV6 1.368 0.057 23.910 0 1.256 1.480 ## lhs op rhs est se z pvalue ci.lower ci.upper ## 1 PAV ~~ PAV 1.00 0.000 NA NA 1.000 1.000 ## 2 PAV ~~ MAV 0.33 0.044 7.529 0.000 0.244 0.416 ## 3 PAV ~~ MAP 0.07 0.033 2.113 0.035 0.005 0.135 We now see the pattern coefficient of PAV4 is being estimated but that the variance of the PAV factor has been set to 1 and is no longer being estimated. If we wish set all of the factors’ variances to 1 while estimating their respective first loadings, we can use the std.lv = TRUE argument with our original model specification: cfa_fit_stdlv &lt;- cfa(mymod, data = dat, estimator = &quot;MLR&quot;, mimic = &quot;Mplus&quot;, std.lv = TRUE ) summary(cfa_fit_stdlv) We can plot that model’s path diagram. semPlot::semPaths(cfa_fit_stdlv, what = &quot;est&quot;, intercept = F, rotation = 2, fade = F, edge.color = &quot;black&quot;, curvePivot = T, sizeMan = 8, residuals = F, curvature = 3, title = F, sizeMan2 = 3, sizeLat2 = 8, label.cex = 1, edge.label.cex = 1.2) title(&quot;Latent-variable standardized solution&quot;, line = 1) 10.3.2 Specifying a competing, nested, model Another reason to change model specifications is to look at a competing theory that is applicable to our data. If our competing theory hypothesized that the covariance between two factors is negligible or zero, we can fix the parameter. For instance, let’s pretend a competing theory was that PAP and MAP are unrelated. The ~~ is for covariance, so we set the covariance between them to be zero using PAP ~~ 0*MAP. library(lavaan) mod_compete &lt;- &quot;PAP =~ PAP1 + PAP2 + PAP3 PAV =~ PAV4 + PAV5 + PAV6 MAV =~ MAV7 + MAV8 + MAV9 MAP =~ MAP10 + MAP11 + MAP12 PAP ~~ 0*MAP &quot; cfa_fit_compete &lt;- cfa(mod_compete, data = dat, estimator = &quot;MLR&quot;, mimic = &quot;Mplus&quot;) summary(cfa_fit_compete) We can look at the fit statistics. Again, because we used estimator = MLR instead of the default maximum likelihood estimation, we are asking for scaled and robust versions of these fit indices. If we used the default, we’d remove the .scaled and .robust from our code here. fit_stats4 &lt;- fitMeasures(cfa_fit_compete, c(&quot;chisq.scaled&quot;,&quot;df.scaled&quot;, &quot;pvalue.scaled&quot;, &quot;cfi.robust&quot;, &quot;tli.robust&quot;, &quot;rmsea.robust&quot;, &quot;srmr&quot;)) fit_stats4 ## chisq.scaled df.scaled pvalue.scaled cfi.robust tli.robust rmsea.robust srmr ## 319.027 49.000 0.000 0.932 0.908 0.077 0.080 This model does not fit as well as that from the original theory that informed the first model specification. 10.3.3 Conducting model comparisons If we have competing theories with accompanying models, one of which is nested within the other, we can compare their fit. A general function that we can use is anova(), with the two models under comparison in the function. anova(cfa_fit, cfa_fit_compete) ## ## Scaled Chi-Squared Difference Test (method = &quot;satorra.bentler.2001&quot;) ## ## lavaan-&gt;lavTestLRT(): ## lavaan NOTE: The &quot;Chisq&quot; column contains standard test statistics, not the robust test that ## should be reported per model. A robust difference test is a function of two standard (not ## robust) statistics. ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## cfa_fit 48 40098 40305 283.98 ## cfa_fit_compete 49 40162 40364 350.04 48.756 1 2.899e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We see a statistically significant difference between the two models. This suggests that the more constrained model, which is the one in which we fixed the covariance between two factors to be zero, is worse than the one without that constraint. We can also look at the Akaike information criterion (AIC) for each model and identify which has worse fit by looking for the larger AIC value, which in this case was cfa_fit_compete. We can probably conclude that our data provide more support for the first theory, as the hypothesized structure has better fit to the data. When we present model comparisons, we should also present the two models’ fit statistics because if both models fit poorly, the comparison is not so meaningful. We could use the fitMeasures() function on each of the two models’ outputs. Another convenient function is from the semTools package, called compareFit(). It includes the results of the anova() function along with each model’s fit indices. 10.4 Estimating factor scores We can estimate the factor scores of each row in our data set using the lavPredict() function.69 f_scores &lt;- lavPredict(cfa_fit) head(f_scores) ## PAP PAV MAV MAP ## [1,] 0.9967932 -0.5423234 0.7889148 0.9098673 ## [2,] 1.7106071 0.8934636 -0.6179518 0.7322254 ## [3,] 0.2637596 0.7919548 0.5488746 0.5869992 ## [4,] 0.8299528 0.6843685 -1.1381761 -0.3129291 ## [5,] 1.7299879 1.4998621 0.6253357 0.8354626 ## [6,] 1.7275616 1.6978886 1.2722685 0.8941725 If we used a covariance matrix as our input (which would preclude us from using using estimator = \"MLR\"), but we have a raw data set on which we seek to estimate the factor scores, we can use the lavPredict() function but specify the data set using newdata = argument. Also, when we fit the CFA model, we should specify what the means of the variables are using the sample.mean = argument. We could have calculated the means for each variable and saved that output to an object to then use in that argument, or we could use the colMeans() function as a shortcut. S &lt;- cov(dat) n_p &lt;- nrow(dat) cfa_fit_S &lt;- cfa(mymod, sample.cov = S, sample.nobs = n_p, sample.mean = colMeans(dat), mimic = &quot;Mplus&quot;) f_scores_S &lt;- lavPredict(cfa_fit_S, newdata = dat) head(f_scores_S) ## PAP PAV MAV MAP ## [1,] 0.9967932 -0.5423234 0.7889148 0.9098673 ## [2,] 1.7106071 0.8934636 -0.6179518 0.7322254 ## [3,] 0.2637596 0.7919548 0.5488746 0.5869992 ## [4,] 0.8299528 0.6843685 -1.1381761 -0.3129291 ## [5,] 1.7299879 1.4998621 0.6253357 0.8354626 ## [6,] 1.7275616 1.6978886 1.2722685 0.8941725 Because the scores are in the same order as our data, we can append them to our data. Let’s attach them to the last four columns of our raw data frame and view those columns, along with the person ID. raw_scored &lt;- data.frame(raw, f_scores) raw_scored[1:5, c(1, 17:20)] # The first five people&#39;s scores ## PID PAP PAV MAV MAP ## 1 S0001 0.9967932 -0.5423234 0.7889148 0.9098673 ## 2 S0002 1.7106071 0.8934636 -0.6179518 0.7322254 ## 3 S0003 0.2637596 0.7919548 0.5488746 0.5869992 ## 4 S0004 0.8299528 0.6843685 -1.1381761 -0.3129291 ## 5 S0005 1.7299879 1.4998621 0.6253357 0.8354626 10.5 CFA with categorical data With categorical data, it is difficult to claim that the data meet the assumption of linearity. As a result, some reviewers will complain if we use our garden variety maximum likelihood estimation with data that are dichotomous or polytomous. One approach to conducting CFA with categorical data is to use weighted-least-squares-means-and-variance-adjusted estimation. Fortunately, we can use the term “WLSMV” to refer to this instead of spelling out the full name. This method uses a robust estimation adjustment on top of the diagonally weighted least squares estimation method, which is analogous to using maximum likelihood robust (MLR) with continuous numeric data instead of straight-up maximum likelihood estimation. There are two ways to specify WLSMV estimation in lavaan. One is to specify our variables as ordered factors. The other is to specify in the cfa() function which variables are ordered factors using the ordered = argument. In the example that follows, we will use the first approach. We should also explicitly use estimator = \"WLSMV\" in the cfa() function, though it is the default if the data are detected as being ordered factors. Remember that the term factor in R refers to the type of variable, to distinguish it from numeric, character, or logical types of variables. Though the same term is used in factor analysis to refer to the latent variable, these are not the same thing. 10.5.1 Using data from an R package Let’s use a data set from the ltm package (Rizopoulos 2022). The Environment data set automatically loads when we bring ltm into our working library.70 To view a description of this data set, use help(\"Environment\", package = \"ltm\"). library(ltm) str(Environment) ## &#39;data.frame&#39;: 291 obs. of 6 variables: ## $ LeadPetrol : Factor w/ 3 levels &quot;very concerned&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ RiverSea : Factor w/ 3 levels &quot;very concerned&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ RadioWaste : Factor w/ 3 levels &quot;very concerned&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ AirPollution: Factor w/ 3 levels &quot;very concerned&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ Chemicals : Factor w/ 3 levels &quot;very concerned&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ Nuclear : Factor w/ 3 levels &quot;very concerned&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... 10.5.2 Preparing the data The data set includes 6 variables and 291 observations. These are responses to items that are intended to capture respondents’ degrees of concern about each possible threat to the environment. Each of these six items is a factor-type variable (not to be confused with a latent-variable factor). The categories are not very concerned, slightly concerned, and very concerned. Let’s make each of these variables an ordered factor with lower levels of concern being ordered lower than higher levels of concern. One way to do this is to specify the possible category responses in this particular order and to use this in the levels = argument with the ordered() function. To make our code easy to read, we can save this order of levels to an object, naming it something like concern_levels. As is usual in R, spelling and case are important. One benefit of having factors over character or numeric data is that if a response does not fit any of the available levels, it is coded as NA, which alerts us to data-entry errors. concern_levels &lt;- c(&quot;not very concerned&quot;, &quot;slightly concerned&quot;, &quot;very concerned&quot;) Now we can use the ordered() function with the levels = argument with each variable using something like Environment$LeadPetrolF &lt;- ordered(Environment$LeadPetrol, levels = concern_levels). Because all of our variables use the same scale, we can apply this across all of them and save it as a dataframe. polydat &lt;- data.frame(lapply(Environment, ordered, levels = concern_levels)) str(polydat) ## &#39;data.frame&#39;: 291 obs. of 6 variables: ## $ LeadPetrol : Ord.factor w/ 3 levels &quot;not very concerned&quot;&lt;..: 3 3 3 3 3 3 3 3 3 3 ... ## $ RiverSea : Ord.factor w/ 3 levels &quot;not very concerned&quot;&lt;..: 3 3 3 3 3 3 3 3 3 3 ... ## $ RadioWaste : Ord.factor w/ 3 levels &quot;not very concerned&quot;&lt;..: 3 3 3 3 3 3 3 3 3 3 ... ## $ AirPollution: Ord.factor w/ 3 levels &quot;not very concerned&quot;&lt;..: 3 3 3 3 3 3 3 3 3 3 ... ## $ Chemicals : Ord.factor w/ 3 levels &quot;not very concerned&quot;&lt;..: 3 3 3 3 3 3 3 3 3 3 ... ## $ Nuclear : Ord.factor w/ 3 levels &quot;not very concerned&quot;&lt;..: 3 3 3 3 3 3 3 3 3 3 ... We can verify the order of levels of individual items using the levels() function. Here it is for the first item: levels(polydat$LeadPetrol) ## [1] &quot;not very concerned&quot; &quot;slightly concerned&quot; &quot;very concerned&quot; 10.5.3 Describing the data We can use the summary() function with our data to get the frequency of responses to each item. summary(polydat) ## LeadPetrol RiverSea RadioWaste AirPollution ## not very concerned: 17 not very concerned: 7 not very concerned: 18 not very concerned: 9 ## slightly concerned: 95 slightly concerned: 51 slightly concerned: 56 slightly concerned: 93 ## very concerned :179 very concerned :233 very concerned :217 very concerned :189 ## Chemicals Nuclear ## not very concerned: 17 not very concerned: 46 ## slightly concerned: 56 slightly concerned: 95 ## very concerned :218 very concerned :150 10.5.4 Fitting the CFA model Let’s say that the theory supports the practice of scoring the responses as reflections of a single latent variable. Mind you, this is simply for pedagogical purposes and not based on the original study (Brook, Taylor, and Prior 1991) that the ltm package cites as the source of these data. Here is our model specification and fit: library(lavaan) mymodcat &lt;- &quot;f1 =~ LeadPetrol + RiverSea + RadioWaste + AirPollution + Chemicals + Nuclear&quot; cat_fit &lt;- cfa(mymodcat, polydat, estimator = &quot;WLSMV&quot;, ordered = names(polydat), mimic = &quot;Mplus&quot;) The ordered = names(polydat) argument is superfluous here because our variables are already coded as ordered factors. However, if our data were numeric (with lower values referring to lower concern), we would need that argument. We can examine the results in the same manner we previously used with numeric data: summary(cat_fit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) ## lavaan 0.6-19 ended normally after 18 iterations ## ## Estimator DWLS ## Optimization method NLMINB ## Number of model parameters 18 ## ## Number of observations 291 ## ## Model Test User Model: ## Standard Scaled ## Test Statistic 30.953 54.517 ## Degrees of freedom 9 9 ## P-value (Chi-square) 0.000 0.000 ## Scaling correction factor 0.581 ## Shift parameter 1.208 ## simple second-order correction (WLSMV) ## ## Model Test Baseline Model: ## ## Test statistic 2326.419 1454.675 ## Degrees of freedom 15 15 ## P-value 0.000 0.000 ## Scaling correction factor 1.606 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.991 0.968 ## Tucker-Lewis Index (TLI) 0.984 0.947 ## ## Robust Comparative Fit Index (CFI) 0.880 ## Robust Tucker-Lewis Index (TLI) 0.799 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.092 0.132 ## 90 Percent confidence interval - lower 0.058 0.100 ## 90 Percent confidence interval - upper 0.128 0.167 ## P-value H_0: RMSEA &lt;= 0.050 0.024 0.000 ## P-value H_0: RMSEA &gt;= 0.080 0.738 0.995 ## ## Robust RMSEA 0.239 ## 90 Percent confidence interval - lower 0.167 ## 90 Percent confidence interval - upper 0.316 ## P-value H_0: Robust RMSEA &lt;= 0.050 0.000 ## P-value H_0: Robust RMSEA &gt;= 0.080 1.000 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.074 0.074 ## ## Parameter Estimates: ## ## Parameterization Delta ## Standard errors Robust.sem ## Information Expected ## Information saturated (h1) model Unstructured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## f1 =~ ## LeadPetrol 1.000 0.626 0.626 ## RiverSea 1.324 0.126 10.542 0.000 0.829 0.829 ## RadioWaste 1.407 0.125 11.246 0.000 0.881 0.881 ## AirPollution 1.402 0.125 11.243 0.000 0.878 0.878 ## Chemicals 1.372 0.117 11.724 0.000 0.859 0.859 ## Nuclear 1.180 0.117 10.077 0.000 0.738 0.738 ## ## Thresholds: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## LeadPetrol|t1 -1.568 0.118 -13.260 0.000 -1.568 -1.568 ## LeadPetrol|t2 -0.293 0.075 -3.909 0.000 -0.293 -0.293 ## RiverSea|t1 -1.976 0.159 -12.408 0.000 -1.976 -1.976 ## RiverSea|t2 -0.844 0.084 -10.035 0.000 -0.844 -0.844 ## RadioWaste|t1 -1.539 0.116 -13.253 0.000 -1.539 -1.539 ## RadioWaste|t2 -0.661 0.080 -8.275 0.000 -0.661 -0.661 ## AirPollutin|t1 -1.867 0.146 -12.795 0.000 -1.867 -1.867 ## AirPollutin|t2 -0.384 0.076 -5.070 0.000 -0.384 -0.384 ## Chemicals|t1 -1.568 0.118 -13.260 0.000 -1.568 -1.568 ## Chemicals|t2 -0.672 0.080 -8.387 0.000 -0.672 -0.672 ## Nuclear|t1 -1.002 0.089 -11.276 0.000 -1.002 -1.002 ## Nuclear|t2 -0.039 0.074 -0.526 0.599 -0.039 -0.039 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .LeadPetrol 0.608 0.608 0.608 ## .RiverSea 0.313 0.313 0.313 ## .RadioWaste 0.224 0.224 0.224 ## .AirPollution 0.230 0.230 0.230 ## .Chemicals 0.262 0.262 0.262 ## .Nuclear 0.455 0.455 0.455 ## f1 0.392 0.068 5.739 0.000 1.000 1.000 ## ## R-Square: ## Estimate ## LeadPetrol 0.392 ## RiverSea 0.687 ## RadioWaste 0.776 ## AirPollution 0.770 ## Chemicals 0.738 ## Nuclear 0.545 We can use the same procedures we used above with our numeric CFA model to examine the model’s estimation, fit statistics, parameters, and plots.71 As with MLR estimation, we report the fit indices under the Robust column of the output.72 In the output, we can notice some differences from what we would observe with continuous data. There are no statistical estimates of the items’ error variances, nor are the intercepts of the items estimated. In other words, there is no mean value of each variable. This is because these are categorical variables.73 What we have instead are thresholds. Because we have three categories for each item, there are two thresholds for each item, t1 and t2. These are estimates on the latent-variable scale where the transition from the category below to the next higher category takes place. With these data, t1 is the treshold between not very concerned and slightly concerned and t2 is the threshold between slightly concerned and very concerned. If we have more categories, we will have more thresholds. With dichotomous data, we will have a single threshold, whereas with a seven point scale, we will have six thresholds. The standard error of a threshold depends in part on the number of data points we have on both sides of that threshold. 10.6 Way TL;DR Summary We use CFA to compare empirical data with theory. We specify a CFA model based on the theory. We can use model fit statistics to make claims about how well the theory matches the data. Similar to EFA, we can examine the pattern coefficients to determine which items are most strongly predicted by the factor(s). We can compare competing theories, which are reflected in different model specifications, and conduct tests to see which provides a better match with the empirical data. Before we conduct the CFA, we need to be sure the CFA model will be appropriate for the data, so we examine statistical assumptions, particularly if there are outliers and if the data are not continuous. If there are outliers, we can perform the entire analysis with them, then remove them, and see if (hope that) we get the same result. Multivariate normality is not really a big deal, unless we wish to use maximum likelihood estimation, which depends on that assumption. If the data are continuous, we can use the “regular” CFA models. If they are categorical, we need to specify a model estimation method that works for those data, such as WLSMV estimation. Before we interpret the results of the model, we need to be sure the model fits the data. We use model fit statistics, such as CFI and RMSEA for this. When we interpret the model and its pattern coefficients, we should pay attention to the scale of the pattern coefficients. For people who are accustomed to EFA, we can use standardized factors, which makes the mean of each factor 0 and the variance 1. These are typically reported in the Std.all column in the output. When we have a model that does not fit well and we want to see where things may have gone wrong, we can examine the modification indices. We should avoid simply making these modifications and refitting our data, as that’s a form of p-hacking. But, it can tell us what seems to be going wrong. We can save people’s factor scores and use them in subsequent analyses. 10.7 References Bandalos, Deborah L. 2018. Measurement Theory and Applications for the Social Sciences. New York: Guilford. Bates, Douglas, Martin Maechler, and Mikael Jagan. 2025. Matrix: Sparse and Dense Matrix Classes and Methods. https://Matrix.R-forge.R-project.org. Brook, L., B. Taylor, and G. Prior. 1991. British Social Attitudes, 1990, Survey. London: SCPR. Cattell, Raymond B. 1966. “The Scree Test for the Number of Factors.” Multivariate Behavioral Research 1 (2): 245–76. https://doi.org/10.1207/s15327906mbr0102_10. Dayanand Ubrangala, Kiran R, Ravi Prasad Kondapalli, and Sayan Putatunda. 2024. SmartEDA: Summarize and Explore the Data. https://daya6489.github.io/SmartEDA/. DiStefano, Christine, Min Zhu, and Diana Mîndrilã. 2009. “Understanding and Using Factor Scores: Considerations for the Applied Researcher” 14 (20). https://doi.org/10.7275/DA8T-4G52. Epskamp, Sacha. 2025. semPlot: Path Diagrams and Visual Analysis of Various SEM Packages’ Output. https://github.com/SachaEpskamp/semPlot. Everitt, Brian, and Torsten Hothorn. 2011. An Introduction to Applied Multivariate Analysis with r. Use r! New York: Springer. Fox, John. 2022. Polycor: Polychoric and Polyserial Correlations. https://r-forge.r-project.org/projects/polycor/. Fox, John, Sanford Weisberg, and Brad Price. 2024. Car: Companion to Applied Regression. https://r-forge.r-project.org/projects/car/. Garrido, Luis Eduardo, Francisco José Abad, and Vicente Ponsoda. 2015. “Are Fit Indices Really Fit to Estimate the Number of Factors with Categorical Variables? Some Cautionary Findings via Monte Carlo Simulation.” Psychological Methods (online first): 1–19. https://doi.org/10.1037/met0000064. Gohel, David, and Panagiotis Skintzos. 2025. Flextable: Functions for Tabular Reporting. https://ardata-fr.github.io/flextable-book/. Horn, John L. 1965. “A Rationale and Test for the Number of Factors in Factor Analysis.” Psychometrika 30 (2): 179–85. https://doi.org/10.1007/BF02289447. Jarek, Sławomir. 2024. Mvnormtest: Normality Test for Multivariate Variables. https://doi.org/10.32614/CRAN.package.mvnormtest. Jolliffe, I. T. 1972. “Discarding Variables in a Principal Component Analysis. I: Artificial Data.” Journal of the Royal Statistical Society. Series C (Applied Statistics) 21 (2): 160–73. https://doi.org/10.2307/2346488. Jorgensen, Terrence D., Sunthud Pornprasertmanit, Alexander M. Schoemann, and Yves Rosseel. 2025. semTools: Useful Tools for Structural Equation Modeling. https://github.com/simsem/semTools/wiki. Kaiser, Henry F. 1974. “An Index of Factorial Simplicity.” Psychometrika 39 (1): 31–36. https://doi.org/10.1007/BF02291575. Korkmaz, Selcuk, Dincer Goksuluk, and Gokmen Zararsiz. 2025. MVN: Multivariate Normality Tests. https://selcukorkmaz.github.io/mvn-tutorial/. Kuhn, Max, Simon Jackson, and Jorge Cimentada. 2025. Corrr: Correlations in r. https://github.com/tidymodels/corrr. Lorenzo-Seva, Urbano, Marieke E. Timmerman, and Henk A. L. Kiers. 2011. “The Hull Method for Selecting the Number of Common Factors.” Multivariate Behavioral Research 46 (2): 340–64. https://doi.org/10.1080/00273171.2011.564527. Luo, Lan, Cara Arizmendi, and Kathleen M. Gates. 2019. “Exploratory Factor Analysis (EFA) Programs in R.” Structural Equation Modeling: A Multidisciplinary Journal 26 (5): 819–26. https://doi.org/10.1080/10705511.2019.1615835. Navarro-Gonzalez, David, and Urbano Lorenzo-Seva. 2021. EFA.MRFA: Dimensionality Assessment Using Minimum Rank Factor Analysis. https://doi.org/10.32614/CRAN.package.EFA.MRFA. Raiche, Gilles, and David Magis. 2025. nFactors: Parallel Analysis and Other Non Graphical Solutions to the Cattell Scree Test. https://doi.org/10.32614/CRAN.package.nFactors. Revelle, William. 2025. Psych: Procedures for Psychological, Psychometric, and Personality Research. https://personality-project.org/r/psych/. Rizopoulos, Dimitris. 2022. Ltm: Latent Trait Models Under IRT. https://github.com/drizopoulos/ltm. Rosseel, Yves, Terrence D. Jorgensen, and Luc De Wilde. 2024. Lavaan: Latent Variable Analysis. https://lavaan.ugent.be. Signorell, Andri. 2025. DescTools: Tools for Descriptive Statistics. https://andrisignorell.github.io/DescTools/. Skrondal, Anders, and Petter Laake. 2001. “Regression Among Factor Scores.” Psychometrika 66 (4): 563–75. https://doi.org/10.1007/BF02296196. Tabachnick, Barbara G., and Linda S. Fidell. 2013. Using Multivariate Statistics. 6th ed. Boston: Pearson Education. Tucker, Ledyard R. 1971. “Relations of Factor Score Estimates to Their Use.” Psychometrika 36 (4): 427–36. https://doi.org/10.1007/BF02291367. References Bandalos, Deborah L. 2018. Measurement Theory and Applications for the Social Sciences. New York: Guilford. Brook, L., B. Taylor, and G. Prior. 1991. British Social Attitudes, 1990, Survey. London: SCPR. Epskamp, Sacha. 2025. semPlot: Path Diagrams and Visual Analysis of Various SEM Packages’ Output. https://github.com/SachaEpskamp/semPlot. Garrido, Luis Eduardo, Francisco José Abad, and Vicente Ponsoda. 2015. “Are Fit Indices Really Fit to Estimate the Number of Factors with Categorical Variables? Some Cautionary Findings via Monte Carlo Simulation.” Psychological Methods (online first): 1–19. https://doi.org/10.1037/met0000064. Gohel, David, and Panagiotis Skintzos. 2025. Flextable: Functions for Tabular Reporting. https://ardata-fr.github.io/flextable-book/. Jarek, Sławomir. 2024. Mvnormtest: Normality Test for Multivariate Variables. https://doi.org/10.32614/CRAN.package.mvnormtest. Jorgensen, Terrence D., Sunthud Pornprasertmanit, Alexander M. Schoemann, and Yves Rosseel. 2025. semTools: Useful Tools for Structural Equation Modeling. https://github.com/simsem/semTools/wiki. Korkmaz, Selcuk, Dincer Goksuluk, and Gokmen Zararsiz. 2025. MVN: Multivariate Normality Tests. https://selcukorkmaz.github.io/mvn-tutorial/. Rizopoulos, Dimitris. 2022. Ltm: Latent Trait Models Under IRT. https://github.com/drizopoulos/ltm. Rosseel, Yves, Terrence D. Jorgensen, and Luc De Wilde. 2024. Lavaan: Latent Variable Analysis. https://lavaan.ugent.be. Here’s the code for printing an ordered set using the Base R approach, which is a little convoluted but is useful if tidyverse misbehaves: out.liars[ order(out.liars$distances, decreasing = TRUE) , ].↩︎ We can also use the psych package’s outlier() function to plot multivariate outliers. psych::outlier(dat).↩︎ We could also use the mvn() function from the MVN package (Korkmaz, Goksuluk, and Zararsiz 2025). The mvn() function has an option to print the histogram of each variable similar to what we created above. The MVN package also has the mardia() function, which includes a Q-Q plot.↩︎ In lavaan, we specify this in our cfa() function with the argument estimator = \"MLR\".↩︎ The estimator = \"MLR\" specification does not work with covariance data. This means that if we use the covariance matrix as the input, we assume the data are multivariate normal.↩︎ Because our factors are predictors of the observed variables, we can call this a phi matrix even though it is listed as psi.↩︎ If you’re curious, the $nu \\((\\nu)\\) portion shows that the model also estimates the intercept of each of the variables, which we can think of as their means and which can be interpreted as the average facility to endorse each item. These are sometimes called tau \\((\\tau)\\) parameters. In item-response theory these are essentially item difficulty parameters. The $alpha \\((\\alpha)\\) intercepts are fixed to zero by default. In most models, these are the factor means. We see that altogether we have 42 parameters being estimated, which we saw in our original output from summary(cfa_fit). We can also remember that when we estimate the model-implied covariance \\((\\Sigma({\\hat{\\theta}}))\\), we are not estimating the items’ means, so we do not include the number of \\(\\nu\\) estimates when we consider the model’s degrees of freedom \\((\\frac{\\nu(\\nu + 1)}{2} - \\text{number of parameters} = 78 - 30 = 48)\\).↩︎ For standardized parameters, include what = \"std\".↩︎ We’re using the dplyr package’s filter() function so we can filter specific rows. Instead of the filter() function, we could use Base R with indexing, such as mod2_ests[mod2_ests$op == \"=~\", ].↩︎ Note that we’re not really predicting future scores; we’re estimating them.↩︎ If we want to see what other data sets are included in that package, we can run data(package = \"ltm\") after bringing the ltm package into our working library.↩︎ with the fit indexes, we specify .scaled suffixes rather than .robust, which are not available with WLSMV estimation.↩︎ There is research into the appropriateness of these fit indices with categorical data that suggests that RMSEA and SRMR are less appropriate than CFI (Garrido, Abad, and Ponsoda 2015).↩︎ Additionally, by default, the mean of the factor, f1, is 0.↩︎ "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
