<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>7 Example 1, to see some properties of PCA | EDUC 644 Applied Multivariate Statistics</title>
  <meta name="description" content="Course materials for multivariate statistics" />
  <meta name="generator" content="bookdown 0.44 and GitBook 2.6.7" />

  <meta property="og:title" content="7 Example 1, to see some properties of PCA | EDUC 644 Applied Multivariate Statistics" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Course materials for multivariate statistics" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="7 Example 1, to see some properties of PCA | EDUC 644 Applied Multivariate Statistics" />
  
  <meta name="twitter:description" content="Course materials for multivariate statistics" />
  

<meta name="author" content="George Harrison" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="write-up.html"/>
<link rel="next" href="example-two.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<link href="libs/tabwid-1.1.3/tabwid.css" rel="stylesheet" />
<script src="libs/tabwid-1.1.3/tabwid.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Multivariate Statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><i class="fa fa-check"></i><b>1</b> Terminology and Matrix Manipulations Used in Multivariate Statistics</a>
<ul>
<li class="chapter" data-level="1.1" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#scalars"><i class="fa fa-check"></i><b>1.1</b> Scalars</a></li>
<li class="chapter" data-level="1.2" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#vectors"><i class="fa fa-check"></i><b>1.2</b> Vectors</a></li>
<li class="chapter" data-level="1.3" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#matrices"><i class="fa fa-check"></i><b>1.3</b> Matrices</a></li>
<li class="chapter" data-level="1.4" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#symmetrical-matrices"><i class="fa fa-check"></i><b>1.4</b> Symmetrical Matrices</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#variance-covariance-matrices"><i class="fa fa-check"></i><b>1.4.1</b> Variance-covariance Matrices</a></li>
<li class="chapter" data-level="1.4.2" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#greek-letters"><i class="fa fa-check"></i><b>1.4.2</b> Greek letters</a></li>
<li class="chapter" data-level="1.4.3" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#example-variance-covariance-matrix"><i class="fa fa-check"></i><b>1.4.3</b> Example variance-covariance matrix</a></li>
<li class="chapter" data-level="1.4.4" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#the-diagonal-and-trace"><i class="fa fa-check"></i><b>1.4.4</b> The diagonal and trace</a></li>
<li class="chapter" data-level="1.4.5" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#an-identity-matrix"><i class="fa fa-check"></i><b>1.4.5</b> An identity matrix</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#transposed-vectors-and-matrices"><i class="fa fa-check"></i><b>1.5</b> Transposed Vectors and Matrices</a></li>
<li class="chapter" data-level="1.6" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#matrix-addition-and-subtraction"><i class="fa fa-check"></i><b>1.6</b> Matrix Addition and Subtraction</a></li>
<li class="chapter" data-level="1.7" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#operations-on-matrices-with-a-scalar"><i class="fa fa-check"></i><b>1.7</b> Operations on Matrices with a Scalar</a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#sum-of-square-and-cross-product-sscp-matrices"><i class="fa fa-check"></i><b>1.7.1</b> Sum-of-square-and-cross-product (SSCP) matrices</a></li>
<li class="chapter" data-level="1.7.2" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#operations-on-elements-of-a-matrix-example-with-a-correlation-matrix"><i class="fa fa-check"></i><b>1.7.2</b> Operations on elements of a matrix: Example with a correlation matrix</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#matrix-multiplication"><i class="fa fa-check"></i><b>1.8</b> Matrix Multiplication</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#another-example-of-matrix-multiplication-to-get-the-sscp"><i class="fa fa-check"></i><b>1.8.1</b> Another example of matrix multiplication, to get the SSCP</a></li>
<li class="chapter" data-level="1.8.2" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#the-ordering-is-important"><i class="fa fa-check"></i><b>1.8.2</b> The ordering is important</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#inverse-of-a-matrix"><i class="fa fa-check"></i><b>1.9</b> Inverse of a Matrix</a></li>
<li class="chapter" data-level="1.10" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#matrix-determinants-as-generalized-variance"><i class="fa fa-check"></i><b>1.10</b> Matrix determinants, as generalized variance</a></li>
<li class="chapter" data-level="1.11" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#eigenvectors-and-eigenvalues"><i class="fa fa-check"></i><b>1.11</b> Eigenvectors and Eigenvalues</a></li>
<li class="chapter" data-level="1.12" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#summary"><i class="fa fa-check"></i><b>1.12</b> Summary</a></li>
<li class="chapter" data-level="1.13" data-path="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html"><a href="terminology-and-matrix-manipulations-used-in-multivariate-statistics.html#optional-spss-and-r-code-to-manipulate-matrices"><i class="fa fa-check"></i><b>1.13</b> Optional: SPSS and R Code to Manipulate Matrices</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="hotellings-t2.html"><a href="hotellings-t2.html"><i class="fa fa-check"></i><b>2</b> Hotelling’s <span class="math inline">\(T^2\)</span></a>
<ul>
<li class="chapter" data-level="2.1" data-path="hotellings-t2.html"><a href="hotellings-t2.html#within-group-covariance-matrices"><i class="fa fa-check"></i><b>2.1</b> Within-group covariance matrices</a></li>
<li class="chapter" data-level="2.2" data-path="hotellings-t2.html"><a href="hotellings-t2.html#sscp-matrices"><i class="fa fa-check"></i><b>2.2</b> SSCP matrices</a></li>
<li class="chapter" data-level="2.3" data-path="hotellings-t2.html"><a href="hotellings-t2.html#conducting-the-test-using-functions"><i class="fa fa-check"></i><b>2.3</b> Conducting the Test using Functions</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="planned-comparisons-and-contrasts.html"><a href="planned-comparisons-and-contrasts.html"><i class="fa fa-check"></i><b>3</b> Planned Comparisons and Contrasts</a>
<ul>
<li class="chapter" data-level="3.1" data-path="planned-comparisons-and-contrasts.html"><a href="planned-comparisons-and-contrasts.html#data-from-week-1"><i class="fa fa-check"></i><b>3.1</b> Data from Week 1</a></li>
<li class="chapter" data-level="3.2" data-path="planned-comparisons-and-contrasts.html"><a href="planned-comparisons-and-contrasts.html#orthogonal-contrasts"><i class="fa fa-check"></i><b>3.2</b> Orthogonal contrasts</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="planned-comparisons-and-contrasts.html"><a href="planned-comparisons-and-contrasts.html#conditions-for-orthogonal-contrasts"><i class="fa fa-check"></i><b>3.2.1</b> Conditions for orthogonal contrasts</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="planned-comparisons-and-contrasts.html"><a href="planned-comparisons-and-contrasts.html#non-orthogonal-contrasts"><i class="fa fa-check"></i><b>3.3</b> Non-orthogonal contrasts</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="planned-comparisons-and-contrasts.html"><a href="planned-comparisons-and-contrasts.html#accounting-for-type-i-error-in-non-orthogonal-contrasts."><i class="fa fa-check"></i><b>3.3.1</b> Accounting for Type I error in non-orthogonal contrasts.</a></li>
<li class="chapter" data-level="3.3.2" data-path="planned-comparisons-and-contrasts.html"><a href="planned-comparisons-and-contrasts.html#post-hoc-tests"><i class="fa fa-check"></i><b>3.3.2</b> Post-hoc tests</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html"><i class="fa fa-check"></i><b>4</b> Week 03 Statistical Assumptions of MANOVA</a>
<ul>
<li class="chapter" data-level="4.1" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html#data"><i class="fa fa-check"></i><b>4.1</b> Data</a></li>
<li class="chapter" data-level="4.2" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html#descriptive-statistics"><i class="fa fa-check"></i><b>4.2</b> Descriptive statistics</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html#by-group-descriptive-statistics"><i class="fa fa-check"></i><b>4.2.1</b> By-group descriptive statistics</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html#normality-assumption"><i class="fa fa-check"></i><b>4.3</b> Normality Assumption</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html#univariate-normality"><i class="fa fa-check"></i><b>4.3.1</b> Univariate Normality</a></li>
<li class="chapter" data-level="4.3.2" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html#examining-univariate-normality-using-quantile-quantile-plots"><i class="fa fa-check"></i><b>4.3.2</b> Examining univariate normality using quantile-quantile plots</a></li>
<li class="chapter" data-level="4.3.3" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html#a-caution-about-statistical-tests-of-statistical-assumptions"><i class="fa fa-check"></i><b>4.3.3</b> A caution about statistical tests of statistical assumptions</a></li>
<li class="chapter" data-level="4.3.4" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html#mahalanobis-distance-and-multivariate-outliers"><i class="fa fa-check"></i><b>4.3.4</b> Mahalanobis Distance and Multivariate Outliers</a></li>
<li class="chapter" data-level="4.3.5" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html#statistical-tests-of-multivariate-normality"><i class="fa fa-check"></i><b>4.3.5</b> Statistical Tests of Multivariate Normality</a></li>
<li class="chapter" data-level="4.3.6" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html#homogeneity-of-covariance"><i class="fa fa-check"></i><b>4.3.6</b> Homogeneity of Covariance</a></li>
<li class="chapter" data-level="4.3.7" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html#conclusions-about-our-statistical-assumptions"><i class="fa fa-check"></i><b>4.3.7</b> Conclusions about our statistical assumptions</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html#fitting-the-manova-model"><i class="fa fa-check"></i><b>4.4</b> Fitting the MANOVA Model</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html#follow-up-analysis-with-univariate-f-tests"><i class="fa fa-check"></i><b>4.4.1</b> Follow-up analysis with univariate <em>F</em>-tests</a></li>
<li class="chapter" data-level="4.4.2" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html#effect-size"><i class="fa fa-check"></i><b>4.4.2</b> Effect size</a></li>
<li class="chapter" data-level="4.4.3" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html#comparing-the-model-without-outlier"><i class="fa fa-check"></i><b>4.4.3</b> Comparing the Model Without Outlier</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html#robust-one-way-manova"><i class="fa fa-check"></i><b>4.5</b> Robust One-way MANOVA</a></li>
<li class="chapter" data-level="4.6" data-path="week-03-statistical-assumptions-of-manova.html"><a href="week-03-statistical-assumptions-of-manova.html#summary-1"><i class="fa fa-check"></i><b>4.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="week-04-factorial-manova.html"><a href="week-04-factorial-manova.html"><i class="fa fa-check"></i><b>5</b> Week 04 Factorial MANOVA</a>
<ul>
<li class="chapter" data-level="5.1" data-path="week-04-factorial-manova.html"><a href="week-04-factorial-manova.html#drawing-from-ch.-7-of-tabachnick-fidell"><i class="fa fa-check"></i><b>5.1</b> Drawing from Ch. 7 of Tabachnick &amp; Fidell</a></li>
<li class="chapter" data-level="5.2" data-path="week-04-factorial-manova.html"><a href="week-04-factorial-manova.html#following-tabachnick-and-fidells-notes"><i class="fa fa-check"></i><b>5.2</b> Following Tabachnick and Fidell’s notes</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="week-04-factorial-manova.html"><a href="week-04-factorial-manova.html#another-way-to-get-the-sscp_w"><i class="fa fa-check"></i><b>5.2.1</b> Another way to get the <span class="math inline">\(SSCP_W\)</span>:</a></li>
<li class="chapter" data-level="5.2.2" data-path="week-04-factorial-manova.html"><a href="week-04-factorial-manova.html#mathbfsscp_between-for-each-factor"><i class="fa fa-check"></i><b>5.2.2</b> <span class="math inline">\(\mathbf{SSCP_{Between}}\)</span> for each factor</a></li>
<li class="chapter" data-level="5.2.3" data-path="week-04-factorial-manova.html"><a href="week-04-factorial-manova.html#mathbfsscp_between-for-the-interaction"><i class="fa fa-check"></i><b>5.2.3</b> <span class="math inline">\(\mathbf{SSCP_{Between}}\)</span> for the interaction</a></li>
<li class="chapter" data-level="5.2.4" data-path="week-04-factorial-manova.html"><a href="week-04-factorial-manova.html#these-by-hand-calculations-will-be-available-in-manova."><i class="fa fa-check"></i><b>5.2.4</b> These by-hand calculations will be available in <code>manova()</code>.</a></li>
<li class="chapter" data-level="5.2.5" data-path="week-04-factorial-manova.html"><a href="week-04-factorial-manova.html#calculating-the-generalized-variance"><i class="fa fa-check"></i><b>5.2.5</b> Calculating the generalized variance</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="week-04-factorial-manova.html"><a href="week-04-factorial-manova.html#calculating-wilks-lambda"><i class="fa fa-check"></i><b>5.3</b> Calculating Wilks’ Lambda</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="week-04-factorial-manova.html"><a href="week-04-factorial-manova.html#fit-the-factorial-manova"><i class="fa fa-check"></i><b>5.3.1</b> Fit the factorial MANOVA</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="week-04-factorial-manova.html"><a href="week-04-factorial-manova.html#mancova"><i class="fa fa-check"></i><b>5.4</b> MANCOVA</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="week-04-factorial-manova.html"><a href="week-04-factorial-manova.html#mancova-example"><i class="fa fa-check"></i><b>5.4.1</b> MANCOVA example</a></li>
<li class="chapter" data-level="5.4.2" data-path="week-04-factorial-manova.html"><a href="week-04-factorial-manova.html#lets-think-about-ordering-and-type-i-sums-of-squares"><i class="fa fa-check"></i><b>5.4.2</b> Let’s think about ordering and Type I sums of squares</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="week-04-factorial-manova.html"><a href="week-04-factorial-manova.html#fitting-the-discriminant-function-analysis-model"><i class="fa fa-check"></i><b>5.5</b> Fitting the discriminant function analysis model</a></li>
<li class="chapter" data-level="5.6" data-path="week-04-factorial-manova.html"><a href="week-04-factorial-manova.html#dimension-reduction-analysis"><i class="fa fa-check"></i><b>5.6</b> Dimension reduction analysis</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="week-04-factorial-manova.html"><a href="week-04-factorial-manova.html#effect-sizes"><i class="fa fa-check"></i><b>5.6.1</b> Effect sizes</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="week-04-factorial-manova.html"><a href="week-04-factorial-manova.html#structure-and-standardized-discriminant-function-coefficients"><i class="fa fa-check"></i><b>5.7</b> Structure and standardized discriminant function coefficients</a></li>
<li class="chapter" data-level="5.8" data-path="week-04-factorial-manova.html"><a href="week-04-factorial-manova.html#discriminant-function-scores"><i class="fa fa-check"></i><b>5.8</b> Discriminant function scores</a></li>
<li class="chapter" data-level="5.9" data-path="week-04-factorial-manova.html"><a href="week-04-factorial-manova.html#group-means-centroids-of-the-discriminant-function-scores"><i class="fa fa-check"></i><b>5.9</b> Group means (centroids) of the discriminant-function scores</a></li>
<li class="chapter" data-level="5.10" data-path="week-04-factorial-manova.html"><a href="week-04-factorial-manova.html#bivariate-scatterplot"><i class="fa fa-check"></i><b>5.10</b> Bivariate scatterplot</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="write-up.html"><a href="write-up.html"><i class="fa fa-check"></i><b>6</b> Write-up</a></li>
<li class="chapter" data-level="7" data-path="example-1-to-see-some-properties-of-pca.html"><a href="example-1-to-see-some-properties-of-pca.html"><i class="fa fa-check"></i><b>7</b> Example 1, to see some properties of PCA</a>
<ul>
<li class="chapter" data-level="7.1" data-path="example-1-to-see-some-properties-of-pca.html"><a href="example-1-to-see-some-properties-of-pca.html#pca-with-covariance-vs.-correlation"><i class="fa fa-check"></i><b>7.1</b> PCA with Covariance vs. Correlation</a></li>
<li class="chapter" data-level="7.2" data-path="example-1-to-see-some-properties-of-pca.html"><a href="example-1-to-see-some-properties-of-pca.html#unscaled-eigenvectors"><i class="fa fa-check"></i><b>7.2</b> Unscaled eigenvectors</a></li>
<li class="chapter" data-level="7.3" data-path="example-1-to-see-some-properties-of-pca.html"><a href="example-1-to-see-some-properties-of-pca.html#looking-at-the-properties-of-eigenvectors-and-eigenvalues"><i class="fa fa-check"></i><b>7.3</b> Looking at the properties of eigenvectors and eigenvalues</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="example-1-to-see-some-properties-of-pca.html"><a href="example-1-to-see-some-properties-of-pca.html#eigenvalues-add-up-to-total-observed-variance"><i class="fa fa-check"></i><b>7.3.1</b> Eigenvalues add up to total observed variance</a></li>
<li class="chapter" data-level="7.3.2" data-path="example-1-to-see-some-properties-of-pca.html"><a href="example-1-to-see-some-properties-of-pca.html#each-components-proportion-of-variance"><i class="fa fa-check"></i><b>7.3.2</b> Each component’s proportion of variance</a></li>
<li class="chapter" data-level="7.3.3" data-path="example-1-to-see-some-properties-of-pca.html"><a href="example-1-to-see-some-properties-of-pca.html#rescaling-the-eigenvectors"><i class="fa fa-check"></i><b>7.3.3</b> Rescaling the eigenvectors</a></li>
<li class="chapter" data-level="7.3.4" data-path="example-1-to-see-some-properties-of-pca.html"><a href="example-1-to-see-some-properties-of-pca.html#reproducing-the-covariance-or-correlation-matrix"><i class="fa fa-check"></i><b>7.3.4</b> Reproducing the covariance (or correlation) matrix</a></li>
<li class="chapter" data-level="7.3.5" data-path="example-1-to-see-some-properties-of-pca.html"><a href="example-1-to-see-some-properties-of-pca.html#summary-of-the-vectors-and-matrices"><i class="fa fa-check"></i><b>7.3.5</b> Summary of the vectors and matrices</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="example-1-to-see-some-properties-of-pca.html"><a href="example-1-to-see-some-properties-of-pca.html#using-the-psych-package"><i class="fa fa-check"></i><b>7.4</b> Using the psych package</a></li>
<li class="chapter" data-level="7.5" data-path="example-1-to-see-some-properties-of-pca.html"><a href="example-1-to-see-some-properties-of-pca.html#determining-how-many-components-to-retain"><i class="fa fa-check"></i><b>7.5</b> Determining how many components to retain</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="example-1-to-see-some-properties-of-pca.html"><a href="example-1-to-see-some-properties-of-pca.html#kaisers-rule"><i class="fa fa-check"></i><b>7.5.1</b> 1. Kaiser’s rule</a></li>
<li class="chapter" data-level="7.5.2" data-path="example-1-to-see-some-properties-of-pca.html"><a href="example-1-to-see-some-properties-of-pca.html#scree-plot"><i class="fa fa-check"></i><b>7.5.2</b> 2. Scree plot</a></li>
<li class="chapter" data-level="7.5.3" data-path="example-1-to-see-some-properties-of-pca.html"><a href="example-1-to-see-some-properties-of-pca.html#parallel-analysis"><i class="fa fa-check"></i><b>7.5.3</b> 3. Parallel analysis</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="example-1-to-see-some-properties-of-pca.html"><a href="example-1-to-see-some-properties-of-pca.html#reduced-pca-model"><i class="fa fa-check"></i><b>7.6</b> Reduced PCA model</a></li>
<li class="chapter" data-level="7.7" data-path="example-1-to-see-some-properties-of-pca.html"><a href="example-1-to-see-some-properties-of-pca.html#interpretation-1"><i class="fa fa-check"></i><b>7.7</b> Interpretation</a>
<ul>
<li class="chapter" data-level="7.7.1" data-path="example-1-to-see-some-properties-of-pca.html"><a href="example-1-to-see-some-properties-of-pca.html#plot"><i class="fa fa-check"></i><b>7.7.1</b> Plot</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="example-1-to-see-some-properties-of-pca.html"><a href="example-1-to-see-some-properties-of-pca.html#interpretation-of-the-rotated-solution"><i class="fa fa-check"></i><b>7.8</b> Interpretation of the rotated solution</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="example-two.html"><a href="example-two.html"><i class="fa fa-check"></i><b>8</b> Example Two</a>
<ul>
<li class="chapter" data-level="8.1" data-path="example-two.html"><a href="example-two.html#pca-on-unstandardized-scores"><i class="fa fa-check"></i><b>8.1</b> PCA on Unstandardized Scores</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="example-two.html"><a href="example-two.html#hand-calculating-component-scores-from-this-pca"><i class="fa fa-check"></i><b>8.1.1</b> Hand calculating component scores from this PCA</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="example-two.html"><a href="example-two.html#pca-on-the-standardized-scores"><i class="fa fa-check"></i><b>8.2</b> PCA on the Standardized Scores</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="example-two.html"><a href="example-two.html#calculating-component-scores"><i class="fa fa-check"></i><b>8.2.1</b> Calculating component scores</a></li>
<li class="chapter" data-level="8.2.2" data-path="example-two.html"><a href="example-two.html#using-the-rescaled-eigenvectors-to-interpret-the-components"><i class="fa fa-check"></i><b>8.2.2</b> Using the rescaled eigenvectors to interpret the components</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="example-two.html"><a href="example-two.html#let-the-psych-package-do-the-work"><i class="fa fa-check"></i><b>8.3</b> Let the Psych Package Do the Work</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="example-two.html"><a href="example-two.html#view-the-component-scores"><i class="fa fa-check"></i><b>8.3.1</b> View the component scores</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="heptathlon-example.html"><a href="heptathlon-example.html"><i class="fa fa-check"></i><b>9</b> Heptathlon example</a>
<ul>
<li class="chapter" data-level="9.1" data-path="heptathlon-example.html"><a href="heptathlon-example.html#data-and-data-cleaning"><i class="fa fa-check"></i><b>9.1</b> Data and data cleaning</a></li>
<li class="chapter" data-level="9.2" data-path="heptathlon-example.html"><a href="heptathlon-example.html#examine-assumption-of-linearity"><i class="fa fa-check"></i><b>9.2</b> Examine assumption of linearity</a></li>
<li class="chapter" data-level="9.3" data-path="heptathlon-example.html"><a href="heptathlon-example.html#multivariate-normality-and-outliers"><i class="fa fa-check"></i><b>9.3</b> Multivariate normality and outliers</a></li>
<li class="chapter" data-level="9.4" data-path="heptathlon-example.html"><a href="heptathlon-example.html#re-examine-linearity-and-normality"><i class="fa fa-check"></i><b>9.4</b> Re-examine linearity and normality</a></li>
<li class="chapter" data-level="9.5" data-path="heptathlon-example.html"><a href="heptathlon-example.html#fit-the-initial-pca"><i class="fa fa-check"></i><b>9.5</b> Fit the initial PCA</a></li>
<li class="chapter" data-level="9.6" data-path="heptathlon-example.html"><a href="heptathlon-example.html#determine-the-number-of-components-to-retain"><i class="fa fa-check"></i><b>9.6</b> Determine the number of components to retain</a></li>
<li class="chapter" data-level="9.7" data-path="heptathlon-example.html"><a href="heptathlon-example.html#refit-the-pca-model-using-rotation-if-that-is-acceptable-in-your-field"><i class="fa fa-check"></i><b>9.7</b> Refit the PCA model, using rotation if that is acceptable in your field</a></li>
<li class="chapter" data-level="9.8" data-path="heptathlon-example.html"><a href="heptathlon-example.html#interpret-the-components"><i class="fa fa-check"></i><b>9.8</b> Interpret the components</a></li>
<li class="chapter" data-level="9.9" data-path="heptathlon-example.html"><a href="heptathlon-example.html#saving-the-scores"><i class="fa fa-check"></i><b>9.9</b> Saving the scores</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="other-example-from-later-in-the-chapter-not-assigned.html"><a href="other-example-from-later-in-the-chapter-not-assigned.html"><i class="fa fa-check"></i><b>10</b> Other example from later in the chapter (not assigned)</a>
<ul>
<li class="chapter" data-level="10.1" data-path="other-example-from-later-in-the-chapter-not-assigned.html"><a href="other-example-from-later-in-the-chapter-not-assigned.html#usairpollution-data"><i class="fa fa-check"></i><b>10.1</b> USairpollution Data</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="another-resource.html"><a href="another-resource.html"><i class="fa fa-check"></i><b>11</b> Another resource</a></li>
<li class="chapter" data-level="12" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>12</b> References</a></li>
<li class="chapter" data-level="13" data-path="introductory-comments.html"><a href="introductory-comments.html"><i class="fa fa-check"></i><b>13</b> Introductory comments</a>
<ul>
<li class="chapter" data-level="13.1" data-path="introductory-comments.html"><a href="introductory-comments.html#packages"><i class="fa fa-check"></i><b>13.1</b> Packages</a></li>
<li class="chapter" data-level="13.2" data-path="introductory-comments.html"><a href="introductory-comments.html#efa-ne-pca"><i class="fa fa-check"></i><b>13.2</b> EFA <span class="math inline">\(\ne\)</span> PCA</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="preparatory-steps.html"><a href="preparatory-steps.html"><i class="fa fa-check"></i><b>14</b> Preparatory steps</a>
<ul>
<li class="chapter" data-level="14.1" data-path="preparatory-steps.html"><a href="preparatory-steps.html#examining-the-data"><i class="fa fa-check"></i><b>14.1</b> Examining the data</a>
<ul>
<li class="chapter" data-level="14.1.1" data-path="preparatory-steps.html"><a href="preparatory-steps.html#descriptive-statistics-1"><i class="fa fa-check"></i><b>14.1.1</b> Descriptive statistics</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="preparatory-steps.html"><a href="preparatory-steps.html#addressing-assumptions"><i class="fa fa-check"></i><b>14.2</b> Addressing assumptions</a>
<ul>
<li class="chapter" data-level="14.2.1" data-path="preparatory-steps.html"><a href="preparatory-steps.html#linearity"><i class="fa fa-check"></i><b>14.2.1</b> Linearity</a></li>
<li class="chapter" data-level="14.2.2" data-path="preparatory-steps.html"><a href="preparatory-steps.html#outliers-and-multivariate-normality"><i class="fa fa-check"></i><b>14.2.2</b> Outliers and multivariate normality</a></li>
<li class="chapter" data-level="14.2.3" data-path="preparatory-steps.html"><a href="preparatory-steps.html#absence-of-perfect-multicollinearity"><i class="fa fa-check"></i><b>14.2.3</b> Absence of perfect multicollinearity</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="preparatory-steps.html"><a href="preparatory-steps.html#determining-factorability"><i class="fa fa-check"></i><b>14.3</b> Determining factorability</a></li>
<li class="chapter" data-level="14.4" data-path="preparatory-steps.html"><a href="preparatory-steps.html#determining-how-many-factors-to-retain"><i class="fa fa-check"></i><b>14.4</b> Determining how many factors to retain</a>
<ul>
<li class="chapter" data-level="14.4.1" data-path="preparatory-steps.html"><a href="preparatory-steps.html#parallel-analysis-1"><i class="fa fa-check"></i><b>14.4.1</b> Parallel analysis</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="preparatory-steps.html"><a href="preparatory-steps.html#factor-analysis-with-a-data-matrix-raw-data"><i class="fa fa-check"></i><b>14.5</b> Factor analysis with a data matrix (raw data)</a>
<ul>
<li class="chapter" data-level="14.5.1" data-path="preparatory-steps.html"><a href="preparatory-steps.html#determine-how-much-of-the-variance-is-explained-by-the-n-factors"><i class="fa fa-check"></i><b>14.5.1</b> Determine how much of the variance is explained by the <em>n</em> factors</a></li>
<li class="chapter" data-level="14.5.2" data-path="preparatory-steps.html"><a href="preparatory-steps.html#fit-the-efa-with-rotation-for-interpretation"><i class="fa fa-check"></i><b>14.5.2</b> Fit the EFA with rotation, for interpretation</a></li>
<li class="chapter" data-level="14.5.3" data-path="preparatory-steps.html"><a href="preparatory-steps.html#reduced-correlation-matrix"><i class="fa fa-check"></i><b>14.5.3</b> Reduced correlation matrix</a></li>
<li class="chapter" data-level="14.5.4" data-path="preparatory-steps.html"><a href="preparatory-steps.html#examining-pattern-coefficients-the-factor-loadings"><i class="fa fa-check"></i><b>14.5.4</b> Examining pattern coefficients (the factor loadings)</a></li>
</ul></li>
<li class="chapter" data-level="14.6" data-path="preparatory-steps.html"><a href="preparatory-steps.html#factor-analysis-with-correlation-data"><i class="fa fa-check"></i><b>14.6</b> Factor analysis with correlation data</a>
<ul>
<li class="chapter" data-level="14.6.1" data-path="preparatory-steps.html"><a href="preparatory-steps.html#setting-up-the-data"><i class="fa fa-check"></i><b>14.6.1</b> Setting up the data</a></li>
<li class="chapter" data-level="14.6.2" data-path="preparatory-steps.html"><a href="preparatory-steps.html#preparatory-steps-1"><i class="fa fa-check"></i><b>14.6.2</b> Preparatory steps</a></li>
<li class="chapter" data-level="14.6.3" data-path="preparatory-steps.html"><a href="preparatory-steps.html#performing-the-factor-analysis"><i class="fa fa-check"></i><b>14.6.3</b> Performing the factor analysis</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="references-1.html"><a href="references-1.html"><i class="fa fa-check"></i><b>15</b> References</a></li>
<li class="chapter" data-level="16" data-path="factor-analysis-with-correlation-data-1.html"><a href="factor-analysis-with-correlation-data-1.html"><i class="fa fa-check"></i><b>16</b> Factor analysis with correlation data</a>
<ul>
<li class="chapter" data-level="16.1" data-path="factor-analysis-with-correlation-data-1.html"><a href="factor-analysis-with-correlation-data-1.html#setting-up-the-data-1"><i class="fa fa-check"></i><b>16.1</b> Setting up the data</a>
<ul>
<li class="chapter" data-level="16.1.1" data-path="factor-analysis-with-correlation-data-1.html"><a href="factor-analysis-with-correlation-data-1.html#some-extra-stuff-importing-half-the-correlation-matrix"><i class="fa fa-check"></i><b>16.1.1</b> Some extra stuff: Importing half the correlation matrix</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="factor-analysis-with-correlation-data-1.html"><a href="factor-analysis-with-correlation-data-1.html#preparatory-steps-2"><i class="fa fa-check"></i><b>16.2</b> Preparatory steps</a>
<ul>
<li class="chapter" data-level="16.2.1" data-path="factor-analysis-with-correlation-data-1.html"><a href="factor-analysis-with-correlation-data-1.html#factorability"><i class="fa fa-check"></i><b>16.2.1</b> Factorability</a></li>
<li class="chapter" data-level="16.2.2" data-path="factor-analysis-with-correlation-data-1.html"><a href="factor-analysis-with-correlation-data-1.html#number-of-factors"><i class="fa fa-check"></i><b>16.2.2</b> Number of factors</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="factor-analysis-with-correlation-data-1.html"><a href="factor-analysis-with-correlation-data-1.html#performing-the-factor-analysis-1"><i class="fa fa-check"></i><b>16.3</b> Performing the factor analysis</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="efa-with-categorical-data.html"><a href="efa-with-categorical-data.html"><i class="fa fa-check"></i><b>17</b> EFA with categorical data</a>
<ul>
<li class="chapter" data-level="17.1" data-path="efa-with-categorical-data.html"><a href="efa-with-categorical-data.html#describing-categorical-data"><i class="fa fa-check"></i><b>17.1</b> Describing categorical data</a>
<ul>
<li class="chapter" data-level="17.1.1" data-path="efa-with-categorical-data.html"><a href="efa-with-categorical-data.html#plotting-categorical-data"><i class="fa fa-check"></i><b>17.1.1</b> Plotting categorical data</a></li>
<li class="chapter" data-level="17.1.2" data-path="efa-with-categorical-data.html"><a href="efa-with-categorical-data.html#specifying-the-variable-type-as-ordinal"><i class="fa fa-check"></i><b>17.1.2</b> Specifying the variable type as ordinal</a></li>
<li class="chapter" data-level="17.1.3" data-path="efa-with-categorical-data.html"><a href="efa-with-categorical-data.html#getting-the-correlation-matrix"><i class="fa fa-check"></i><b>17.1.3</b> Getting the correlation matrix</a></li>
<li class="chapter" data-level="17.1.4" data-path="efa-with-categorical-data.html"><a href="efa-with-categorical-data.html#appraising-the-factorability-and-determining-the-number-of-factors-to-retain"><i class="fa fa-check"></i><b>17.1.4</b> Appraising the factorability and determining the number of factors to retain</a></li>
<li class="chapter" data-level="17.1.5" data-path="efa-with-categorical-data.html"><a href="efa-with-categorical-data.html#factor-analysis-on-the-polychoric-matrix"><i class="fa fa-check"></i><b>17.1.5</b> Factor analysis on the polychoric matrix</a></li>
<li class="chapter" data-level="17.1.6" data-path="efa-with-categorical-data.html"><a href="efa-with-categorical-data.html#the-polycor-package-if-needed"><i class="fa fa-check"></i><b>17.1.6</b> The polycor package, if needed</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="efa-with-categorical-data.html"><a href="efa-with-categorical-data.html#factor-scores"><i class="fa fa-check"></i><b>17.2</b> Factor scores</a>
<ul>
<li class="chapter" data-level="17.2.1" data-path="efa-with-categorical-data.html"><a href="efa-with-categorical-data.html#which-factor-scores-to-use"><i class="fa fa-check"></i><b>17.2.1</b> Which factor scores to use?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="practice.html"><a href="practice.html"><i class="fa fa-check"></i><b>18</b> Practice</a>
<ul>
<li class="chapter" data-level="18.1" data-path="practice.html"><a href="practice.html#summary-2"><i class="fa fa-check"></i><b>18.1</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="references-2.html"><a href="references-2.html"><i class="fa fa-check"></i><b>19</b> References</a></li>
<li class="chapter" data-level="20" data-path="preparatory-steps-3.html"><a href="preparatory-steps-3.html"><i class="fa fa-check"></i><b>20</b> Preparatory steps</a>
<ul>
<li class="chapter" data-level="20.1" data-path="preparatory-steps-3.html"><a href="preparatory-steps-3.html#describing-the-data"><i class="fa fa-check"></i><b>20.1</b> Describing the data</a>
<ul>
<li class="chapter" data-level="20.1.1" data-path="preparatory-steps-3.html"><a href="preparatory-steps-3.html#an-article-reported-r-and-the-sds-so-how-can-i-get-s"><i class="fa fa-check"></i><b>20.1.1</b> An article reported R and the SDs, so how can I get S?</a></li>
<li class="chapter" data-level="20.1.2" data-path="preparatory-steps-3.html"><a href="preparatory-steps-3.html#describing-from-a-categorical-perspective"><i class="fa fa-check"></i><b>20.1.2</b> Describing from a categorical perspective</a></li>
<li class="chapter" data-level="20.1.3" data-path="preparatory-steps-3.html"><a href="preparatory-steps-3.html#addressing-assumptions-1"><i class="fa fa-check"></i><b>20.1.3</b> Addressing assumptions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="21" data-path="fitting-a-cfa-model.html"><a href="fitting-a-cfa-model.html"><i class="fa fa-check"></i><b>21</b> Fitting a CFA model</a>
<ul>
<li class="chapter" data-level="21.1" data-path="fitting-a-cfa-model.html"><a href="fitting-a-cfa-model.html#making-sure-the-model-was-correctly-estimated"><i class="fa fa-check"></i><b>21.1</b> Making sure the model was correctly estimated</a></li>
<li class="chapter" data-level="21.2" data-path="fitting-a-cfa-model.html"><a href="fitting-a-cfa-model.html#evaluating-the-model-fit"><i class="fa fa-check"></i><b>21.2</b> Evaluating the model fit</a></li>
<li class="chapter" data-level="21.3" data-path="fitting-a-cfa-model.html"><a href="fitting-a-cfa-model.html#interpretting-the-parameter-estimates"><i class="fa fa-check"></i><b>21.3</b> Interpretting the parameter estimates</a></li>
<li class="chapter" data-level="21.4" data-path="fitting-a-cfa-model.html"><a href="fitting-a-cfa-model.html#further-inspecting-our-model"><i class="fa fa-check"></i><b>21.4</b> Further inspecting our model</a>
<ul>
<li class="chapter" data-level="21.4.1" data-path="fitting-a-cfa-model.html"><a href="fitting-a-cfa-model.html#looking-at-the-models-covariance-matrices"><i class="fa fa-check"></i><b>21.4.1</b> Looking at the model’s covariance matrices</a></li>
</ul></li>
<li class="chapter" data-level="21.5" data-path="fitting-a-cfa-model.html"><a href="fitting-a-cfa-model.html#further-inspecting-our-models-parameters"><i class="fa fa-check"></i><b>21.5</b> Further inspecting our model’s parameters</a>
<ul>
<li class="chapter" data-level="21.5.1" data-path="fitting-a-cfa-model.html"><a href="fitting-a-cfa-model.html#seeing-which-parameters-were-estimated"><i class="fa fa-check"></i><b>21.5.1</b> Seeing which parameters were estimated</a></li>
<li class="chapter" data-level="21.5.2" data-path="fitting-a-cfa-model.html"><a href="fitting-a-cfa-model.html#optional-saving-parameter-estimates-to-a-data-frame"><i class="fa fa-check"></i><b>21.5.2</b> Optional: Saving parameter estimates to a data frame</a></li>
<li class="chapter" data-level="21.5.3" data-path="fitting-a-cfa-model.html"><a href="fitting-a-cfa-model.html#generating-path-diagrams"><i class="fa fa-check"></i><b>21.5.3</b> Generating path diagrams</a></li>
<li class="chapter" data-level="21.5.4" data-path="fitting-a-cfa-model.html"><a href="fitting-a-cfa-model.html#examining-modification-indices"><i class="fa fa-check"></i><b>21.5.4</b> Examining modification indices</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="22" data-path="using-alternative-model-specifications.html"><a href="using-alternative-model-specifications.html"><i class="fa fa-check"></i><b>22</b> Using alternative model specifications</a>
<ul>
<li class="chapter" data-level="22.1" data-path="using-alternative-model-specifications.html"><a href="using-alternative-model-specifications.html#rescaling-the-factors-to-be-standardized"><i class="fa fa-check"></i><b>22.1</b> Rescaling the factors to be standardized</a></li>
<li class="chapter" data-level="22.2" data-path="using-alternative-model-specifications.html"><a href="using-alternative-model-specifications.html#specifying-a-competing-nested-model"><i class="fa fa-check"></i><b>22.2</b> Specifying a competing, nested, model</a></li>
<li class="chapter" data-level="22.3" data-path="using-alternative-model-specifications.html"><a href="using-alternative-model-specifications.html#conducting-model-comparisons"><i class="fa fa-check"></i><b>22.3</b> Conducting model comparisons</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="estimating-factor-scores.html"><a href="estimating-factor-scores.html"><i class="fa fa-check"></i><b>23</b> Estimating factor scores</a></li>
<li class="chapter" data-level="24" data-path="cfa-with-categorical-data.html"><a href="cfa-with-categorical-data.html"><i class="fa fa-check"></i><b>24</b> CFA with categorical data</a>
<ul>
<li class="chapter" data-level="24.1" data-path="cfa-with-categorical-data.html"><a href="cfa-with-categorical-data.html#using-data-from-an-r-package"><i class="fa fa-check"></i><b>24.1</b> Using data from an R package</a></li>
<li class="chapter" data-level="24.2" data-path="cfa-with-categorical-data.html"><a href="cfa-with-categorical-data.html#preparing-the-data"><i class="fa fa-check"></i><b>24.2</b> Preparing the data</a></li>
<li class="chapter" data-level="24.3" data-path="cfa-with-categorical-data.html"><a href="cfa-with-categorical-data.html#describing-the-data-1"><i class="fa fa-check"></i><b>24.3</b> Describing the data</a></li>
<li class="chapter" data-level="24.4" data-path="cfa-with-categorical-data.html"><a href="cfa-with-categorical-data.html#fitting-the-cfa-model"><i class="fa fa-check"></i><b>24.4</b> Fitting the CFA model</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="way-tldr-summary.html"><a href="way-tldr-summary.html"><i class="fa fa-check"></i><b>25</b> Way TL;DR Summary</a></li>
<li class="chapter" data-level="26" data-path="references-3.html"><a href="references-3.html"><i class="fa fa-check"></i><b>26</b> References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">EDUC 644 Applied Multivariate Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="example-1-to-see-some-properties-of-pca" class="section level1 hasAnchor" number="7">
<h1><span class="header-section-number">7</span> Example 1, to see some properties of PCA<a href="example-1-to-see-some-properties-of-pca.html#example-1-to-see-some-properties-of-pca" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Here is the correlation matrix from the chapter. (I simply copied and pasted this and reformatted it, so there might be rounding error discrepancies with what is reported in the chapter.) The <em>N</em>-size is 72 but we do not have individual cases’ data.</p>
<div class="sourceCode" id="cb307"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb307-1"><a href="example-1-to-see-some-properties-of-pca.html#cb307-1" tabindex="-1"></a>varbnames <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;rblood&quot;</span>, <span class="st">&quot;plate&quot;</span>, <span class="st">&quot;wblood&quot;</span>, <span class="st">&quot;neut&quot;</span>, <span class="st">&quot;lymph&quot;</span>, <span class="st">&quot;bilir&quot;</span>, <span class="st">&quot;sodium&quot;</span>, <span class="st">&quot;potass&quot;</span>)</span>
<span id="cb307-2"><a href="example-1-to-see-some-properties-of-pca.html#cb307-2" tabindex="-1"></a></span>
<span id="cb307-3"><a href="example-1-to-see-some-properties-of-pca.html#cb307-3" tabindex="-1"></a>blood_corr <span class="ot">&lt;-</span> <span class="fu">matrix</span>(</span>
<span id="cb307-4"><a href="example-1-to-see-some-properties-of-pca.html#cb307-4" tabindex="-1"></a>  <span class="fu">c</span>(    <span class="dv">1</span>,   <span class="fl">0.29</span>,   <span class="fl">0.202</span>, <span class="sc">-</span><span class="fl">0.055</span>, <span class="sc">-</span><span class="fl">0.105</span>, <span class="sc">-</span><span class="fl">0.252</span>, <span class="sc">-</span><span class="fl">0.229</span>,  <span class="fl">0.058</span>,</span>
<span id="cb307-5"><a href="example-1-to-see-some-properties-of-pca.html#cb307-5" tabindex="-1"></a>    <span class="fl">0.290</span>,      <span class="dv">1</span>,   <span class="fl">0.415</span>,  <span class="fl">0.285</span>, <span class="sc">-</span><span class="fl">0.376</span>, <span class="sc">-</span><span class="fl">0.349</span>, <span class="sc">-</span><span class="fl">0.164</span>, <span class="sc">-</span><span class="fl">0.129</span>,</span>
<span id="cb307-6"><a href="example-1-to-see-some-properties-of-pca.html#cb307-6" tabindex="-1"></a>    <span class="fl">0.202</span>,  <span class="fl">0.415</span>,       <span class="dv">1</span>,  <span class="fl">0.419</span>, <span class="sc">-</span><span class="fl">0.521</span>, <span class="sc">-</span><span class="fl">0.441</span>, <span class="sc">-</span><span class="fl">0.145</span>, <span class="sc">-</span><span class="fl">0.076</span>,</span>
<span id="cb307-7"><a href="example-1-to-see-some-properties-of-pca.html#cb307-7" tabindex="-1"></a>   <span class="sc">-</span><span class="fl">0.055</span>,  <span class="fl">0.285</span>,   <span class="fl">0.419</span>,      <span class="dv">1</span>, <span class="sc">-</span><span class="fl">0.877</span>, <span class="sc">-</span><span class="fl">0.076</span>,  <span class="fl">0.023</span>, <span class="sc">-</span><span class="fl">0.131</span>,</span>
<span id="cb307-8"><a href="example-1-to-see-some-properties-of-pca.html#cb307-8" tabindex="-1"></a>   <span class="sc">-</span><span class="fl">0.105</span>, <span class="sc">-</span><span class="fl">0.376</span>,  <span class="sc">-</span><span class="fl">0.521</span>, <span class="sc">-</span><span class="fl">0.877</span>,      <span class="dv">1</span>,  <span class="fl">0.206</span>,  <span class="fl">0.034</span>,  <span class="fl">0.151</span>,</span>
<span id="cb307-9"><a href="example-1-to-see-some-properties-of-pca.html#cb307-9" tabindex="-1"></a>   <span class="sc">-</span><span class="fl">0.252</span>, <span class="sc">-</span><span class="fl">0.349</span>,  <span class="sc">-</span><span class="fl">0.441</span>, <span class="sc">-</span><span class="fl">0.076</span>,  <span class="fl">0.206</span>,      <span class="dv">1</span>,  <span class="fl">0.192</span>,  <span class="fl">0.077</span>,</span>
<span id="cb307-10"><a href="example-1-to-see-some-properties-of-pca.html#cb307-10" tabindex="-1"></a>   <span class="sc">-</span><span class="fl">0.229</span>, <span class="sc">-</span><span class="fl">0.164</span>,  <span class="sc">-</span><span class="fl">0.145</span>,  <span class="fl">0.023</span>,  <span class="fl">0.034</span>,  <span class="fl">0.192</span>,      <span class="dv">1</span>,  <span class="fl">0.423</span>,</span>
<span id="cb307-11"><a href="example-1-to-see-some-properties-of-pca.html#cb307-11" tabindex="-1"></a>    <span class="fl">0.058</span>, <span class="sc">-</span><span class="fl">0.129</span>,  <span class="sc">-</span><span class="fl">0.076</span>, <span class="sc">-</span><span class="fl">0.131</span>,  <span class="fl">0.151</span>,  <span class="fl">0.077</span>,  <span class="fl">0.423</span>,      <span class="dv">1</span>),</span>
<span id="cb307-12"><a href="example-1-to-see-some-properties-of-pca.html#cb307-12" tabindex="-1"></a>  <span class="at">ncol =</span> <span class="dv">8</span>, <span class="at">byrow =</span> <span class="cn">TRUE</span>,</span>
<span id="cb307-13"><a href="example-1-to-see-some-properties-of-pca.html#cb307-13" tabindex="-1"></a>  <span class="at">dimnames =</span>  <span class="fu">list</span>(varbnames, varbnames) )</span>
<span id="cb307-14"><a href="example-1-to-see-some-properties-of-pca.html#cb307-14" tabindex="-1"></a></span>
<span id="cb307-15"><a href="example-1-to-see-some-properties-of-pca.html#cb307-15" tabindex="-1"></a>blood_sd <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.371</span>,    <span class="fl">41.253</span>, <span class="fl">1.935</span>,  <span class="fl">0.077</span>,  <span class="fl">0.071</span>,  <span class="fl">4.037</span>,  <span class="fl">2.732</span>,  <span class="fl">0.297</span>)</span>
<span id="cb307-16"><a href="example-1-to-see-some-properties-of-pca.html#cb307-16" tabindex="-1"></a><span class="fu">names</span>(blood_sd) <span class="ot">&lt;-</span> varbnames</span></code></pre></div>
<p>Let’s print the correlation matrix to be sure it is what we expect:</p>
<div class="sourceCode" id="cb308"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb308-1"><a href="example-1-to-see-some-properties-of-pca.html#cb308-1" tabindex="-1"></a>blood_corr</span></code></pre></div>
<pre><code>##        rblood  plate wblood   neut  lymph  bilir sodium potass
## rblood  1.000  0.290  0.202 -0.055 -0.105 -0.252 -0.229  0.058
## plate   0.290  1.000  0.415  0.285 -0.376 -0.349 -0.164 -0.129
## wblood  0.202  0.415  1.000  0.419 -0.521 -0.441 -0.145 -0.076
## neut   -0.055  0.285  0.419  1.000 -0.877 -0.076  0.023 -0.131
## lymph  -0.105 -0.376 -0.521 -0.877  1.000  0.206  0.034  0.151
## bilir  -0.252 -0.349 -0.441 -0.076  0.206  1.000  0.192  0.077
## sodium -0.229 -0.164 -0.145  0.023  0.034  0.192  1.000  0.423
## potass  0.058 -0.129 -0.076 -0.131  0.151  0.077  0.423  1.000</code></pre>
<p>Here is the vector of standard deviations, which are needed to calculate the covariance matrix, which we only look at for comparison purposes:</p>
<div class="sourceCode" id="cb310"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb310-1"><a href="example-1-to-see-some-properties-of-pca.html#cb310-1" tabindex="-1"></a>blood_sd</span></code></pre></div>
<pre><code>## rblood  plate wblood   neut  lymph  bilir sodium potass 
##  0.371 41.253  1.935  0.077  0.071  4.037  2.732  0.297</code></pre>
<p>Given the standard deviations and the correlation matrix, we can get the covariance matrix using some matrix algebra. Recall that to get from covariance to correlation, we divide each cell by the product of the two standard-deviations corresponding to the row and column. Here, we’re going in the opposite direction, so we’re multiplying each cell by the product of the row and column standard deviations. <a href="#fn29" class="footnote-ref" id="fnref29"><sup>29</sup></a> Everitt and Hothorn are using the covariance matrix for demonstration of the difference between covariance and correlation—and of what not to do—so we normally wouldn’t care about this if we have the correlation matrix.</p>
<div class="sourceCode" id="cb312"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb312-1"><a href="example-1-to-see-some-properties-of-pca.html#cb312-1" tabindex="-1"></a>blood_cov <span class="ot">&lt;-</span> <span class="fu">diag</span>(blood_sd) <span class="sc">%*%</span> blood_corr <span class="sc">%*%</span> <span class="fu">diag</span>(blood_sd)</span>
<span id="cb312-2"><a href="example-1-to-see-some-properties-of-pca.html#cb312-2" tabindex="-1"></a><span class="fu">round</span>(blood_cov, <span class="dv">2</span> )</span></code></pre></div>
<pre><code>##       [,1]    [,2]  [,3]  [,4]  [,5]   [,6]   [,7]  [,8]
## [1,]  0.14    4.44  0.15  0.00  0.00  -0.38  -0.23  0.01
## [2,]  4.44 1701.81 33.13  0.91 -1.10 -58.12 -18.48 -1.58
## [3,]  0.15   33.13  3.74  0.06 -0.07  -3.44  -0.77 -0.04
## [4,]  0.00    0.91  0.06  0.01  0.00  -0.02   0.00  0.00
## [5,]  0.00   -1.10 -0.07  0.00  0.01   0.06   0.01  0.00
## [6,] -0.38  -58.12 -3.44 -0.02  0.06  16.30   2.12  0.09
## [7,] -0.23  -18.48 -0.77  0.00  0.01   2.12   7.46  0.34
## [8,]  0.01   -1.58 -0.04  0.00  0.00   0.09   0.34  0.09</code></pre>
<div id="pca-with-covariance-vs.-correlation" class="section level2 hasAnchor" number="7.1">
<h2><span class="header-section-number">7.1</span> PCA with Covariance vs. Correlation<a href="example-1-to-see-some-properties-of-pca.html#pca-with-covariance-vs.-correlation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We can use a PCA on a covariance matrix, correlation matrix, or the raw data. The correlation matrix is better than the covariance matrix if the observed variables are not on the same scale (which is most of the time). Conducting a PCA on the raw data is useful if we want to save each case’s composite scores and use them later on, say, in a regression or a plot.</p>
<p>Here is the PCA on the <strong>covariance matrix</strong>, which is on the original scale of the variables. The results of this are reported on Page 67 of the chapter.</p>
<div class="sourceCode" id="cb314"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb314-1"><a href="example-1-to-see-some-properties-of-pca.html#cb314-1" tabindex="-1"></a>blood_pcacov <span class="ot">&lt;-</span> <span class="fu">princomp</span>(<span class="at">covmat =</span> blood_cov)</span>
<span id="cb314-2"><a href="example-1-to-see-some-properties-of-pca.html#cb314-2" tabindex="-1"></a><span class="fu">summary</span>(blood_pcacov, <span class="at">loadings =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<pre><code>## Importance of components:
##                            Comp.1      Comp.2     Comp.3      Comp.4      Comp.5       Comp.6
## Standard deviation     41.2877486 3.880212624 2.64197339 1.624583979 0.353951757 2.561722e-01
## Proportion of Variance  0.9856182 0.008705172 0.00403574 0.001525986 0.000072436 3.794288e-05
## Cumulative Proportion   0.9856182 0.994323381 0.99835912 0.999885108 0.999957544 9.999955e-01
##                              Comp.7       Comp.8
## Standard deviation     8.510631e-02 2.372715e-02
## Proportion of Variance 4.187837e-06 3.255049e-07
## Cumulative Proportion  9.999997e-01 1.000000e+00
## 
## Loadings:
##      Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8
## [1,]                              0.943  0.329              
## [2,]  0.999                                                 
## [3,]         0.192         0.981                            
## [4,]                                            0.758  0.650
## [5,]                                           -0.649  0.760
## [6,]        -0.961  0.195  0.191                            
## [7,]        -0.193 -0.979                                   
## [8,]                              0.329 -0.942</code></pre>
<p>In this output under the label <code>Loadings:</code>, the eigenvector coefficients with very low absolute values (less than 0.10, it seems) are left blank in the printout. These coefficients actually do exist, and we can see them if we ask for them using <code>blood_pcacov$loadings[1:64]</code>, where 64 is the number of cells in the eigenvector matrix (8 eigenvectors across 8 items).</p>
<p>What do you notice about the first component? How much of the total variance does it explain? Which observed variables have the largest coefficients? (Refer back to the standard deviations of the raw data and look at which variable seems to have a much larger scale, as reflected by the standard deviations.)</p>
<p>And, for comparison, here is the PCA on the <strong>correlation matrix</strong>, which is the preferred method.</p>
<div class="sourceCode" id="cb316"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb316-1"><a href="example-1-to-see-some-properties-of-pca.html#cb316-1" tabindex="-1"></a>blood_pcacor <span class="ot">&lt;-</span> <span class="fu">princomp</span>(<span class="at">covmat =</span> blood_corr)</span>
<span id="cb316-2"><a href="example-1-to-see-some-properties-of-pca.html#cb316-2" tabindex="-1"></a><span class="fu">summary</span>(blood_pcacor, <span class="at">loadings =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<pre><code>## Importance of components:
##                           Comp.1    Comp.2    Comp.3     Comp.4     Comp.5     Comp.6     Comp.7
## Standard deviation     1.6710100 1.2375848 1.1177138 0.88227419 0.78839505 0.69917350 0.66002394
## Proportion of Variance 0.3490343 0.1914520 0.1561605 0.09730097 0.07769584 0.06110545 0.05445395
## Cumulative Proportion  0.3490343 0.5404863 0.6966468 0.79394778 0.87164363 0.93274908 0.98720303
##                            Comp.8
## Standard deviation     0.31996216
## Proportion of Variance 0.01279697
## Cumulative Proportion  1.00000000
## 
## Loadings:
##        Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8
## rblood  0.194  0.417  0.400  0.652  0.175  0.363  0.176  0.102
## plate   0.400  0.154  0.168        -0.848 -0.230 -0.110       
## wblood  0.459         0.168 -0.274  0.251 -0.403  0.677       
## neut    0.430 -0.472 -0.171  0.169  0.118        -0.237  0.678
## lymph  -0.494  0.360        -0.180 -0.139 -0.136  0.157  0.724
## bilir  -0.319 -0.320 -0.277  0.633 -0.162 -0.384  0.377       
## sodium -0.177 -0.535  0.410 -0.163 -0.299  0.513  0.367       
## potass -0.171 -0.245  0.709         0.198 -0.469 -0.376</code></pre>
<p>Notice how the correlation matrix has a very different proportion of variance being explained by the first component and that several of the observed variables are contributing to that component. This is very different from what we observed with the covariance matrix. In a correlation matrix, all of the variables are treated equally, being placed on the same scale that is bound between -1 and +1, which in turn provides a more accurate estimate of the components.</p>
</div>
<div id="unscaled-eigenvectors" class="section level2 hasAnchor" number="7.2">
<h2><span class="header-section-number">7.2</span> Unscaled eigenvectors<a href="example-1-to-see-some-properties-of-pca.html#unscaled-eigenvectors" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Note that what the <code>princomp()</code> function labels as “loadings” are actually the <strong><em>unscaled</em> eigenvectors</strong>. To get the <strong><em>rescaled</em> eigenvectors</strong>, which are more appropriate for interpreting the components, and which are what analysts from a factor-analysis tradition call <strong>loadings</strong>, we use the equation at the bottom of Page 70 in the chapter:</p>
<p><span class="math display">\[\mathbf{a}^*_i = \mathbf{a}_i\sqrt{\lambda}_i \]</span>
where <span class="math inline">\(i\)</span> refers to the component number—in this example we have eight possible components. Following Everitt and Hothorn’s labeling system, <span class="math inline">\(\mathbf{a}_i\)</span> represents the eigenvector for component <span class="math inline">\(i\)</span>, and the rescaled eigenvector has an asterisk. Also, the lowercase lambda, <span class="math inline">\(\lambda_i\)</span>, represents the eigenvalue of component <span class="math inline">\(i\)</span>. If we stuck these eight columns of vectors side-by-side in a matrix, we have a matrix of eigenvectors, <span class="math inline">\(\mathbf{A}\)</span> and <span class="math inline">\(\mathbf{A^*}\)</span>, for the unscaled and scaled eigenvectors respectively (described on pp. 70–71 in the chapter).</p>
<p>Here are the eight eigenvectors, as they’re reported in the <code>princomp()</code> function under each column, labeled by the component. (Again, these are <strong>not</strong> loadings—they’re unscaled eigenvectors.)</p>
<div class="sourceCode" id="cb318"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb318-1"><a href="example-1-to-see-some-properties-of-pca.html#cb318-1" tabindex="-1"></a>blood_pcacor<span class="sc">$</span>loadings</span></code></pre></div>
<pre><code>## 
## Loadings:
##        Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8
## rblood  0.194  0.417  0.400  0.652  0.175  0.363  0.176  0.102
## plate   0.400  0.154  0.168        -0.848 -0.230 -0.110       
## wblood  0.459         0.168 -0.274  0.251 -0.403  0.677       
## neut    0.430 -0.472 -0.171  0.169  0.118        -0.237  0.678
## lymph  -0.494  0.360        -0.180 -0.139 -0.136  0.157  0.724
## bilir  -0.319 -0.320 -0.277  0.633 -0.162 -0.384  0.377       
## sodium -0.177 -0.535  0.410 -0.163 -0.299  0.513  0.367       
## potass -0.171 -0.245  0.709         0.198 -0.469 -0.376       
## 
##                Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8
## SS loadings     1.000  1.000  1.000  1.000  1.000  1.000  1.000  1.000
## Proportion Var  0.125  0.125  0.125  0.125  0.125  0.125  0.125  0.125
## Cumulative Var  0.125  0.250  0.375  0.500  0.625  0.750  0.875  1.000</code></pre>
<p>The eigenvalues are available from the output, as well. The <code>$sdev</code> includes the standard deviations of each component (based on the <strong>unscaled</strong> eigenvector coefficients). If we square those, we have the component’s variance, or eigenvalue. There are eight eigenvalues—one for each component and these represent the repackaged variances when we create these components from the variables.</p>
<div class="sourceCode" id="cb320"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb320-1"><a href="example-1-to-see-some-properties-of-pca.html#cb320-1" tabindex="-1"></a>blood_pcacor<span class="sc">$</span>sdev<span class="sc">^</span><span class="dv">2</span></span></code></pre></div>
<pre><code>##    Comp.1    Comp.2    Comp.3    Comp.4    Comp.5    Comp.6    Comp.7    Comp.8 
## 2.7922743 1.5316161 1.2492841 0.7784077 0.6215668 0.4888436 0.4356316 0.1023758</code></pre>
<p>The components with higher eigenvalues explain more variability in the data. The first component, (which is the “principal” component that Pearson originally looked at), explains most of the variability in our data set.</p>
</div>
<div id="looking-at-the-properties-of-eigenvectors-and-eigenvalues" class="section level2 hasAnchor" number="7.3">
<h2><span class="header-section-number">7.3</span> Looking at the properties of eigenvectors and eigenvalues<a href="example-1-to-see-some-properties-of-pca.html#looking-at-the-properties-of-eigenvectors-and-eigenvalues" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Our handy R calculator has done the work in obtaining the eigenvectors and eigenvalues for us. The eigenvectors are the weights, analogous to regression coefficients; the eigenvalues represent the variance of the component, with higher values representing a stronger component. To obtain the eigenvectors, two constraints had to be in place:</p>
<ol style="list-style-type: decimal">
<li>The sum of the squared coefficients is one, <span class="math inline">\(\mathbf{a}_i\prime\mathbf{a}_i = 1\)</span>.</li>
<li>The cross-products of each eigenvector is zero, <span class="math inline">\(\mathbf{a}_i\prime\mathbf{a}_j = 0 \text{, where } j &gt; i\)</span>.</li>
</ol>
<p>Let’s check this out:</p>
<p>Here’s our first eigenvector:</p>
<div class="sourceCode" id="cb322"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb322-1"><a href="example-1-to-see-some-properties-of-pca.html#cb322-1" tabindex="-1"></a>a1 <span class="ot">&lt;-</span> blood_pcacor<span class="sc">$</span>loadings[, <span class="dv">1</span>]</span>
<span id="cb322-2"><a href="example-1-to-see-some-properties-of-pca.html#cb322-2" tabindex="-1"></a>a1</span></code></pre></div>
<pre><code>##     rblood      plate     wblood       neut      lymph      bilir     sodium     potass 
##  0.1942203  0.4003625  0.4588793  0.4303359 -0.4937748 -0.3194549 -0.1768857 -0.1705160</code></pre>
<p>Does <span class="math inline">\(\mathbf{a}_1\prime\mathbf{a}_1 = 1\)</span>?</p>
<p>In other words, does</p>
<p><span class="math display">\[\begin{bmatrix}
  0.19 &amp;  0.40  &amp; 0.46 &amp;  0.43 &amp; -0.49 &amp; -0.32 &amp; -0.18 &amp; -0.17
\end{bmatrix}
\begin{bmatrix}
  0.19 \\  0.40  \\ 0.46 \\  0.43 \\ -0.49 \\ -0.32 \\ -0.18 \\ -0.17
\end{bmatrix} = 1 \text{ ?}\]</span></p>
<div class="sourceCode" id="cb324"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb324-1"><a href="example-1-to-see-some-properties-of-pca.html#cb324-1" tabindex="-1"></a><span class="fu">t</span>(a1) <span class="sc">%*%</span> a1</span></code></pre></div>
<pre><code>##      [,1]
## [1,]    1</code></pre>
<p>It does.</p>
<p>Here’s our second eigenvector:</p>
<div class="sourceCode" id="cb326"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb326-1"><a href="example-1-to-see-some-properties-of-pca.html#cb326-1" tabindex="-1"></a>a2 <span class="ot">&lt;-</span> blood_pcacor<span class="sc">$</span>loadings[, <span class="dv">2</span>]</span>
<span id="cb326-2"><a href="example-1-to-see-some-properties-of-pca.html#cb326-2" tabindex="-1"></a>a2</span></code></pre></div>
<pre><code>##        rblood         plate        wblood          neut         lymph         bilir        sodium 
##  0.4171230843  0.1539289974 -0.0002984974 -0.4724424229  0.3604497505 -0.3201664742 -0.5352734751 
##        potass 
## -0.2452834631</code></pre>
<p>Does <span class="math inline">\(\mathbf{a}_2\prime\mathbf{a}_2 = 1\)</span>?</p>
<div class="sourceCode" id="cb328"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb328-1"><a href="example-1-to-see-some-properties-of-pca.html#cb328-1" tabindex="-1"></a><span class="fu">t</span>(a2) <span class="sc">%*%</span> a2</span></code></pre></div>
<pre><code>##      [,1]
## [1,]    1</code></pre>
<p>It does. This also sums to 1.</p>
<p>Okay, how about their cross-products: Does</p>
<p><span class="math display">\[\begin{bmatrix}
  0.19 &amp;  0.40  &amp; 0.46 &amp;  0.43 &amp; -0.49 &amp; -0.32 &amp; -0.18 &amp; -0.17
\end{bmatrix}
\begin{bmatrix}
  0.42 \\  0.15 \\   0.00 \\  -0.47  \\  0.36 \\  -0.32 \\  -0.54 \\  -0.25
\end{bmatrix} = 0 \text{ ?}\]</span></p>
<div class="sourceCode" id="cb330"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb330-1"><a href="example-1-to-see-some-properties-of-pca.html#cb330-1" tabindex="-1"></a><span class="fu">round</span>( <span class="fu">t</span>(a1) <span class="sc">%*%</span> a2 , <span class="dv">3</span>)</span></code></pre></div>
<pre><code>##      [,1]
## [1,]    0</code></pre>
<p>Indeed it does.</p>
<p>This latter condition ensures that the components are orthogonal—that is, unrelated.</p>
<p>We only checked the first two components, but we would find the same results with the rest.</p>
<div id="eigenvalues-add-up-to-total-observed-variance" class="section level3 hasAnchor" number="7.3.1">
<h3><span class="header-section-number">7.3.1</span> Eigenvalues add up to total observed variance<a href="example-1-to-see-some-properties-of-pca.html#eigenvalues-add-up-to-total-observed-variance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We can also see that the sum of the eigenvalues (<span class="math inline">\(\lambda_1\)</span> through <span class="math inline">\(\lambda_8\)</span>) equals the sum of the variances of the observed variables. In other words, does
<span class="math display">\[\sum{\lambda_i = s^2_1 + s^2_2 + \cdots + s^2_p} \text{ ?}\]</span>
In our case, we fit the PCA to the data’s correlation matrix, which includes <code>1</code> on the diagonal, meaning that the variables are scaled to have a variance (and standard deviation) of <code>1</code>. With that, the sum of our eigenvalues (before we reduce the number of components) should be the number of variables.</p>
<p>So we can ask whether the sum of the eigenvalues is 8, given that we have eight variables.</p>
<div class="sourceCode" id="cb332"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb332-1"><a href="example-1-to-see-some-properties-of-pca.html#cb332-1" tabindex="-1"></a><span class="fu">sum</span>(blood_pcacor<span class="sc">$</span>sdev<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 8</code></pre>
<p>This demonstrates that the amount of variance is the same as it is in our original data, just repackaged.</p>
</div>
<div id="each-components-proportion-of-variance" class="section level3 hasAnchor" number="7.3.2">
<h3><span class="header-section-number">7.3.2</span> Each component’s proportion of variance<a href="example-1-to-see-some-properties-of-pca.html#each-components-proportion-of-variance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>What is the proportion of variance that each component explains? Given that we know the total variance, and each eigenvalue, we can use this code:</p>
<div class="sourceCode" id="cb334"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb334-1"><a href="example-1-to-see-some-properties-of-pca.html#cb334-1" tabindex="-1"></a>props <span class="ot">&lt;-</span> blood_pcacor<span class="sc">$</span>sdev<span class="sc">^</span><span class="dv">2</span> <span class="sc">/</span> <span class="fu">sum</span>(blood_pcacor<span class="sc">$</span>sdev<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb334-2"><a href="example-1-to-see-some-properties-of-pca.html#cb334-2" tabindex="-1"></a><span class="fu">round</span>(props, <span class="dv">3</span>)</span></code></pre></div>
<pre><code>## Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8 
##  0.349  0.191  0.156  0.097  0.078  0.061  0.054  0.013</code></pre>
<p>If we look up above to the results of <code>summary(blood_pcacor, loadings = TRUE)</code>, we see these same values reported in the <code>Proportion of Variance</code> row.</p>
</div>
<div id="rescaling-the-eigenvectors" class="section level3 hasAnchor" number="7.3.3">
<h3><span class="header-section-number">7.3.3</span> Rescaling the eigenvectors<a href="example-1-to-see-some-properties-of-pca.html#rescaling-the-eigenvectors" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s calculate the rescaled eigenvalues so they can be more easily interpreted. These coefficients represent the correlation between the observed variable and the component. These are what are outputted in SPSS and in the psych package’s <code>pca()</code> function, which we’ll use for our actual analysis. First, let’s calculate</p>
<p><span class="math display">\[\mathbf{a}^*_i = \mathbf{a}_i\sqrt{\lambda}_i \]</span></p>
<p>We could get the unscaled eigenvalues from the <code>princomp()</code> object using <code>blood_pcacor$loadings[1:64]</code> but that is less convenient (and not as easily reproducible in R code). Instead, we can use the <code>eigen()</code> function on the correlation matrix to get each component’s eigenvector and eigenvalue.</p>
<div class="sourceCode" id="cb336"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb336-1"><a href="example-1-to-see-some-properties-of-pca.html#cb336-1" tabindex="-1"></a>vals.and.vects <span class="ot">&lt;-</span> <span class="fu">eigen</span>(blood_corr)</span>
<span id="cb336-2"><a href="example-1-to-see-some-properties-of-pca.html#cb336-2" tabindex="-1"></a>vals.and.vects</span></code></pre></div>
<pre><code>## eigen() decomposition
## $values
## [1] 2.7922743 1.5316161 1.2492841 0.7784077 0.6215668 0.4888436 0.4356316 0.1023758
## 
## $vectors
##            [,1]          [,2]        [,3]        [,4]       [,5]        [,6]       [,7]        [,8]
## [1,] -0.1942203  0.4171230843  0.39976114  0.65159275  0.1752060  0.36281561  0.1763116 -0.10240413
## [2,] -0.4003625  0.1539289974  0.16772869  0.06371996 -0.8476003 -0.23041340 -0.1104646 -0.01017235
## [3,] -0.4588793 -0.0002984974  0.16777536 -0.27379988  0.2512311 -0.40295337  0.6769694 -0.05038622
## [4,] -0.4303359 -0.4724424229 -0.17128192  0.16914858  0.1177228  0.06459323 -0.2367450 -0.67792429
## [5,]  0.4937748  0.3604497505  0.08716408 -0.18037205 -0.1389990 -0.13572092  0.1572459 -0.72364606
## [6,]  0.3194549 -0.3201664742 -0.27661854  0.63331424 -0.1615438 -0.38374488  0.3765128  0.05214312
## [7,]  0.1768857 -0.5352734751  0.41027697 -0.16314269 -0.2988956  0.51279925  0.3670544 -0.01484605
## [8,]  0.1705160 -0.2452834631  0.70861094  0.08690517  0.1978511 -0.46913476 -0.3757113  0.02620828</code></pre>
<p>Let’s plug these into our equation, <span class="math inline">\(\mathbf{a}^*_i = \mathbf{a}_i\sqrt{\lambda}_i\)</span>. We could do each rescaled eigenvector at a time, or we could use this fancy matrix operation.</p>
<div class="sourceCode" id="cb338"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb338-1"><a href="example-1-to-see-some-properties-of-pca.html#cb338-1" tabindex="-1"></a>loads <span class="ot">&lt;-</span> vals.and.vects<span class="sc">$</span>vectors <span class="sc">%*%</span> <span class="fu">diag</span>(<span class="fu">sqrt</span>(vals.and.vects<span class="sc">$</span>values))</span>
<span id="cb338-2"><a href="example-1-to-see-some-properties-of-pca.html#cb338-2" tabindex="-1"></a><span class="fu">round</span>(loads, <span class="dv">2</span>)</span></code></pre></div>
<pre><code>##       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]
## [1,] -0.32  0.52  0.45  0.57  0.14  0.25  0.12 -0.03
## [2,] -0.67  0.19  0.19  0.06 -0.67 -0.16 -0.07  0.00
## [3,] -0.77  0.00  0.19 -0.24  0.20 -0.28  0.45 -0.02
## [4,] -0.72 -0.58 -0.19  0.15  0.09  0.05 -0.16 -0.22
## [5,]  0.83  0.45  0.10 -0.16 -0.11 -0.09  0.10 -0.23
## [6,]  0.53 -0.40 -0.31  0.56 -0.13 -0.27  0.25  0.02
## [7,]  0.30 -0.66  0.46 -0.14 -0.24  0.36  0.24  0.00
## [8,]  0.28 -0.30  0.79  0.08  0.16 -0.33 -0.25  0.01</code></pre>
<p>In this code, the <code>diag(sqrt(vals.and.vects$values))</code> is the square root of each component’s eigenvalue placed on the diagonal in a square matrix, which has zeros on the off-diagonal. With our data, the eigenvalue diagonal matrix (before square-rooting) looks like this, with capital lambda, <span class="math inline">\(\mathbf{\Lambda}\)</span>, being used to symbolize it given that each element on the diagonal is a little lambda <span class="math inline">\(\lambda_i\)</span>:</p>
<p><span class="math display">\[ \mathbf{\Lambda} =
\begin{bmatrix}
  2.79 &amp;  0  &amp; 0 &amp;  0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
  0 &amp;  1.53  &amp; 0 &amp;  0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
  0 &amp;  0  &amp; 1.25 &amp;  0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
  0 &amp;  0  &amp; 0 &amp;  0.78 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
  0 &amp;  0  &amp; 0 &amp;  0 &amp; 0.62 &amp; 0 &amp; 0 &amp; 0\\
  0 &amp;  0  &amp; 0 &amp;  0 &amp; 0 &amp; 0.49 &amp; 0 &amp; 0\\
  0 &amp;  0  &amp; 0 &amp;  0 &amp; 0 &amp; 0 &amp; 0.44 &amp; 0\\
  0 &amp;  0  &amp; 0 &amp;  0 &amp; 0 &amp; 0 &amp; 0 &amp; 0.10
\end{bmatrix} \]</span></p>
<p>This is also represented as <span class="math inline">\(\mathbf{\lambda} \mathbf{I}\)</span> because multiplying the vector of eigenvalues by a <span class="math inline">\(p \times p\)</span> identity matrix will give the same result as <code>diag(eigenvalues)</code>.</p>
<p>Again, we’re using the square roots of the diagonal, <code>diag(sqrt(vals.and.vects$values))</code>:
<span class="math display">\[ \begin{bmatrix}
  \sqrt{2.79} &amp;  0  &amp; 0 &amp;  0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
  0 &amp;  \sqrt{1.53}  &amp; 0 &amp;  0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
  0 &amp;  0  &amp; \sqrt{1.25} &amp;  0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
  0 &amp;  0  &amp; 0 &amp;  \sqrt{0.78} &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
  0 &amp;  0  &amp; 0 &amp;  0 &amp; \sqrt{0.62} &amp; 0 &amp; 0 &amp; 0\\
  0 &amp;  0  &amp; 0 &amp;  0 &amp; 0 &amp; \sqrt{0.49} &amp; 0 &amp; 0\\
  0 &amp;  0  &amp; 0 &amp;  0 &amp; 0 &amp; 0 &amp; \sqrt{0.44} &amp; 0\\
  0 &amp;  0  &amp; 0 &amp;  0 &amp; 0 &amp; 0 &amp; 0 &amp; \sqrt{0.10}
\end{bmatrix} \]</span></p>
<p>When we matrix-multiply the matrix of unscaled eigenvectors (<code>vals.and.vects$vectors</code>) by this diagonal matrix, each eigenvector cell is multiplied by its respective component’s <span class="math inline">\(\sqrt{\text{eigenvalue}}\)</span> and placed into its corresponding cell in the resulting matrix, which contains our rescaled eigenvectors (<span class="math inline">\(\mathbf{a}^*_i\)</span>), AKA “loadings”.</p>
<p>This is the set of coefficients that represents the correlation between the observed variable and the component.</p>
</div>
<div id="reproducing-the-covariance-or-correlation-matrix" class="section level3 hasAnchor" number="7.3.4">
<h3><span class="header-section-number">7.3.4</span> Reproducing the covariance (or correlation) matrix<a href="example-1-to-see-some-properties-of-pca.html#reproducing-the-covariance-or-correlation-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We can reproduce the matrix from which we performed the principal components analysis on (in this case, the correlation matrix) using the equation at the top of Page 71 in the chapter:</p>
<p><span class="math display">\[\mathbf{S} =  \mathbf{A^*}\mathbf{A^*}^\prime \]</span></p>
<div class="sourceCode" id="cb340"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb340-1"><a href="example-1-to-see-some-properties-of-pca.html#cb340-1" tabindex="-1"></a>A.star <span class="ot">&lt;-</span> loads</span>
<span id="cb340-2"><a href="example-1-to-see-some-properties-of-pca.html#cb340-2" tabindex="-1"></a>R.reprod <span class="ot">&lt;-</span> A.star <span class="sc">%*%</span> <span class="fu">t</span>(A.star)</span>
<span id="cb340-3"><a href="example-1-to-see-some-properties-of-pca.html#cb340-3" tabindex="-1"></a><span class="fu">round</span>( R.reprod, <span class="dv">3</span>)</span></code></pre></div>
<pre><code>##        [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]   [,8]
## [1,]  1.000  0.290  0.202 -0.055 -0.105 -0.252 -0.229  0.058
## [2,]  0.290  1.000  0.415  0.285 -0.376 -0.349 -0.164 -0.129
## [3,]  0.202  0.415  1.000  0.419 -0.521 -0.441 -0.145 -0.076
## [4,] -0.055  0.285  0.419  1.000 -0.877 -0.076  0.023 -0.131
## [5,] -0.105 -0.376 -0.521 -0.877  1.000  0.206  0.034  0.151
## [6,] -0.252 -0.349 -0.441 -0.076  0.206  1.000  0.192  0.077
## [7,] -0.229 -0.164 -0.145  0.023  0.034  0.192  1.000  0.423
## [8,]  0.058 -0.129 -0.076 -0.131  0.151  0.077  0.423  1.000</code></pre>
<p>We can get the <strong>residual covariance matrix</strong> from this by subtracting the reproduced covariance from the original covariance (or correlations in this case):</p>
<div class="sourceCode" id="cb342"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb342-1"><a href="example-1-to-see-some-properties-of-pca.html#cb342-1" tabindex="-1"></a><span class="fu">round</span>( blood_corr <span class="sc">-</span> R.reprod, <span class="dv">3</span>)</span></code></pre></div>
<pre><code>##        rblood plate wblood neut lymph bilir sodium potass
## rblood      0     0      0    0     0     0      0      0
## plate       0     0      0    0     0     0      0      0
## wblood      0     0      0    0     0     0      0      0
## neut        0     0      0    0     0     0      0      0
## lymph       0     0      0    0     0     0      0      0
## bilir       0     0      0    0     0     0      0      0
## sodium      0     0      0    0     0     0      0      0
## potass      0     0      0    0     0     0      0      0</code></pre>
<p>This reproduced correlation is indeed the same as our raw correlation matrix. (If we had conducted the PCA on the covariance matrix, it would return the covariance matrix). This demonstrates that the linear recombination of the variables stil represents the covariance of the raw data.</p>
<p>But, <strong>what if we used only seven of those components instead of all eight</strong>? Our reproduced covariance or correlation matrix will be very similar, but not identical to, the original matrix because that last component was explaining some proportion of the variance. Now, we see some residuals. The seven components do not perfectly explain the data.</p>
<div class="sourceCode" id="cb344"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb344-1"><a href="example-1-to-see-some-properties-of-pca.html#cb344-1" tabindex="-1"></a>R.reprod3 <span class="ot">&lt;-</span> A.star[,<span class="dv">1</span><span class="sc">:</span><span class="dv">7</span>] <span class="sc">%*%</span> <span class="fu">t</span>( (A.star)[,<span class="dv">1</span><span class="sc">:</span><span class="dv">7</span>] )</span>
<span id="cb344-2"><a href="example-1-to-see-some-properties-of-pca.html#cb344-2" tabindex="-1"></a><span class="fu">round</span>( blood_corr <span class="sc">-</span> R.reprod3, <span class="dv">3</span>)</span></code></pre></div>
<pre><code>##        rblood plate wblood   neut  lymph  bilir sodium potass
## rblood  0.001 0.000  0.001  0.007  0.008 -0.001  0.000  0.000
## plate   0.000 0.000  0.000  0.001  0.001  0.000  0.000  0.000
## wblood  0.001 0.000  0.000  0.003  0.004  0.000  0.000  0.000
## neut    0.007 0.001  0.003  0.047  0.050 -0.004  0.001 -0.002
## lymph   0.008 0.001  0.004  0.050  0.054 -0.004  0.001 -0.002
## bilir  -0.001 0.000  0.000 -0.004 -0.004  0.000  0.000  0.000
## sodium  0.000 0.000  0.000  0.001  0.001  0.000  0.000  0.000
## potass  0.000 0.000  0.000 -0.002 -0.002  0.000  0.000  0.000</code></pre>
<p>When we reduce the data to fewer components, we’re seeking parsimony while capitalizing on the optimal linear combinations of the variables, but we are not perfectly explaining the data.</p>
</div>
<div id="summary-of-the-vectors-and-matrices" class="section level3 hasAnchor" number="7.3.5">
<h3><span class="header-section-number">7.3.5</span> Summary of the vectors and matrices<a href="example-1-to-see-some-properties-of-pca.html#summary-of-the-vectors-and-matrices" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We have the following vectors and matrices:</p>
<blockquote>
<p><span class="math inline">\(\mathbf{a}_i\)</span> An eigenvector. <span class="math inline">\(\mathbf{a}_1\)</span> is eigenvector 1, <span class="math inline">\(\mathbf{a}_2\)</span> is eigenvector 2, and so forth</p>
</blockquote>
<blockquote>
<p><span class="math inline">\(\mathbf{A}\)</span> A matrix of eigenvectors. Column 1 is eigenvector 1. Column 2 is eigenvector 2, and so forth</p>
</blockquote>
<blockquote>
<p><span class="math inline">\(\mathbf{a}^*_i\)</span> A rescaled eigenvector, so we can interpret the coefficients as correlations (or so-called loadings)</p>
</blockquote>
<blockquote>
<p><span class="math inline">\(\mathbf{A}^*\)</span> A matrix of rescaled eigenvectors</p>
</blockquote>
<blockquote>
<p><span class="math inline">\(\mathbf{\lambda_i}\)</span> Lowercase lambda, the eigenvalue of Component <span class="math inline">\(i\)</span></p>
</blockquote>
<blockquote>
<p><span class="math inline">\(\mathbf{\Lambda}\)</span> Uppercase lambda, a diagonal matrix of eigenvalues, also represented as <span class="math inline">\(\mathbf{\lambda} \mathbf{I}\)</span></p>
</blockquote>
<blockquote>
<p><span class="math inline">\(\mathbf{S}\)</span> The covariance matrix of the observed data</p>
</blockquote>
<blockquote>
<p><span class="math inline">\(\mathbf{R}\)</span> The correlation matrix of the observed data</p>
</blockquote>
<p>It is worth noting that correlation, eigenvectors, and eigenvalues are all related:</p>
<blockquote>
<p>An eigenvalue matrix is related to the eigevectors and correlation matrix, <span class="math inline">\(\mathbf{\Lambda} = \mathbf{A}^\prime \mathbf{R} \mathbf{A}\)</span></p>
</blockquote>
<div class="sourceCode" id="cb346"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb346-1"><a href="example-1-to-see-some-properties-of-pca.html#cb346-1" tabindex="-1"></a>Lambda <span class="ot">&lt;-</span> <span class="fu">t</span>(vals.and.vects<span class="sc">$</span>vectors) <span class="sc">%*%</span> blood_corr <span class="sc">%*%</span> vals.and.vects<span class="sc">$</span>vectors </span>
<span id="cb346-2"><a href="example-1-to-see-some-properties-of-pca.html#cb346-2" tabindex="-1"></a><span class="fu">round</span>(Lambda, <span class="dv">2</span>)</span></code></pre></div>
<pre><code>##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]
## [1,] 2.79 0.00 0.00 0.00 0.00 0.00 0.00  0.0
## [2,] 0.00 1.53 0.00 0.00 0.00 0.00 0.00  0.0
## [3,] 0.00 0.00 1.25 0.00 0.00 0.00 0.00  0.0
## [4,] 0.00 0.00 0.00 0.78 0.00 0.00 0.00  0.0
## [5,] 0.00 0.00 0.00 0.00 0.62 0.00 0.00  0.0
## [6,] 0.00 0.00 0.00 0.00 0.00 0.49 0.00  0.0
## [7,] 0.00 0.00 0.00 0.00 0.00 0.00 0.44  0.0
## [8,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.1</code></pre>
<blockquote>
<p>A correlation is reproduced from the eigenvectors and eigenvalues, <span class="math inline">\(\mathbf{R} = \mathbf{A} \mathbf{\Lambda} \mathbf{A}^\prime\)</span>, or <span class="math inline">\(\mathbf{R} = (\mathbf{A} \mathbf{\Lambda^{1/2}})( \mathbf{\Lambda^{1/2}} \mathbf{A}^\prime)\)</span></p>
</blockquote>
<div class="sourceCode" id="cb348"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb348-1"><a href="example-1-to-see-some-properties-of-pca.html#cb348-1" tabindex="-1"></a>R <span class="ot">&lt;-</span> vals.and.vects<span class="sc">$</span>vectors <span class="sc">%*%</span> Lambda <span class="sc">%*%</span> <span class="fu">t</span>(vals.and.vects<span class="sc">$</span>vectors) </span>
<span id="cb348-2"><a href="example-1-to-see-some-properties-of-pca.html#cb348-2" tabindex="-1"></a><span class="fu">round</span>(R, <span class="dv">2</span>)</span></code></pre></div>
<pre><code>##       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]
## [1,]  1.00  0.29  0.20 -0.06 -0.11 -0.25 -0.23  0.06
## [2,]  0.29  1.00  0.42  0.29 -0.38 -0.35 -0.16 -0.13
## [3,]  0.20  0.42  1.00  0.42 -0.52 -0.44 -0.15 -0.08
## [4,] -0.06  0.28  0.42  1.00 -0.88 -0.08  0.02 -0.13
## [5,] -0.11 -0.38 -0.52 -0.88  1.00  0.21  0.03  0.15
## [6,] -0.25 -0.35 -0.44 -0.08  0.21  1.00  0.19  0.08
## [7,] -0.23 -0.16 -0.15  0.02  0.03  0.19  1.00  0.42
## [8,]  0.06 -0.13 -0.08 -0.13  0.15  0.08  0.42  1.00</code></pre>
</div>
</div>
<div id="using-the-psych-package" class="section level2 hasAnchor" number="7.4">
<h2><span class="header-section-number">7.4</span> Using the psych package<a href="example-1-to-see-some-properties-of-pca.html#using-the-psych-package" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The psych package’s <code>pca()</code> function reports results that are ready for interpretation. It automatically uses the correlation matrix rather than the covariance matrix.</p>
<p>We specify the correlation matrix or the raw data, the number of factors we wish to extract, which in a preliminary analysis is the number of observed variables (8 in our example). In PCA, we can avoid rotating the solution for now.</p>
<div class="sourceCode" id="cb350"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb350-1"><a href="example-1-to-see-some-properties-of-pca.html#cb350-1" tabindex="-1"></a><span class="fu">library</span>(psych)</span>
<span id="cb350-2"><a href="example-1-to-see-some-properties-of-pca.html#cb350-2" tabindex="-1"></a>pc_psy <span class="ot">&lt;-</span> psych<span class="sc">::</span><span class="fu">pca</span>(blood_corr,   <span class="co"># can also use the raw data as input.</span></span>
<span id="cb350-3"><a href="example-1-to-see-some-properties-of-pca.html#cb350-3" tabindex="-1"></a>              <span class="at">nfactors =</span> <span class="dv">8</span>, </span>
<span id="cb350-4"><a href="example-1-to-see-some-properties-of-pca.html#cb350-4" tabindex="-1"></a>              <span class="at">rotate =</span> <span class="st">&#39;none&#39;</span>)</span>
<span id="cb350-5"><a href="example-1-to-see-some-properties-of-pca.html#cb350-5" tabindex="-1"></a><span class="fu">print</span>(pc_psy,<span class="at">digits=</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## Principal Components Analysis
## Call: principal(r = r, nfactors = nfactors, residuals = residuals, 
##     rotate = rotate, n.obs = n.obs, covar = covar, scores = scores, 
##     missing = missing, impute = impute, oblique.scores = oblique.scores, 
##     method = method, use = use, cor = cor, correct = 0.5, weight = NULL)
## Standardized loadings (pattern matrix) based upon correlation matrix
##          PC1   PC2   PC3   PC4   PC5   PC6   PC7   PC8 h2       u2 com
## rblood  0.32 -0.52  0.45  0.57 -0.14 -0.25  0.12  0.03  1 -4.4e-16 4.2
## plate   0.67 -0.19  0.19  0.06  0.67  0.16 -0.07  0.00  1  1.1e-15 2.5
## wblood  0.77  0.00  0.19 -0.24 -0.20  0.28  0.45  0.02  1  1.4e-15 2.5
## neut    0.72  0.58 -0.19  0.15 -0.09 -0.05 -0.16  0.22  1  2.0e-15 2.6
## lymph  -0.83 -0.45  0.10 -0.16  0.11  0.09  0.10  0.23  1  6.7e-16 2.0
## bilir  -0.53  0.40 -0.31  0.56  0.13  0.27  0.25 -0.02  1  1.8e-15 4.5
## sodium -0.30  0.66  0.46 -0.14  0.24 -0.36  0.24  0.00  1  1.8e-15 3.7
## potass -0.28  0.30  0.79  0.08 -0.16  0.33 -0.25 -0.01  1  2.8e-15 2.4
## 
##                        PC1  PC2  PC3  PC4  PC5  PC6  PC7  PC8
## SS loadings           2.79 1.53 1.25 0.78 0.62 0.49 0.44 0.10
## Proportion Var        0.35 0.19 0.16 0.10 0.08 0.06 0.05 0.01
## Cumulative Var        0.35 0.54 0.70 0.79 0.87 0.93 0.99 1.00
## Proportion Explained  0.35 0.19 0.16 0.10 0.08 0.06 0.05 0.01
## Cumulative Proportion 0.35 0.54 0.70 0.79 0.87 0.93 0.99 1.00
## 
## Mean item complexity =  3
## Test of the hypothesis that 8 components are sufficient.
## 
## The root mean square of the residuals (RMSR) is  0 
## 
## Fit based upon off diagonal values = 1</code></pre>
<p>The output under the label <code>Standardized loadings</code> is the same reweighted eigenvector matrix, <span class="math inline">\(\mathbf{A^*}\)</span>, we calculated above. Again, these represent the correlations between each observed variable and each component.</p>
</div>
<div id="determining-how-many-components-to-retain" class="section level2 hasAnchor" number="7.5">
<h2><span class="header-section-number">7.5</span> Determining how many components to retain<a href="example-1-to-see-some-properties-of-pca.html#determining-how-many-components-to-retain" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>One of the most valuable reasons for using PCA is for dimension reduction. There are several different methods for determining how many components to retain. Some analysts set the threshold to be sure that they have enough components to explain some pre-specified proportion of variance, such as 80%. Some use Kaiser’s rule of retaining components of values of at least <span class="math inline">\(1.00\)</span>, though Joliffe <span class="citation">(<a href="#ref-jolliffe_discarding_1972">1972</a>)</span> suggested a threshold of <span class="math inline">\(0.70\)</span>. Others <span class="citation">(<a href="#ref-cattell_scree_1966">Cattell 1966</a>)</span> suggested using a scree plot to see where the elbow in the curve is, though this may be more appropriate for factor analysis rather than PCA. Another is the parallel test. And still another is the Hull test, which can be used if the PCA is on the data set rather than a correlation matrix.</p>
<div id="kaisers-rule" class="section level3 hasAnchor" number="7.5.1">
<h3><span class="header-section-number">7.5.1</span> 1. Kaiser’s rule<a href="example-1-to-see-some-properties-of-pca.html#kaisers-rule" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The eigenvalue-greater-than-one approach, also called the K1 criterion named after the Kaiser’s <span class="citation">(<a href="#ref-kaiser_index_1974">1974</a>)</span> work, can be used in PCA to determine the number of factors to retain.</p>
<div class="sourceCode" id="cb352"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb352-1"><a href="example-1-to-see-some-properties-of-pca.html#cb352-1" tabindex="-1"></a>pc_psy<span class="sc">$</span>loadings</span></code></pre></div>
<pre><code>## 
## Loadings:
##        PC1    PC2    PC3    PC4    PC5    PC6    PC7    PC8   
## rblood  0.325 -0.516  0.447  0.575 -0.138 -0.254  0.116       
## plate   0.669 -0.191  0.187         0.668  0.161              
## wblood  0.767         0.188 -0.242 -0.198  0.282  0.447       
## neut    0.719  0.585 -0.191  0.149               -0.156  0.217
## lymph  -0.825 -0.446        -0.159  0.110         0.104  0.232
## bilir  -0.534  0.396 -0.309  0.559  0.127  0.268  0.249       
## sodium -0.296  0.662  0.459 -0.144  0.236 -0.359  0.242       
## potass -0.285  0.304  0.792        -0.156  0.328 -0.248       
## 
##                  PC1   PC2   PC3   PC4   PC5   PC6   PC7   PC8
## SS loadings    2.792 1.532 1.249 0.778 0.622 0.489 0.436 0.102
## Proportion Var 0.349 0.191 0.156 0.097 0.078 0.061 0.054 0.013
## Cumulative Var 0.349 0.540 0.697 0.794 0.872 0.933 0.987 1.000</code></pre>
<p>Based on this criterion, we would retain three components, as the eigenvalues, listed in the <code>SS loadings</code> row are above 1.00 in the first three components. If we used 80% of the variance explained as a criterion instead of Kaiser’s rule, we would look at the <code>pca()</code> output at the <code>Cumulative Proportion</code> line and see that we’d need more at least five components, as Component 4 explains 79% of the variance. It’s borderline, but them’s the rules we would have set for ourselves.</p>
</div>
<div id="scree-plot" class="section level3 hasAnchor" number="7.5.2">
<h3><span class="header-section-number">7.5.2</span> 2. Scree plot<a href="example-1-to-see-some-properties-of-pca.html#scree-plot" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s generate a scree plot. We can use this to not only see those components with eigenvalues that exceed 1.00 but we can see where the plot levels off.</p>
<p>Though there are packages that can do this for us, we can do this with ggplot.<a href="#fn30" class="footnote-ref" id="fnref30"><sup>30</sup></a> First, we create a data frame that includes the component number and the eigenvalue. For this, we can get the eigenvalues from the <code>psych::pca()</code> outputted object with the <code>$values</code>.</p>
<div class="sourceCode" id="cb354"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb354-1"><a href="example-1-to-see-some-properties-of-pca.html#cb354-1" tabindex="-1"></a>vals.and.vects <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">Component.number =</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(pc_psy<span class="sc">$</span>values), </span>
<span id="cb354-2"><a href="example-1-to-see-some-properties-of-pca.html#cb354-2" tabindex="-1"></a>                             <span class="at">Eigenvalue =</span> pc_psy<span class="sc">$</span>values)</span>
<span id="cb354-3"><a href="example-1-to-see-some-properties-of-pca.html#cb354-3" tabindex="-1"></a><span class="co"># Commented out here is the corresponding code if we used the object outputted </span></span>
<span id="cb354-4"><a href="example-1-to-see-some-properties-of-pca.html#cb354-4" tabindex="-1"></a><span class="co"># from the `princomp()` function.</span></span>
<span id="cb354-5"><a href="example-1-to-see-some-properties-of-pca.html#cb354-5" tabindex="-1"></a><span class="co"># vals.and.vects &lt;- data.frame(Component.number = 1:length(blood_pcacor$sdev^2), </span></span>
<span id="cb354-6"><a href="example-1-to-see-some-properties-of-pca.html#cb354-6" tabindex="-1"></a><span class="co">#                    Eigenvalue    = blood_pcacor$sdev^2)</span></span>
<span id="cb354-7"><a href="example-1-to-see-some-properties-of-pca.html#cb354-7" tabindex="-1"></a><span class="fu">ggplot</span>(vals.and.vects, <span class="fu">aes</span>(<span class="at">x =</span> Component.number, <span class="at">y =</span> Eigenvalue ) ) <span class="sc">+</span></span>
<span id="cb354-8"><a href="example-1-to-see-some-properties-of-pca.html#cb354-8" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb354-9"><a href="example-1-to-see-some-properties-of-pca.html#cb354-9" tabindex="-1"></a>  <span class="fu">geom_point</span>()</span></code></pre></div>
<p><img src="multivariate_stats_files/figure-html/unnamed-chunk-174-1.png" width="672" /></p>
<p>With this plot, we seek to find where the slope changes. The first component is clearly present, as the slope is much steeper from the first to second component than it is from the second to third component, whose slope resembles the rest of the scree. In other words, if we drew a line of best for all of the eigenvalues from Component 2 through 8, such a line would have the points close to it. However, we might decide to retain three components because the eigenvalues after Component 3 seem to flatten out. This is a good example of when this eyeballing technique is not so straightforward.</p>
</div>
<div id="parallel-analysis" class="section level3 hasAnchor" number="7.5.3">
<h3><span class="header-section-number">7.5.3</span> 3. Parallel analysis<a href="example-1-to-see-some-properties-of-pca.html#parallel-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The parallel analysis <span class="citation">(<a href="#ref-horn_rationale_1965">Horn 1965</a>)</span> provides a method for comparing our data to randomly generated data sets having the same number of cases and observed variables. After ranking these randomly generated data from low to high, the procedure can estimate the eigenvalue at each <span class="math inline">\(n\)</span>th component at the 95 percentile level. We compare the eigenvalues calculated from our observed data with those of the randomly generated data. If with a given model, such as one with <span class="math inline">\(n\)</span> components, 95% of the randomly drawn samples have eigenvalues that <strong>are lower than</strong> what we observe in our data, we can assume that the component structure in our data lies outside that 0-to-95 percent range (in other words, our extreme eigenvalues do not likely to occur by random luck). The last component to meet this criterion is the number of components to retain. Up to that point, the components explain more of the variance than would occur by chance at the 95% cutoff.</p>
<p>We can use the <code>parallel()</code> function from the nFactors package <span class="citation">(<a href="#ref-R-nFactors"><strong>R-nFactors?</strong></a>)</span> to perform a parallel analysis. The parallel analysis takes random draws of data and estimates the eigenvalues that would occur randomly in each random draw. The mean of the random draws is reported, along with the 95 percentile. We want to use the 95 percentile.</p>
<p>The first step is to compute the eigenvalues, which we did above with the scree plot.</p>
<div class="sourceCode" id="cb355"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb355-1"><a href="example-1-to-see-some-properties-of-pca.html#cb355-1" tabindex="-1"></a><span class="fu">library</span>(nFactors)</span>
<span id="cb355-2"><a href="example-1-to-see-some-properties-of-pca.html#cb355-2" tabindex="-1"></a><span class="co"># help(package=&quot;nFactors&quot;)</span></span>
<span id="cb355-3"><a href="example-1-to-see-some-properties-of-pca.html#cb355-3" tabindex="-1"></a>n_i  <span class="ot">&lt;-</span> <span class="dv">72</span> <span class="co"># The number of cases in our data</span></span>
<span id="cb355-4"><a href="example-1-to-see-some-properties-of-pca.html#cb355-4" tabindex="-1"></a>n_p <span class="ot">&lt;-</span> <span class="fu">ncol</span>(blood_corr) <span class="co"># The number of variables in our data</span></span>
<span id="cb355-5"><a href="example-1-to-see-some-properties-of-pca.html#cb355-5" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)            <span class="co"># To reproduce our randomly generated results.</span></span>
<span id="cb355-6"><a href="example-1-to-see-some-properties-of-pca.html#cb355-6" tabindex="-1"></a>Eigs <span class="ot">&lt;-</span> pc_psy<span class="sc">$</span>values    <span class="co"># The eigenvalues</span></span>
<span id="cb355-7"><a href="example-1-to-see-some-properties-of-pca.html#cb355-7" tabindex="-1"></a>n_components  <span class="ot">&lt;-</span> <span class="fu">length</span>(Eigs) <span class="co"># number of components</span></span>
<span id="cb355-8"><a href="example-1-to-see-some-properties-of-pca.html#cb355-8" tabindex="-1"></a>paral <span class="ot">&lt;-</span> <span class="fu">parallel</span>(<span class="at">subject =</span> n_i,  <span class="co"># The number of cases in our data</span></span>
<span id="cb355-9"><a href="example-1-to-see-some-properties-of-pca.html#cb355-9" tabindex="-1"></a>                      <span class="at">var =</span> n_p,  <span class="co"># The number of variables in our data</span></span>
<span id="cb355-10"><a href="example-1-to-see-some-properties-of-pca.html#cb355-10" tabindex="-1"></a>                      <span class="at">rep =</span> <span class="dv">100</span>,</span>
<span id="cb355-11"><a href="example-1-to-see-some-properties-of-pca.html#cb355-11" tabindex="-1"></a>                 <span class="at">quantile =</span> .<span class="dv">95</span>,</span>
<span id="cb355-12"><a href="example-1-to-see-some-properties-of-pca.html#cb355-12" tabindex="-1"></a>                   <span class="at">model  =</span> <span class="st">&quot;components&quot;</span>)</span>
<span id="cb355-13"><a href="example-1-to-see-some-properties-of-pca.html#cb355-13" tabindex="-1"></a></span>
<span id="cb355-14"><a href="example-1-to-see-some-properties-of-pca.html#cb355-14" tabindex="-1"></a>ParallelAna <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">Ncomponent  =</span> <span class="dv">1</span><span class="sc">:</span>n_components,</span>
<span id="cb355-15"><a href="example-1-to-see-some-properties-of-pca.html#cb355-15" tabindex="-1"></a>                           Eigs,</span>
<span id="cb355-16"><a href="example-1-to-see-some-properties-of-pca.html#cb355-16" tabindex="-1"></a>                           <span class="at">RandEigM =</span> paral<span class="sc">$</span>eigen<span class="sc">$</span>mevpea,</span>
<span id="cb355-17"><a href="example-1-to-see-some-properties-of-pca.html#cb355-17" tabindex="-1"></a>                           <span class="at">RandEig95=</span> paral<span class="sc">$</span>eigen<span class="sc">$</span>qevpea)</span>
<span id="cb355-18"><a href="example-1-to-see-some-properties-of-pca.html#cb355-18" tabindex="-1"></a>ParallelAna <span class="ot">&lt;-</span> <span class="fu">round</span>(ParallelAna, <span class="dv">3</span>)</span>
<span id="cb355-19"><a href="example-1-to-see-some-properties-of-pca.html#cb355-19" tabindex="-1"></a>ParallelAna</span></code></pre></div>
<pre><code>##   Ncomponent  Eigs RandEigM RandEig95
## 1          1 2.792    1.527     1.707
## 2          2 1.532    1.315     1.454
## 3          3 1.249    1.159     1.265
## 4          4 0.778    1.033     1.122
## 5          5 0.622    0.919     0.980
## 6          6 0.489    0.803     0.882
## 7          7 0.436    0.688     0.778
## 8          8 0.102    0.556     0.660</code></pre>
<p>Based on these results, it looks like we should retain two components, as explained below.</p>
<p>First, it is worth mentioning that because this operation is based on random draws of the data, the point estimate and 95th percentile estimate will differ if we performed this operation again. In our code, we used the <code>set.seed()</code> function to make this particular result reproducible. We can remove that line of code or change the seed values and observe different results.<a href="#fn31" class="footnote-ref" id="fnref31"><sup>31</sup></a></p>
<p>In our output, we can use the rightmost column and identify at what point 95% of the randomly drawn data’s eigenvalues exceed our reduced-eigenvalue estimates. We see that it is at component number <span class="math inline">\(3\)</span> that our observed reduced eigenvalue (<span class="math inline">\(1.249\)</span>) is exceeded by the randomly generated eigenvalue (<span class="math inline">\(1.265\)</span>). Based on this, we step down to one component below this and decide we should retain <span class="math inline">\(2\)</span> components.</p>
<p><strong>How many should we retain?</strong> The parallel analysis might make the most sense with these data, as it is compares random draws. With this, we might retain two components.</p>
</div>
</div>
<div id="reduced-pca-model" class="section level2 hasAnchor" number="7.6">
<h2><span class="header-section-number">7.6</span> Reduced PCA model<a href="example-1-to-see-some-properties-of-pca.html#reduced-pca-model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb357"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb357-1"><a href="example-1-to-see-some-properties-of-pca.html#cb357-1" tabindex="-1"></a><span class="fu">library</span>(psych)</span>
<span id="cb357-2"><a href="example-1-to-see-some-properties-of-pca.html#cb357-2" tabindex="-1"></a>pc_psy <span class="ot">&lt;-</span> psych<span class="sc">::</span><span class="fu">pca</span>(blood_corr, </span>
<span id="cb357-3"><a href="example-1-to-see-some-properties-of-pca.html#cb357-3" tabindex="-1"></a>              <span class="at">nfactors =</span> <span class="dv">2</span>, </span>
<span id="cb357-4"><a href="example-1-to-see-some-properties-of-pca.html#cb357-4" tabindex="-1"></a>              <span class="at">rotate =</span> <span class="st">&#39;none&#39;</span>)</span>
<span id="cb357-5"><a href="example-1-to-see-some-properties-of-pca.html#cb357-5" tabindex="-1"></a><span class="fu">print</span>(pc_psy, <span class="at">digits =</span> <span class="dv">3</span>)</span></code></pre></div>
<pre><code>## Principal Components Analysis
## Call: principal(r = r, nfactors = nfactors, residuals = residuals, 
##     rotate = rotate, n.obs = n.obs, covar = covar, scores = scores, 
##     missing = missing, impute = impute, oblique.scores = oblique.scores, 
##     method = method, use = use, cor = cor, correct = 0.5, weight = NULL)
## Standardized loadings (pattern matrix) based upon correlation matrix
##           PC1    PC2    h2    u2  com
## rblood  0.325 -0.516 0.372 0.628 1.68
## plate   0.669 -0.191 0.484 0.516 1.16
## wblood  0.767  0.000 0.588 0.412 1.00
## neut    0.719  0.585 0.859 0.141 1.92
## lymph  -0.825 -0.446 0.880 0.120 1.54
## bilir  -0.534  0.396 0.442 0.558 1.85
## sodium -0.296  0.662 0.526 0.474 1.38
## potass -0.285  0.304 0.173 0.827 1.99
## 
##                         PC1   PC2
## SS loadings           2.792 1.532
## Proportion Var        0.349 0.191
## Cumulative Var        0.349 0.540
## Proportion Explained  0.646 0.354
## Cumulative Proportion 0.646 1.000
## 
## Mean item complexity =  1.6
## Test of the hypothesis that 2 components are sufficient.
## 
## The root mean square of the residuals (RMSR) is  0.127 
## 
## Fit based upon off diagonal values = 0.823</code></pre>
</div>
<div id="interpretation-1" class="section level2 hasAnchor" number="7.7">
<h2><span class="header-section-number">7.7</span> Interpretation<a href="example-1-to-see-some-properties-of-pca.html#interpretation-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The pysch output is formatted for factor-analysis, which is closely related to PCA but used for a a different purpose (EFA is for estimating latent variables). Here, we have the first two components, <code>PC1</code> and <code>PC2</code>, the communalities <code>h2</code>, the unique variances, <code>u2</code>, and the complexity, <code>com</code>. These were not explained in our reading but will be in the explanation about exploratory factor analysis. Each observed variable’s communality is the proportion of its variance that is shared with the two factors. If we sum the squared coefficients, we get the communality; for example, with <code>rblood</code>, <span class="math inline">\(.325^2 + (-.516)^2 = .372\)</span>. The unique variance is what is not explained by the shared variance; with this example, it is <span class="math inline">\(1 - .3718 = .628\)</span>. The complexity refers to how strongly the item loads on other components. The last observed variable, potassium, seems to be evenly shared across two components; it’s complexity is <code>1.99</code> and we can see that indeed its loadings on PC1 and PC2 are nearly equal in strength. It is easier to interpret models that have low complexity.</p>
<p>These coefficients (or “loadings” to some researchers) are unrotated. In this data set, it seems that the first component is due to a large part from the variability in the lymphocites variable, (-825) as well as to the white blood cell counts, neutrophil levels, platelet counts, bilirubin levels, and to a small extent red blood cell counts. The component seems to be explained in one direction by lymphocyte levels and bilirubin levels and in the opposite direction from the others.<a href="#fn32" class="footnote-ref" id="fnref32"><sup>32</sup></a></p>
<p>The second component seems to be an indicator of levels of sodium, neutrophil, red blood cells, lymphocytes, bilirubin, and to a small extent, potassium, with different signs among these. Interpreting these is not easy because the components do not seem to parsimoniously explain the observed variables. When this occurs, many analysts will rotate the solution so that the components more easily align with the variables; others will disagree and state that this changes the structure.</p>
<div id="plot" class="section level3 hasAnchor" number="7.7.1">
<h3><span class="header-section-number">7.7.1</span> Plot<a href="example-1-to-see-some-properties-of-pca.html#plot" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s create a plot of the component scores.</p>
<p>To prepare for this, let’s save the rescaled eigenvector matrix (the loadings). We’ll use the <code>cbind()</code> function on the <code>$loadings</code> part of the outputted object from our PCA model.</p>
<div class="sourceCode" id="cb359"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb359-1"><a href="example-1-to-see-some-properties-of-pca.html#cb359-1" tabindex="-1"></a>A <span class="ot">&lt;-</span> <span class="fu">cbind</span>(pc_psy<span class="sc">$</span>loadings) </span>
<span id="cb359-2"><a href="example-1-to-see-some-properties-of-pca.html#cb359-2" tabindex="-1"></a>A</span></code></pre></div>
<pre><code>##               PC1           PC2
## rblood  0.3245441 -0.5162251871
## plate   0.6690097 -0.1905001868
## wblood  0.7667919  0.0003694159
## neut    0.7190955  0.5846875596
## lymph  -0.8251026 -0.4460871309
## bilir  -0.5338122  0.3962331606
## sodium -0.2955777  0.6624463145
## potass -0.2849339  0.3035590847</code></pre>
<div class="sourceCode" id="cb361"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb361-1"><a href="example-1-to-see-some-properties-of-pca.html#cb361-1" tabindex="-1"></a>A <span class="sc">%&gt;%</span> </span>
<span id="cb361-2"><a href="example-1-to-see-some-properties-of-pca.html#cb361-2" tabindex="-1"></a>  <span class="fu">data.frame</span>(.) <span class="sc">%&gt;%</span> </span>
<span id="cb361-3"><a href="example-1-to-see-some-properties-of-pca.html#cb361-3" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> PC1, <span class="at">y =</span> PC2)) <span class="sc">+</span></span>
<span id="cb361-4"><a href="example-1-to-see-some-properties-of-pca.html#cb361-4" tabindex="-1"></a>  <span class="fu">geom_vline</span>( <span class="fu">aes</span>(<span class="at">xintercept =</span> <span class="dv">0</span>) ) <span class="sc">+</span></span>
<span id="cb361-5"><a href="example-1-to-see-some-properties-of-pca.html#cb361-5" tabindex="-1"></a>  <span class="fu">geom_hline</span>( <span class="fu">aes</span>(<span class="at">yintercept =</span> <span class="dv">0</span>) ) <span class="sc">+</span></span>
<span id="cb361-6"><a href="example-1-to-see-some-properties-of-pca.html#cb361-6" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb361-7"><a href="example-1-to-see-some-properties-of-pca.html#cb361-7" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">limits=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb361-8"><a href="example-1-to-see-some-properties-of-pca.html#cb361-8" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="at">limits=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb361-9"><a href="example-1-to-see-some-properties-of-pca.html#cb361-9" tabindex="-1"></a>  <span class="fu">geom_text</span>(<span class="at">hjust=</span><span class="sc">-</span>.<span class="dv">2</span>, <span class="at">vjust=</span><span class="dv">0</span>, <span class="at">label =</span> <span class="fu">rownames</span>(A)) <span class="sc">+</span></span>
<span id="cb361-10"><a href="example-1-to-see-some-properties-of-pca.html#cb361-10" tabindex="-1"></a>  <span class="fu">labs</span>( <span class="at">title =</span> <span class="st">&quot;Unrotated PCA Solution&quot;</span>)</span></code></pre></div>
<p><img src="multivariate_stats_files/figure-html/unnamed-chunk-179-1.png" width="672" /></p>
<p>This kind of plot helps us to see which variables are more strongly explaining which component. Those closest to the horizontal axis and farther away from the origin are more strongly contributing to Component 1. Those closer to the vertical axis and farther away from the origin more strongly explain Component 2, which in this case does not seem to include many; sodium is the strongest one with Component 2.</p>
</div>
</div>
<div id="interpretation-of-the-rotated-solution" class="section level2 hasAnchor" number="7.8">
<h2><span class="header-section-number">7.8</span> Interpretation of the rotated solution<a href="example-1-to-see-some-properties-of-pca.html#interpretation-of-the-rotated-solution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Some researchers caution against rotating the solution, as it changes structure. Others argue for it, as it helps us to explain the components. We specify <code>rotate = varimax</code> for varimax rotation so that the PCA solution remains orthogonal; that is, so that the components do not correlate with each other but rather explain unrelated features of the combination of variables.<a href="#fn33" class="footnote-ref" id="fnref33"><sup>33</sup></a> This type of orthogonal rotation is probably fine with data that are assumed to exist in the world rather. With data that are based on social or psychological constructions, such as academic proficiency, orthogonal rotation makes less sense.</p>
<div class="sourceCode" id="cb362"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb362-1"><a href="example-1-to-see-some-properties-of-pca.html#cb362-1" tabindex="-1"></a><span class="fu">library</span>(psych)</span>
<span id="cb362-2"><a href="example-1-to-see-some-properties-of-pca.html#cb362-2" tabindex="-1"></a>pc_psy_rot <span class="ot">&lt;-</span> psych<span class="sc">::</span><span class="fu">pca</span>(blood_corr, </span>
<span id="cb362-3"><a href="example-1-to-see-some-properties-of-pca.html#cb362-3" tabindex="-1"></a>              <span class="at">nfactors =</span> <span class="dv">2</span>, </span>
<span id="cb362-4"><a href="example-1-to-see-some-properties-of-pca.html#cb362-4" tabindex="-1"></a>              <span class="at">rotate =</span> <span class="st">&#39;varimax&#39;</span>)</span>
<span id="cb362-5"><a href="example-1-to-see-some-properties-of-pca.html#cb362-5" tabindex="-1"></a><span class="fu">print</span>(pc_psy_rot, <span class="at">digits =</span> <span class="dv">3</span>)</span></code></pre></div>
<pre><code>## Principal Components Analysis
## Call: principal(r = r, nfactors = nfactors, residuals = residuals, 
##     rotate = rotate, n.obs = n.obs, covar = covar, scores = scores, 
##     missing = missing, impute = impute, oblique.scores = oblique.scores, 
##     method = method, use = use, cor = cor, correct = 0.5, weight = NULL)
## Standardized loadings (pattern matrix) based upon correlation matrix
##           RC1    RC2    h2    u2  com
## rblood  0.003 -0.610 0.372 0.628 1.00
## plate   0.467 -0.515 0.484 0.516 1.98
## wblood  0.651 -0.405 0.588 0.412 1.67
## neut    0.919  0.116 0.859 0.141 1.03
## lymph  -0.936  0.058 0.880 0.120 1.01
## bilir  -0.244  0.619 0.442 0.558 1.30
## sodium  0.099  0.719 0.526 0.474 1.04
## potass -0.081  0.408 0.173 0.827 1.08
## 
##                         RC1   RC2
## SS loadings           2.440 1.884
## Proportion Var        0.305 0.235
## Cumulative Var        0.305 0.540
## Proportion Explained  0.564 0.436
## Cumulative Proportion 0.564 1.000
## 
## Mean item complexity =  1.3
## Test of the hypothesis that 2 components are sufficient.
## 
## The root mean square of the residuals (RMSR) is  0.127 
## 
## Fit based upon off diagonal values = 0.823</code></pre>
<p>In this rotated structure, we see it is easier to give a label to the first component as being explained by lymphocyte and neutrophil levels (in opposite directions), and white blood cell and platelet counts.</p>
<div class="sourceCode" id="cb364"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb364-1"><a href="example-1-to-see-some-properties-of-pca.html#cb364-1" tabindex="-1"></a>A_rot <span class="ot">&lt;-</span> <span class="fu">cbind</span>(pc_psy_rot<span class="sc">$</span>loadings) </span>
<span id="cb364-2"><a href="example-1-to-see-some-properties-of-pca.html#cb364-2" tabindex="-1"></a></span>
<span id="cb364-3"><a href="example-1-to-see-some-properties-of-pca.html#cb364-3" tabindex="-1"></a>A_rot <span class="sc">%&gt;%</span> </span>
<span id="cb364-4"><a href="example-1-to-see-some-properties-of-pca.html#cb364-4" tabindex="-1"></a>  <span class="fu">data.frame</span>(.) <span class="sc">%&gt;%</span> </span>
<span id="cb364-5"><a href="example-1-to-see-some-properties-of-pca.html#cb364-5" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> RC1, <span class="at">y =</span> RC2)) <span class="sc">+</span></span>
<span id="cb364-6"><a href="example-1-to-see-some-properties-of-pca.html#cb364-6" tabindex="-1"></a>  <span class="fu">geom_vline</span>( <span class="fu">aes</span>(<span class="at">xintercept =</span> <span class="dv">0</span>) ) <span class="sc">+</span></span>
<span id="cb364-7"><a href="example-1-to-see-some-properties-of-pca.html#cb364-7" tabindex="-1"></a>  <span class="fu">geom_hline</span>( <span class="fu">aes</span>(<span class="at">yintercept =</span> <span class="dv">0</span>) ) <span class="sc">+</span></span>
<span id="cb364-8"><a href="example-1-to-see-some-properties-of-pca.html#cb364-8" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb364-9"><a href="example-1-to-see-some-properties-of-pca.html#cb364-9" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">limits=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb364-10"><a href="example-1-to-see-some-properties-of-pca.html#cb364-10" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="at">limits=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb364-11"><a href="example-1-to-see-some-properties-of-pca.html#cb364-11" tabindex="-1"></a>  <span class="fu">geom_text</span>(<span class="at">hjust=</span><span class="sc">-</span>.<span class="dv">2</span>, <span class="at">vjust=</span><span class="dv">0</span>, <span class="at">label =</span> <span class="fu">rownames</span>(A)) <span class="sc">+</span></span>
<span id="cb364-12"><a href="example-1-to-see-some-properties-of-pca.html#cb364-12" tabindex="-1"></a>  <span class="fu">labs</span>( <span class="at">title =</span> <span class="st">&quot;Rotated PCA Solution&quot;</span>)</span></code></pre></div>
<p><img src="multivariate_stats_files/figure-html/unnamed-chunk-181-1.png" width="672" />
In this plot, the variables all remain in same space relative to each other. It is the component axes that have moved. In comparing this to the unrotated solution, it looks like we rotated the axes about 45 degrees.</p>
<p>We see that with the rotated solution, the axes of the components have been moved closer to the observed variables, thereby reducing their complexity and making it easier to associate some variables with individual components. This comes at a cost, however, as we also see that now, compared to our unrotated model, the platelet and white-blood cell variables are now somewhere in between the two components. Their complexities are now high whereas before in the unrotated model, they were low because they were both close to the first component axis (the horizontal axis). Whether to rotate or not is a decision made by the researcher. In the social sciences, we tend to rotate the components, though in the social sciences, we should not usually use PCA because we tend to think more about latent constructs, which are best estimated using EFA.</p>
</div>
</div>
<h3> References<a href="references-3.html#references-3" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-cattell_scree_1966" class="csl-entry">
Cattell, Raymond B. 1966. <span>“The Scree Test for the Number of Factors.”</span> <em>Multivariate Behavioral Research</em> 1 (2): 245–76. <a href="https://doi.org/10.1207/s15327906mbr0102_10">https://doi.org/10.1207/s15327906mbr0102_10</a>.
</div>
<div id="ref-horn_rationale_1965" class="csl-entry">
Horn, John L. 1965. <span>“A Rationale and Test for the Number of Factors in Factor Analysis.”</span> <em>Psychometrika</em> 30 (2): 179–85. <a href="https://doi.org/10.1007/BF02289447">https://doi.org/10.1007/BF02289447</a>.
</div>
<div id="ref-jolliffe_discarding_1972" class="csl-entry">
Jolliffe, I. T. 1972. <span>“Discarding Variables in a Principal Component Analysis. I: Artificial Data.”</span> <em>Journal of the Royal Statistical Society. Series C (Applied Statistics)</em> 21 (2): 160–73. <a href="https://doi.org/10.2307/2346488">https://doi.org/10.2307/2346488</a>.
</div>
<div id="ref-kaiser_index_1974" class="csl-entry">
Kaiser, Henry F. 1974. <span>“An Index of Factorial Simplicity.”</span> <em>Psychometrika</em> 39 (1): 31–36. <a href="https://doi.org/10.1007/BF02291575">https://doi.org/10.1007/BF02291575</a>.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="29">
<li id="fn29"><p>We can also be lazy and use the <code>cor_to_cov()</code> function from the <code>correlation</code> package <span class="citation">(<a href="#ref-R-correlation"><strong>R-correlation?</strong></a>)</span>.<a href="example-1-to-see-some-properties-of-pca.html#fnref29" class="footnote-back">↩︎</a></p></li>
<li id="fn30"><p>There are packages we can use for this; for example, the <code>factoextra</code> package <span class="citation">(<a href="#ref-R-factoextra"><strong>R-factoextra?</strong></a>)</span> has the <code>fviz_screeplot()</code> function.<a href="example-1-to-see-some-properties-of-pca.html#fnref30" class="footnote-back">↩︎</a></p></li>
<li id="fn31"><p>Strictly speaking, we should only run this procedure one time—or with a single random seed so we can reproduce our one procedure—to avoid phishing for the desired number of components.<a href="example-1-to-see-some-properties-of-pca.html#fnref31" class="footnote-back">↩︎</a></p></li>
<li id="fn32"><p>I’m sure I got some of those incorrect as I only have the labels that were presented in Everitt and Hothorn’s (2011) chapter.<a href="example-1-to-see-some-properties-of-pca.html#fnref32" class="footnote-back">↩︎</a></p></li>
<li id="fn33"><p>There are other non-orthogonal rotations, which we will use in EFA.<a href="example-1-to-see-some-properties-of-pca.html#fnref33" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="write-up.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="example-two.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": null,
    "text": null
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
